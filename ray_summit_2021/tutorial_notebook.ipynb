{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "# Hands-on RL with Ray’s RLlib\n",
    "## A beginner’s tutorial for working with multi-agent environments, models, and algorithms\n",
    "\n",
    "<img src=\"images/pitfall.jpg\" width=250> <img src=\"images/tesla.jpg\" width=254> <img src=\"images/forklifts.jpg\" width=169> <img src=\"images/robots.jpg\" width=252> <img src=\"images/dota2.jpg\" width=213>\n",
    "\n",
    "### Overview\n",
    "“Hands-on RL with Ray’s RLlib” is a beginners tutorial for working with reinforcement learning (RL) environments, models, and algorithms using Ray’s RLlib library. RLlib offers high scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL) before proceeding to RLlib (multi- and single-agent) environments, neural network models, hyperparameter tuning, debugging, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo.\n",
    "\n",
    "\"Hands-on RL with Ray’s RLlib\"은 Ray의 RLlib 라이브러리를 사용하여 강화학습(RL) 환경, 모델 및 알고리즘 개발작업을 위한 초보자 자습서입니다. RLlib는 높은 확장성, 선택할 수있는 다양한 알고리즘(오프라인 RL, 모델 기반 RL, 모델 프리 RL 등), TensorFlow 및 PyTorch 지원, 다양한 애플리케이션을 위한 통합된 API를 제공합니다. 이 자습서에는 RLlib (다중 및 단일 에이전트) 환경, 신경망 모델, 하이퍼파라미터 튜닝, 디버깅, excercises, Q/A 등을 진행하기 전에 RL에 대한 개념을 설명하는 개요(예 : RL을 사용하는 이유)가 포함되어 있습니다. 모든 코드는 GitHub 저장소에서 .py 파일로 제공됩니다.\n",
    "\n",
    "### Intended Audience\n",
    "* 강화 학습 및 RLlib를 시작하려는 Python 프로그래머.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "To get this very notebook up and running on your local machine, you can follow these steps here:\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install cmake \"ray[rllib]\"\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install [tensorflow|torch]  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "$ conda install pywin32\n",
    "```\n",
    "\n",
    "Also, for Win10 Atari support, we have to install atari_py from a different source (gym does not support Atari envs on Windows).\n",
    "\n",
    "```\n",
    "$ pip install git+https://github.com/Kojoley/atari-py.git\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and why RLlib?\n",
    "* Core concepts of RLlib: Environments, Trainers, Policies, and Models.\n",
    "* How to configure, hyperparameter-tune, and parallelize RLlib.\n",
    "* RLlib debugging best practices.\n",
    "\n",
    "* 강화 학습이란 무엇이며 왜 RLlib 인가요?\n",
    "* RLlib의 핵심 개념 : 환경, 트레이너, 정책 및 모델.\n",
    "* RLlib를 구성, 하이퍼 파라미터 조정 및 병렬화하는 방법.\n",
    "* RLlib 디버깅 모범 사례.\n",
    "\n",
    "### Tutorial Outline\n",
    "1. RL and RLlib in a nutshell.\n",
    "1. Defining an RL-solvable problem: Our first environment.\n",
    "1. **Exercise No.1**: Environment loop.\n",
    "\n",
    "1. RL 및 RLlib 간단 요약.\n",
    "1. RL로 해결 가능한 문제 정의 : 첫번째 환경 소개.\n",
    "1. **Exercise No.1** : 환경 루프.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Picking an algorithm and training our first RLlib Trainer.\n",
    "1. Configurations and hyperparameters - Easy tuning with Ray Tune.\n",
    "1. Fixing our experiment's config - Going multi-agent.\n",
    "1. The \"infinite laptop\": Quick intro into how to use RLlib with Anyscale's product.\n",
    "1. **Exercise No.2**: Run your own Ray RLlib+Tune experiment)\n",
    "1. Neural network models - Provide your custom models using tf.keras or torch.nn.\n",
    "\n",
    "1. 알고리즘을 선택하고 첫번째 RLlib 트레이너를 학습.\n",
    "1. Config 및 하이퍼파라미터 - Ray Tune을 통한 손쉬운 튜닝.\n",
    "1. 실험 Config 수정 - 다중 에이전트로 전환.\n",
    "1. \"Infinite laptop\" : Anyscale 제품에서 RLlib를 사용하는 방법에 대한 간단 소개.\n",
    "1. **Exercise No.2** : 자신만의 Ray RLlib + Tune 실험 실행.\n",
    "1. 신경망 모델-tf.keras 또는 torch.nn을 사용하여 Custom 모델을 제공.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Deeper dive into RLlib's parallelization architecture.\n",
    "1. Specifying different compute resources and parallelization options through our config.\n",
    "1. \"Hacking in\": Using callbacks to customize the RL loop and generate our own metrics.\n",
    "1. **Exercise No.3**: Write your own custom callback.\n",
    "1. \"Hacking in (part II)\" - Debugging with RLlib and PyCharm.\n",
    "1. Checking on the \"infinite laptop\" - Did RLlib learn to solve the problem?\n",
    "\n",
    "1. RLlib의 병렬화 아키텍처에 대해 자세히 알아봅니다.\n",
    "1. 구성을 통해 다양한 컴퓨팅 리소스 및 병렬화 옵션을 지정합니다.\n",
    "1. \"Hacking in\": 콜백을 사용하여 RL 루프를 사용자 정의하고 자체 메트릭을 생성합니다.\n",
    "1. **Exercise No.3** : 사용자 지정 콜백을 작성합니다.\n",
    "1. \"Hacking in (part II)\"-RLlib 및 PyCharm을 사용한 디버깅.\n",
    "1. \"infinite laptop\" 에서 'RLlib를 통해서 문제 해결 방법을 학습했는지' 확인합니다.\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<img src=\"images/unity3d_blog_post.png\" width=400>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "## The RL cycle\n",
    "\n",
    "<img src=\"images/rl-cycle.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62744730",
   "metadata": {},
   "source": [
    "### Coding/defining our \"problem\" via an RL environment.\n",
    "\n",
    "We will use the following (adversarial) multi-agent environment\n",
    "throughout this tutorial to demonstrate a large fraction of RLlib's\n",
    "APIs, features, and customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb35116-efda-4799-8bae-e96d7775a0d1",
   "metadata": {},
   "source": [
    "<img src=\"images/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe753-d7e0-4de1-b937-160507f75ed8",
   "metadata": {},
   "source": [
    "### Spaces 에 대해 간단히 알아 봅니다:\n",
    "\n",
    "ML에서 Space은 신경망의 입력 및 출력이 가질 수있는 가능한/유효한 값을 설명하는 데 사용됩니다.\n",
    "\n",
    "RL 환경은 또한 이를 사용하여 유효한 관찰(Observation) 및 행동(Action)이 무엇인지 설명합니다.\n",
    "\n",
    "Space는 일반적으로 쉐이프(예 : 84x84x3 RGB 이미지) 및 데이터 타입(예 : 0에서 255 사이의 RGB 값에 대한 uint8)으로 정의됩니다.\n",
    "그러나 Space은 다른 Space로 구성되거나(튜플 또는 딕트 공백 참조) n개의 고정 가능한 값으로 단순히 이산적(정수)일 수도 있습니다. 예를 들어 각 에이전트가 위 / 아래 / 왼쪽 / 오른쪽으로 만 이동할 수있는 게임에서 액션 공간은`Discrete(4)`입니다(이 경우, 데이터 타입이 없고, 쉐이프를 정의할 필요가 없습니다). 우리의 Observation Space는 `MultiDiscrete([n, m])`이 될 것입니다. 이 경우, n은 내 에이전트의 위치이고 m은 상대 에이전트의 위치입니다. 따라서 agent1이 왼쪽 상단에서 시작하고 agent2가 하단에서 시작한다면, 오른쪽 모서리에서 agent1의 관측치는`[0, 63]`(8 x 8 그리드에서)이고 agent2의 관측치는`[63, 0]`입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e4135-98ed-4e65-9e26-66f340747529",
   "metadata": {},
   "source": [
    "<img src=\"images/spaces.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6925507-0210-49d2-9e68-5d1e1157ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "|.         |\n",
      "|1         |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|         2|\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "\n",
      "R1= 1.0\n",
      "R2=-0.1\n",
      "\n",
      "Agent1's x/y position=[1, 0]\n",
      "Agent2's x/y position=[8, 9]\n",
      "Env timesteps=1\n"
     ]
    }
   ],
   "source": [
    "# Let's code our multi-agent environment.\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        # Dimensions of the grid.\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        # End an episode after this many timesteps.\n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Returns initial observation of next(!) episode.\"\"\"\n",
    "        # Row-major coords.\n",
    "        self.agent1_pos = [0, 0]  # upper left corner\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]  # lower bottom corner\n",
    "\n",
    "        # Accumulated rewards in this episode.\n",
    "        self.agent1_R = 0.0\n",
    "        self.agent2_R = 0.0\n",
    "\n",
    "        # Reset agent1's visited fields.\n",
    "        self.agent1_visited_fields = set([tuple(self.agent1_pos)])\n",
    "\n",
    "        # How many timesteps have we done in this episode.\n",
    "        self.timesteps = 0\n",
    "\n",
    "        # Return the initial observation in the new episode.\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \"\"\"\n",
    "        Returns (next observation, rewards, dones, infos) after having taken the given actions.\n",
    "        \n",
    "        e.g.\n",
    "        `action={\"agent1\": action_for_agent1, \"agent2\": action_for_agent2}`\n",
    "        \"\"\"\n",
    "        \n",
    "        # increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        # Agent2 always moves first.\n",
    "        # events = [collision|agent1_new_field]\n",
    "        events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "            \n",
    "        # Get observations (based on new agent positions).\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        \n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
    "            self.agent1_visited_fields.add(tuple(coords))\n",
    "            return {\"agent1_new_field\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "        print(\"_\" * (self.width + 2))\n",
    "        for r in range(self.height):\n",
    "            print(\"|\", end=\"\")\n",
    "            for c in range(self.width):\n",
    "                field = r * self.width + c % self.width\n",
    "                if self.agent1_pos == [r, c]:\n",
    "                    print(\"1\", end=\"\")\n",
    "                elif self.agent2_pos == [r, c]:\n",
    "                    print(\"2\", end=\"\")\n",
    "                elif (r, c) in self.agent1_visited_fields:\n",
    "                    print(\".\", end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"‾\" * (self.width + 2))\n",
    "        print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
    "        print(\"R1={: .1f}\".format(self.agent1_R))\n",
    "        print(\"R2={: .1f}\".format(self.agent2_R))\n",
    "        print()\n",
    "\n",
    "\n",
    "env = MultiAgentArena()\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Agent1 will move down, Agent2 moves up.\n",
    "obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Exercise No 1\n",
    "\n",
    "<hr />\n",
    "\n",
    "<img src=\"images/exercise1.png\" width=400>\n",
    "\n",
    "위의 셀에서 `reset()`과 단일 `step()`호출을 수행했습니다. 전체 에피소드를 살펴 보면, 반환된 `done` 딕셔너리가 \"agent1\" 또는 \"agent2\"(또는 \"__all__\") 키가 True로 설정될 때까지 일반적으로 `step()`을 (다른 Action으로) 반복적으로 호출합니다. 여러분의 임무는 `MultiAgentArena` 클래스를 사용하여 정확히 하나의 에피소드에 대해 실행되는 \"환경 루프\"를 작성하는 것입니다.\n",
    "\n",
    "이 작업을 수행하기 위해 아래 지침을 따르십시오.\n",
    "\n",
    "1. 이미 생성된 (변수`env`) 환경을 `재설정`하여 첫 번째 (초기)Observation을 얻습니다.\n",
    "1. 무한 while 루프를 돕니다.\n",
    "1.`DummyTrainer.compute_action([obs])`을 두 번 호출하여 \"agent1\" 및 \"agent2\"에 대한 Action을 계산합니다 (각 에이전트에 대해 한 번씩).\n",
    "1. Action 계산의 결과를 Action dict(`{ \"agent1\": ..., \"agent2\": ...}`)에 입력합니다.\n",
    "1. 위의 셀(단일 `step()`을 실행)에서 수행한 것처럼 이 Action dict를 env의 `step()`메서드에 전달합니다.\n",
    "1. 반환된 'dones' dict에서 True(에피소드가 종료됨을 의미함)를 확인하고 True인 경우 루프를 종료합니다.\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_agent1=0\n",
      "action_agent2=3\n",
      "\n",
      "action_agent1=3\n",
      "action_agent2=2\n",
      "\n",
      "action_agent1=3\n",
      "action_agent2=1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DummyTrainer:\n",
    "    \"\"\"Dummy Trainer class used in Exercise #1.\n",
    "\n",
    "    Use its `compute_action` method to get a new action for one of the agents,\n",
    "    given the agent's observation (a single discrete value encoding the field\n",
    "    the agent is currently in).\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_action(self, single_agent_obs=None):\n",
    "        # Returns a random action for a single agent.\n",
    "        return np.random.randint(4)  # Discrete(4) -> return rand int between 0 and 3 (incl. 3).\n",
    "\n",
    "dummy_trainer = DummyTrainer()\n",
    "# Check, whether it's working.\n",
    "for _ in range(3):\n",
    "    # Get action for agent1 (providing agent1's and agent2's positions).\n",
    "    print(\"action_agent1={}\".format(dummy_trainer.compute_action(np.array([0, 99]))))\n",
    "\n",
    "    # Get action for agent2 (providing agent2's and agent1's positions).\n",
    "    print(\"action_agent2={}\".format(dummy_trainer.compute_action(np.array([99, 0]))))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa8a1d-7d82-4b79-b2d4-3ce7ffa6fae1",
   "metadata": {},
   "source": [
    "Write your solution code into this cell here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b12373b-4b71-4ee9-a7a1-13077a59840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025c7d37a42541a8824c61784ca3f85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "# Leave the following as-is. It'll help us with rendering the env in this very cell's output.\n",
    "import time\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "\n",
    "    # Solution to Exercise #1:\n",
    "\n",
    "    # Start coding here inside this `with`-block:\n",
    "    # 1) Reset the env.\n",
    "    env = MultiAgentArena()\n",
    "    obs = env.reset()\n",
    "    \n",
    "    dummy_trainer = DummyTrainer()\n",
    "    # 2) Enter an infinite while loop (to step through the episode).\n",
    "    while True:\n",
    "        # 3) Calculate both agents' actions individually, using dummy_trainer.compute_action([individual agent's obs])\n",
    "        a1 = dummy_trainer.compute_action(obs[\"agent1\"])\n",
    "        a2 = dummy_trainer.compute_action(obs[\"agent2\"])\n",
    "        # 4) Compile the actions dict from both individual agents' actions.\n",
    "        actions = {\"agent1\":a1, \"agent2\":a2}\n",
    "        # 5) Send the actions dict to the env's `step()` method to receive: obs, rewards, dones, info dicts\n",
    "        obs, rewards, dones, _  = env.step(actions)\n",
    "        # 6) We'll do this together: Render the env.\n",
    "        # Don't write any code here (skip directly to 7).\n",
    "        out.clear_output(wait=True)\n",
    "        time.sleep(0.08)\n",
    "        env.render()\n",
    "\n",
    "        # 7) Check, whether the episde is done, if yes, break out of the while loop.\n",
    "        if dones[\"__all__\"]:\n",
    "            break\n",
    "# 8) Run it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4196a5-7e7a-442a-8100-96bc7393c59d",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "### And now for something completely different:\n",
    "#### Plugging in RLlib!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd830b90-5762-4d22-8fa9-0abf0777a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-01 11:06:46,197\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.30.1.40',\n",
       " 'raylet_ip_address': '172.30.1.40',\n",
       " 'redis_address': '172.30.1.40:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-07-01_11-06-44_562960_4956/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-07-01_11-06-44_562960_4956/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-07-01_11-06-44_562960_4956',\n",
       " 'metrics_export_port': 55923,\n",
       " 'node_id': '589bd408048d2c7dfc1b96eac59af4507da2caa95f5a1de1c6719304'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "\n",
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "### Picking an RLlib algorithm - We'll use PPO throughout this tutorial (one-size-fits-all-kind-of-algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algos.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bcc1116-a14c-4479-87c0-6ece58ab0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-01 11:06:53,133\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-07-01 11:06:59,982\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPO"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the PPO algorithm here b/c its very flexible wrt its supported\n",
    "# action spaces and model types and b/c it learns well almost any problem.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Specify a very simple config, defining our environment and some environment\n",
    "# options (see environment.py).\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,  # \"my_env\" <- if we previously have registered the env with `tune.register_env(\"[name]\", lambda config: [returns env object])`.\n",
    "    \"env_config\": {\n",
    "        \"config\": {\n",
    "            \"width\": 10,\n",
    "            \"height\": 10,\n",
    "            \"ts\": 100,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # !PyTorch users!\n",
    "    \"framework\": \"torch\",  # If users have chosen to install torch instead of tf.\n",
    "\n",
    "    \"create_env_on_driver\": True,\n",
    "}\n",
    "# Instantiate the Trainer object using above config.\n",
    "rllib_trainer = PPOTrainer(config=config)\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ae150-c0a3-477f-8d78-d0a34f147958",
   "metadata": {},
   "source": [
    "### Ready to train with RLlib's PPO algorithm\n",
    "\n",
    "That's it, we are ready to train.\n",
    "Calling `Trainer.train()` will execute a single \"training iteration\".\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1) sampling from the environment(s)\n",
    "2) using the sampled data (observations, actions taken, rewards) to update the policy model (neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:\n",
    "\n",
    "이제 학습할 준비가되었습니다.\n",
    "`Trainer.train()`을 호출하면 단일 \"학습 이터레이션\"이 실행됩니다.\n",
    "\n",
    "알고리즘에 대한 한 번의 이터레이션은 다음과 같습니다.\n",
    "\n",
    "1) 환경에서 샘플링\n",
    "2) 샘플링된 데이터 (observations, actions taken, rewards)를 사용하여 policy 모델(신경망)을 업데이트하여 향후 더 나은 Action을 선택하여 더 높은 Reward를 얻습니다.\n",
    "\n",
    "실행해 봅시다. : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6c94d4-6871-4d20-81af-3d4081f05f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 4000,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2021-07-01_11-07-30',\n",
      " 'done': False,\n",
      " 'episode_len_mean': 100.0,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': 10.79999999999997,\n",
      " 'episode_reward_mean': -4.3950000000000005,\n",
      " 'episode_reward_min': -28.500000000000043,\n",
      " 'episodes_this_iter': 20,\n",
      " 'episodes_total': 20,\n",
      " 'experiment_id': '7db9968dfd0f49b1b9e90e48edd6e575',\n",
      " 'hist_stats': {'episode_lengths': [100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100],\n",
      "                'episode_reward': [10.79999999999997,\n",
      "                                   -13.499999999999979,\n",
      "                                   -7.499999999999991,\n",
      "                                   9.600000000000026,\n",
      "                                   8.100000000000001,\n",
      "                                   -21.00000000000002,\n",
      "                                   1.8000000000000176,\n",
      "                                   4.500000000000018,\n",
      "                                   3.9000000000000195,\n",
      "                                   -27.300000000000054,\n",
      "                                   -10.499999999999998,\n",
      "                                   7.200000000000024,\n",
      "                                   4.199999999999958,\n",
      "                                   -10.499999999999988,\n",
      "                                   3.0000000000000124,\n",
      "                                   -19.800000000000004,\n",
      "                                   -28.500000000000043,\n",
      "                                   -13.499999999999995,\n",
      "                                   1.8000000000000158,\n",
      "                                   9.300000000000017]},\n",
      " 'hostname': 'SKCC17N00536.local',\n",
      " 'info': {'learner': defaultdict(<class 'dict'>,\n",
      "                                 {'default_policy': {'learner_stats': {'allreduce_latency': 0.0,\n",
      "                                                                       'cur_kl_coeff': 0.2,\n",
      "                                                                       'cur_lr': 5e-05,\n",
      "                                                                       'entropy': 1.3672765083611012,\n",
      "                                                                       'entropy_coeff': 0.0,\n",
      "                                                                       'kl': 0.019294706464279443,\n",
      "                                                                       'policy_loss': -0.060960935516050085,\n",
      "                                                                       'total_loss': 22.693559885025024,\n",
      "                                                                       'vf_explained_var': 0.1681119,\n",
      "                                                                       'vf_loss': 22.750661969184875}}}),\n",
      "          'num_agent_steps_sampled': 4000,\n",
      "          'num_steps_sampled': 4000,\n",
      "          'num_steps_trained': 4000},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '172.30.1.40',\n",
      " 'num_healthy_workers': 2,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 20.41627906976744,\n",
      "          'ram_util_percent': 62.32325581395349},\n",
      " 'pid': 4956,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {'mean_action_processing_ms': 0.05665990141602782,\n",
      "                  'mean_env_render_ms': 0.0,\n",
      "                  'mean_env_wait_ms': 0.031360736736408136,\n",
      "                  'mean_inference_ms': 0.8491848136757042,\n",
      "                  'mean_raw_obs_processing_ms': 0.152932180391325},\n",
      " 'time_since_restore': 8.0693941116333,\n",
      " 'time_this_iter_s': 8.0693941116333,\n",
      " 'time_total_s': 8.0693941116333,\n",
      " 'timers': {'learn_throughput': 576.878,\n",
      "            'learn_time_ms': 6933.876,\n",
      "            'sample_throughput': 3537.515,\n",
      "            'sample_time_ms': 1130.737,\n",
      "            'update_time_ms': 1.425},\n",
      " 'timestamp': 1625105250,\n",
      " 'timesteps_since_restore': 0,\n",
      " 'timesteps_total': 4000,\n",
      " 'training_iteration': 1}\n"
     ]
    }
   ],
   "source": [
    "results = rllib_trainer.train()\n",
    "\n",
    "# Delete the config from the results for clarity.\n",
    "# Only the stats will remain, then.\n",
    "del results[\"config\"]\n",
    "# Pretty print the stats.\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96f682-fc7d-46a0-b136-f5d62cd7ad67",
   "metadata": {},
   "source": [
    "### Going from single policy (RLlib's default) to multi-policy:\n",
    "\n",
    "So far, our experiment has been ill-configured, because both\n",
    "agents, which should behave differently due to their different\n",
    "tasks and reward functions, learn the same policy: the \"default_policy\",\n",
    "which RLlib always provides if you don't configure anything else.\n",
    "Remember that RLlib does not know at Trainer setup time, how many and which agents\n",
    "the environment will \"produce\". Agent control (adding agents, removing them, terminating\n",
    "episodes for agents) is entirely in the Env's hands.\n",
    "Let's fix our single policy problem and introduce the \"multiagent\" API.\n",
    "지금까지 우리의 실험은 잘못 구성되었습니다.\n",
    "에이전트는 서로 다르기 때문에 다르게 행동해야합니다.\n",
    "각 에이전트는 Task 및 Reward Function이 다른데 동일한 Policy를 학습하고 있습니다.\n",
    "RLlib가 디폴트로 동일한 Policy(\"default_policy\")를 제공합니다.\n",
    "RLlib는 Trainer 설정 시 환경은 \"생산\"할 에이전트의 수와 에이전트를 알지 못한다는 점을 기억하십시오.\n",
    "에이전트 제어(에이전트 추가, 제거, 에피소드 종료)는 전적으로 Env의 손에 달려 있습니다.\n",
    "그러면 이제부터 이와 같은 Single Policy 문제를 수정하면서 \"다중 에이전트(multiagent)\" API를 소개하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13900163-f520-40f1-87be-d759760bd3a5",
   "metadata": {},
   "source": [
    "<img src=\"images/from_single_agent_to_multi_agent.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a813b988-e40f-4890-8c9c-f5f7d0f49cc9",
   "metadata": {},
   "source": [
    "In order to turn on RLlib's multi-agent functionality, we need two things:\n",
    "\n",
    "1. A policy mapping function, mapping agent IDs (e.g. a string like \"agent1\", produced by the environment in the returned observation/rewards/dones-dicts) to a policy ID (another string, e.g. \"policy1\", which is under our control).\n",
    "1. A policies definition dict, mapping policy IDs (e.g. \"policy1\") to 4-tuples consisting of 1) policy class (None for using the default class), 2) observation space, 3) action space, and 4) config overrides (empty dict for no overrides and using the Trainer's main config dict).\n",
    "\n",
    "Let's take a closer look:\n",
    "\n",
    "RLlib의 다중 에이전트 기능을 켜려면 다음 두 가지가 필요합니다.\n",
    "\n",
    "1. policy mapping function : 매핑 '에이전트 ID(Agent ID)'(예 : 반환된 observation / reward / dones-dicts에 있는 환경에 의해 생성된 \"agent1\"과 같은 문자열)를 '정책 ID(Policy ID)'(다른 문자열, 예 : \"policy1\", 이것은 우리의 통제권 안에 있음)와 맵핑해야 합니다.\n",
    "1. policies definition dict : 정책 ID(예 : \"policy1\")를 1) 정책 클래스(Policy class: 기본 클래스를 사용하는 경우는 None), 2) Observation space, 3) Action space, 4) Config 재정의(Trainer의 기본 구성 딕셔너리를 사용함. 재정의가 없을 때는 빈 딕셔너리 사용)로 구성된 4-튜플에 매핑해야 합니다..\n",
    "\n",
    "자세히 살펴 봅시다. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dff7017-f1b9-41e8-94fd-266bbe56cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'create_env_on_driver': True,\n",
      " 'env': <class '__main__.MultiAgentArena'>,\n",
      " 'env_config': {'config': {'height': 10, 'ts': 100, 'width': 10}},\n",
      " 'framework': 'torch',\n",
      " 'multiagent': {'policies': {'policy1': (None,\n",
      "                                         MultiDiscrete([100 100]),\n",
      "                                         Discrete(4),\n",
      "                                         {}),\n",
      "                             'policy2': (None,\n",
      "                                         MultiDiscrete([100 100]),\n",
      "                                         Discrete(4),\n",
      "                                         {'lr': 0.0002})},\n",
      "                'policy_mapping_fn': <function policy_mapping_fn at 0x7f81915380d0>}}\n",
      "\n",
      "agent1 is now mapped to policy1\n",
      "agent2 is now mapped to policy2\n"
     ]
    }
   ],
   "source": [
    "# Define the policies definition dict:\n",
    "# Each policy in there is defined by its ID (key) mapping to a 4-tuple (value):\n",
    "# - Policy class (None for using the \"default\" class, e.g. PPOTFPolicy for PPO+tf or PPOTorchPolicy for PPO+torch).\n",
    "# - obs-space (we get this directly from our already created env object).\n",
    "# - act-space (we get this directly from our already created env object).\n",
    "# - config-overrides dict (leave empty for using the Trainer's config as-is)\n",
    "policies = {\n",
    "    \"policy1\": (None, env.observation_space, env.action_space, {}),\n",
    "    \"policy2\": (None, env.observation_space, env.action_space, {\"lr\": 0.0002}),\n",
    "}\n",
    "# Note that now we won't have a \"default_policy\" anymore, just \"policy1\" and \"policy2\".\n",
    "\n",
    "# Define an agent->policy mapping function.\n",
    "# Which agents (defined by the environment) use which policies (defined by us)?\n",
    "# The mapping here is M (agents) -> N (policies), where M >= N.\n",
    "def policy_mapping_fn(agent_id: str):\n",
    "    # Make sure agent ID is valid.\n",
    "    assert agent_id in [\"agent1\", \"agent2\"], f\"ERROR: invalid agent ID {agent_id}!\"\n",
    "    # Map agent1 to policy1, and agent2 to policy2.\n",
    "    return \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "\n",
    "# We could - if we wanted - specify, which policies should be learnt (by default, RLlib learns all).\n",
    "# Non-learnt policies will be frozen and not updated:\n",
    "# policies_to_train = [\"policy1\", \"policy2\"]\n",
    "\n",
    "# Adding the above to our config.\n",
    "config.update({\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        # We'll leave this empty: Means, we train both policy1 and policy2.\n",
    "        # \"policies_to_train\": policies_to_train,\n",
    "    },\n",
    "})\n",
    "\n",
    "pprint.pprint(config)\n",
    "print()\n",
    "print(f\"agent1 is now mapped to {policy_mapping_fn('agent1')}\")\n",
    "print(f\"agent2 is now mapped to {policy_mapping_fn('agent2')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646f8800-941b-43cb-a924-622af6788aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-01 11:07:47,429\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPO"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate our Trainer (we cannot just change the config on-the-fly).\n",
    "rllib_trainer.stop()\n",
    "\n",
    "# Using our updated (now multiagent!) config dict.\n",
    "rllib_trainer = PPOTrainer(config=config)\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "Now that we are setup correctly with two policies as per our \"multiagent\" config, let's call `train()` on the new Trainer several times (what about 10 times?).\n",
    "\n",
    "이제 \"멀티 에이전트\"구성에 따라 두 가지 Policy로 올바르게 설정되었으므로 새 Trainer에서 `train()`을 여러 번 호출 해 보겠습니다 (약 10번 정도?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17ae724d-71cc-422b-96cb-3dc9faa2d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1: R(\"return\")=-10.687500000000002\n",
      "Iteration=2: R(\"return\")=-5.0362499999999955\n",
      "Iteration=3: R(\"return\")=-1.7219999999999933\n",
      "Iteration=4: R(\"return\")=1.8510000000000093\n",
      "Iteration=5: R(\"return\")=1.5630000000000106\n",
      "Iteration=6: R(\"return\")=1.5600000000000147\n",
      "Iteration=7: R(\"return\")=1.5300000000000133\n",
      "Iteration=8: R(\"return\")=2.1150000000000126\n",
      "Iteration=9: R(\"return\")=3.3930000000000073\n",
      "Iteration=10: R(\"return\")=5.769\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "# Move on once you see (agent1 + agent2) episode rewards of 10.0 or more.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365ef0d7-9977-4d9d-9fa5-ffaa7c111b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=11: R(\"return\")=6.624000000000014 R1=13.61 R2=-6.9859999999999856\n",
      "Iteration=12: R(\"return\")=6.981000000000013 R1=13.725 R2=-6.7439999999999864\n",
      "Iteration=13: R(\"return\")=8.145000000000014 R1=14.405 R2=-6.2599999999999865\n",
      "Iteration=14: R(\"return\")=10.344000000000014 R1=15.845 R2=-5.500999999999987\n",
      "Iteration=15: R(\"return\")=11.763000000000012 R1=16.89 R2=-5.126999999999989\n",
      "Iteration=16: R(\"return\")=12.27300000000001 R1=17.125 R2=-4.8519999999999905\n",
      "Iteration=17: R(\"return\")=12.567000000000009 R1=17.54 R2=-4.97299999999999\n",
      "Iteration=18: R(\"return\")=14.190000000000012 R1=19.295 R2=-5.104999999999989\n",
      "Iteration=19: R(\"return\")=16.35600000000001 R1=21.45 R2=-5.09399999999999\n",
      "Iteration=20: R(\"return\")=17.55000000000001 R1=21.225 R2=-3.674999999999989\n"
     ]
    }
   ],
   "source": [
    "# Do another loop, but this time, we will print out each policies' individual rewards.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    r1 = results['policy_reward_mean']['policy1']\n",
    "    r2 = results['policy_reward_mean']['policy2']\n",
    "    r = r1 + r2\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={r} R1={r1} R2={r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80ad33-a55b-4e18-857b-b884eedda0a4",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK! (<-- we will not do these during the tutorial, but feel free to try these cells by yourself)\n",
    "\n",
    "Use the above solution of Exercise #1 and replace our `dummy_trainer` in that solution\n",
    "with the now trained `rllib_trainer`. You should see a better performance of the two agents.\n",
    "\n",
    "However, keep in mind that we are mostly training agent1 as we only trian a single policy and agent1\n",
    "is the \"easier\" one to collect high rewards with.\n",
    "\n",
    "위의 Exercise #1 에 위 솔루션을 사용해 봅니다. 'dummy_trainer'를 대체합니다.\n",
    "이제 학습된`rllib_trainer`로. 두 에이전트의 더 나은 성능을 볼 수 있습니다.\n",
    "\n",
    "이 경우 Single policy와 agent1만 학습하기 때문에 대부분 agent1만이 학습되어 agent1이 높은 Reward를 모으는 데 \"더 쉬워 진다\"는 점을 주의하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using Policies and their model(s) inside our Trainer object):\n",
    "\n",
    "RLlib(내부에서)가 Trainer 객체 내부의 Policy 및 해당 모델을 사용하여 환경 Observation에서 Action을 계산하는 방법을 알아 보려면 다음 코드를 자유롭게 사용하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aff679e8-74b4-4603-9d5c-4cc0c6ebe45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our (only!) Policy right now is: <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7f8194131610>\n",
      "Our Policy's observation space is: Box(-1.0, 1.0, (200,), float32)\n",
      "Our Policy's action space is: Discrete(4)\n",
      "sampled action=1\n"
     ]
    }
   ],
   "source": [
    "# Let's actually \"look inside\" our Trainer to see what's in there.\n",
    "from ray.rllib.utils.numpy import softmax\n",
    "\n",
    "# To get to one of the policies inside the Trainer, use `Trainer.get_policy([policy ID])`:\n",
    "policy = rllib_trainer.get_policy(\"policy1\")\n",
    "print(f\"Our (only!) Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = np.array([policy.observation_space.sample()])\n",
    "# Alternatively for PyTorch:\n",
    "import torch\n",
    "obs = torch.from_numpy(obs)\n",
    "\n",
    "# Get the action logits (as tf tensor).\n",
    "# If you are using torch, you would get a torch tensor here.\n",
    "logits, _ = model({\"obs\": obs})\n",
    "logits\n",
    "\n",
    "# Numpyize the tensor by running `logits` through the Policy's own tf.Session.\n",
    "# logits_np = policy.get_session().run(logits)\n",
    "# For torch, you can simply do: `logits_np = logits.detach().cpu().numpy()`.\n",
    "logits_np = logits.detach().cpu().numpy()\n",
    "\n",
    "# Convert logits into action probabilities and remove the B=1.\n",
    "action_probs = np.squeeze(softmax(logits_np))\n",
    "\n",
    "# Sample an action, using the probabilities.\n",
    "action = np.random.choice([0, 1, 2, 3], p=action_probs)\n",
    "\n",
    "# Print out the action.\n",
    "print(f\"sampled action={action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `rllib_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Policy's model that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!\n",
    "\n",
    "현재`rllib_trainer`는 이미 학습된 상태입니다.\n",
    "Policy 모델에 최적화된 가중치를 가지고 있고 이를 통해 주어진 환경에서 받은 Observation에 대해 어느정도 스마트한 행동을 합니다.\n",
    "\n",
    "하지만 지금 이 노트를 닫는다면 모든 노력이 헛되게 됩니다.\n",
    "나중을 위해 trainer의 상태를 디스크에 저장해 봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 20 was saved in '/Users/parksurk/ray_results/PPO_MultiAgentArena_2021-07-01_11-07-40poki00jw/checkpoint_000020/checkpoint-20'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-20', 'checkpoint-20.tune_metadata', '.is_checkpoint']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = rllib_trainer.save()\n",
    "print(f\"Trainer (at iteration {rllib_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "import os\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file).\n",
    "\n",
    "다음 셀에서는 체크 포인트 파일에서 저장된 Trainer를 복원하는 방법을 알아 봅니다.\n",
    "\n",
    "또한 완전히 새로운 트레이너(다소 무작위로 작동해야 함)와 이미 학습된 트레이너(위에서 만든 체크 포인트 파일에서 방금 복원 한 것)를 평가할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-01 11:40:23,033\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "2021-07-01 11:40:26,861\tINFO trainable.py:377 -- Restored on 172.30.1.40 from checkpoint: /Users/parksurk/ray_results/PPO_MultiAgentArena_2021-07-01_11-07-40poki00jw/checkpoint_000020/checkpoint-20\n",
      "2021-07-01 11:40:26,862\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 368.76109981536865, '_episodes_total': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating new trainer: R=-12.330000000000002\n",
      "Before restoring: Trainer is at iteration=0\n",
      "After restoring: Trainer is at iteration=20\n",
      "Evaluating restored trainer: R=18.70499999999994\n"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = PPOTrainer(config=config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "new_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps.\n",
    "\n",
    "Trainer에서 모든 리소스를 해제하려면 Trainer의`stop()`메서드를 사용할 수 있습니다.\n",
    "병렬 하이퍼 파라미터 스윕을 수행 할 때 이 자습서의 뒷부분에서 필요한 리소스를 확보해야 하므로 아래 셀을 실행해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rllib_trainer.stop()\n",
    "new_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1e4c-cb02-4719-ac5a-0106172a6c6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Moving stuff to the professional level: RLlib in connection w/ Ray Tune\n",
    "\n",
    "Running any experiments through Ray Tune is the recommended way of doing things with RLlib. If you look at our\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/rllib/examples\">examples scripts folder</a>, you will see that almost all of the scripts use Ray Tune to run the particular RLlib workload demonstrated in each script.\n",
    "\n",
    "Ray Tune을 통해 실험을 실행하는 것은 RLlib로 작업을 수행할 때 권장하는 방법입니다. 다음 링크를 보면\n",
    "\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/rllib/examples\">예제 스크립트 폴더</a>\n",
    "\n",
    "거의 모든 스크립트가 Ray Tune을 사용하여 각 스크립트에 설명된 특정 RLlib 워크로드를 실행하는 것을 볼 수 있습니다.\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdacebb-d27f-4174-9002-35c5657f146c",
   "metadata": {
    "tags": []
   },
   "source": [
    "When setting up hyperparameter sweeps for Tune, we'll do this in our already familiar config dict.\n",
    "\n",
    "So let's take a quick look at our PPO algo's default config to understand, which hyperparameters we may want to play around with:\n",
    "\n",
    "Tune에 대한 하이퍼파라미터 스윕을 설정할 때 이미 익숙한 구성 딕션어리에서 수행합니다.\n",
    "\n",
    "그러면 PPO 알고리즘의 기본 구성을 간략히 살펴보고, 어떻게 하이퍼파라미터를 선택하는지 알아 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1b32582-52bd-4585-9009-2f877a0723a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO's default config is:\n",
      "{'_fake_gpus': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': True,\n",
      " 'clip_param': 0.3,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': 180,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'eager_tracing': False,\n",
      " 'entropy_coeff': 0.0,\n",
      " 'entropy_coeff_schedule': None,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': 10,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'tf',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': None,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'kl_coeff': 0.2,\n",
      " 'kl_target': 0.01,\n",
      " 'lambda': 1.0,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 5e-05,\n",
      " 'lr_schedule': None,\n",
      " 'metrics_smoothing_episodes': 100,\n",
      " 'min_iter_time_s': 0,\n",
      " 'model': {'_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'num_framestacks': 'auto',\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': False,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': False,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_sgd_iter': 30,\n",
      " 'num_workers': 2,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 200,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'sgd_minibatch_size': 128,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'shuffle_sequences': True,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 0,\n",
      " 'train_batch_size': 4000,\n",
      " 'use_critic': True,\n",
      " 'use_gae': True,\n",
      " 'vf_clip_param': 10.0,\n",
      " 'vf_loss_coeff': 1.0,\n",
      " 'vf_share_layers': -1}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# PPO algorithm:\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as PPO_DEFAULT_CONFIG\n",
    "print(f\"PPO's default config is:\")\n",
    "pprint.pprint(PPO_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded886cc-436e-46cd-8fea-d68af8b41236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's do a very simple grid-search over two learning rates with tune.run().\n",
    "\n",
    "In particular, we will try the learning rates 0.00005 and 0.5 using `tune.grid_search([...])`\n",
    "inside our config dict:\n",
    "\n",
    "`tune.grid_search ([...])`를 사용하여 학습률 0.00005와 0.5를 시도합니다.\n",
    "구성 딕션어리 내부 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5063991e-173b-49be-a4e7-467e2e18321a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (4 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m 2021-07-01 11:56:13,300\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m 2021-07-01 11:56:13,302\tWARNING ppo.py:135 -- `train_batch_size` (3000) cannot be achieved with your other settings (num_workers=2 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1500.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m 2021-07-01 11:56:13,300\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m 2021-07-01 11:56:13,302\tWARNING ppo.py:135 -- `train_batch_size` (3000) cannot be achieved with your other settings (num_workers=2 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1500.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m 2021-07-01 11:56:23,687\tINFO trainable.py:101 -- Trainable.setup took 10.387 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m 2021-07-01 11:56:23,687\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m 2021-07-01 11:56:23,685\tINFO trainable.py:101 -- Trainable.setup took 10.385 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m 2021-07-01 11:56:23,685\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-56-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.200000000000028\n",
      "  episode_reward_mean: -9.09\n",
      "  episode_reward_min: -37.50000000000004\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 109393779bf04cd2a072197ec47e0f68\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.3596946497758229\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027194303460419178\n",
      "          policy_loss: -0.07365838422750433\n",
      "          total_loss: 39.00392214457194\n",
      "          vf_explained_var: 0.16660884022712708\n",
      "          vf_loss: 39.072141806284584\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.3389754345019658\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04884385302041968\n",
      "          policy_loss: -0.09155460451923621\n",
      "          total_loss: 2.14621339738369\n",
      "          vf_explained_var: 0.3911871910095215\n",
      "          vf_loss: 2.2279992451270423\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.72083333333333\n",
      "    ram_util_percent: 62.0\n",
      "  pid: 5318\n",
      "  policy_reward_max:\n",
      "    policy1: 19.0\n",
      "    policy2: -2.300000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 0.06666666666666667\n",
      "    policy2: -9.156666666666649\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09256057307213483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04530937809852027\n",
      "    mean_inference_ms: 2.1661865480894407\n",
      "    mean_raw_obs_processing_ms: 0.23578040842847933\n",
      "  time_since_restore: 16.415369987487793\n",
      "  time_this_iter_s: 16.415369987487793\n",
      "  time_total_s: 16.415369987487793\n",
      "  timers:\n",
      "    learn_throughput: 238.912\n",
      "    learn_time_ms: 12556.906\n",
      "    sample_throughput: 778.965\n",
      "    sample_time_ms: 3851.266\n",
      "    update_time_ms: 2.354\n",
      "  timestamp: 1625108200\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: e0548_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>RUNNING </td><td>172.30.1.40:5318</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         16.4154</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">   -9.09</td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>RUNNING </td><td>                </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00001:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-56-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.200000000000001\n",
      "  episode_reward_mean: -11.0\n",
      "  episode_reward_min: -30.00000000000005\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 30c1148e916449e1884fd7c5abd4d048\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.09071534869838398\n",
      "          entropy_coeff: 0.0\n",
      "          kl: .inf\n",
      "          policy_loss: 0.477949483320117\n",
      "          total_loss: .inf\n",
      "          vf_explained_var: 0.031452279537916183\n",
      "          vf_loss: 42.38547468185425\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.3447722991307576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.043328814363727965\n",
      "          policy_loss: -0.08124266894689451\n",
      "          total_loss: 1.715460975964864\n",
      "          vf_explained_var: 0.4538188874721527\n",
      "          vf_loss: 1.788037896156311\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.323076923076925\n",
      "    ram_util_percent: 62.00384615384615\n",
      "  pid: 5317\n",
      "  policy_reward_max:\n",
      "    policy1: 18.0\n",
      "    policy2: -5.599999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: -1.55\n",
      "    policy2: -9.449999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -20.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09303105663729067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.046509095305684894\n",
      "    mean_inference_ms: 2.159596998480302\n",
      "    mean_raw_obs_processing_ms: 0.23515164097653163\n",
      "  time_since_restore: 18.153107166290283\n",
      "  time_this_iter_s: 18.153107166290283\n",
      "  time_total_s: 18.153107166290283\n",
      "  timers:\n",
      "    learn_throughput: 209.944\n",
      "    learn_time_ms: 14289.505\n",
      "    sample_throughput: 779.376\n",
      "    sample_time_ms: 3849.233\n",
      "    update_time_ms: 4.834\n",
      "  timestamp: 1625108201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: e0548_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0548_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-56-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999996\n",
      "  episode_reward_mean: -4.479999999999993\n",
      "  episode_reward_min: -37.50000000000004\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 109393779bf04cd2a072197ec47e0f68\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.321617305278778\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029126984998583794\n",
      "          policy_loss: -0.07572167890612036\n",
      "          total_loss: 37.677381912867226\n",
      "          vf_explained_var: 0.243690624833107\n",
      "          vf_loss: 37.74436545372009\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2914625306924183\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03892589515695969\n",
      "          policy_loss: -0.0889766524778679\n",
      "          total_loss: 2.163855398694674\n",
      "          vf_explained_var: 0.4156763553619385\n",
      "          vf_loss: 2.241154278318087\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.251999999999995\n",
      "    ram_util_percent: 62.228\n",
      "  pid: 5318\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: -2.300000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.291666666666667\n",
      "    policy2: -8.77166666666665\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09419008688536303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04596429595285493\n",
      "    mean_inference_ms: 2.2042476648291522\n",
      "    mean_raw_obs_processing_ms: 0.23728821863494948\n",
      "  time_since_restore: 34.18112277984619\n",
      "  time_this_iter_s: 17.7657527923584\n",
      "  time_total_s: 34.18112277984619\n",
      "  timers:\n",
      "    learn_throughput: 229.064\n",
      "    learn_time_ms: 13096.767\n",
      "    sample_throughput: 752.825\n",
      "    sample_time_ms: 3984.987\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1625108217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: e0548_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>RUNNING </td><td>172.30.1.40:5318</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         34.1811</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">   -4.48</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>RUNNING </td><td>172.30.1.40:5317</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         18.1531</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">  -11   </td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-57-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.200000000000001\n",
      "  episode_reward_mean: -21.575000000000014\n",
      "  episode_reward_min: -35.70000000000004\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 30c1148e916449e1884fd7c5abd4d048\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: .inf\n",
      "          policy_loss: 0.1340596245136112\n",
      "          total_loss: .inf\n",
      "          vf_explained_var: 0.013232949189841747\n",
      "          vf_loss: 101.28861967722575\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2741331954797108\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02967210579663515\n",
      "          policy_loss: -0.04995511914603412\n",
      "          total_loss: 2.7457468609015145\n",
      "          vf_explained_var: 0.24788694083690643\n",
      "          vf_loss: 2.7868003845214844\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.675000000000004\n",
      "    ram_util_percent: 62.285714285714285\n",
      "  pid: 5317\n",
      "  policy_reward_max:\n",
      "    policy1: 18.0\n",
      "    policy2: -0.10000000000000298\n",
      "  policy_reward_mean:\n",
      "    policy1: -13.225\n",
      "    policy2: -8.349999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -30.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09478377086196339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04770196571446717\n",
      "    mean_inference_ms: 2.2220640369063367\n",
      "    mean_raw_obs_processing_ms: 0.23906663124864178\n",
      "  time_since_restore: 37.76154613494873\n",
      "  time_this_iter_s: 19.608438968658447\n",
      "  time_total_s: 37.76154613494873\n",
      "  timers:\n",
      "    learn_throughput: 202.514\n",
      "    learn_time_ms: 14813.797\n",
      "    sample_throughput: 739.675\n",
      "    sample_time_ms: 4055.835\n",
      "    update_time_ms: 3.718\n",
      "  timestamp: 1625108221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: e0548_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0548_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-57-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999996\n",
      "  episode_reward_mean: -3.6899999999999897\n",
      "  episode_reward_min: -37.50000000000004\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 109393779bf04cd2a072197ec47e0f68\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.285477876663208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02560847656180461\n",
      "          policy_loss: -0.08114509594937165\n",
      "          total_loss: 27.33042375246684\n",
      "          vf_explained_var: 0.2483212798833847\n",
      "          vf_loss: 27.400043964385986\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.254161849617958\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.034299994663645826\n",
      "          policy_loss: -0.08241988574930777\n",
      "          total_loss: 1.8448101878166199\n",
      "          vf_explained_var: 0.45341166853904724\n",
      "          vf_loss: 1.9117951144774754\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.10833333333333\n",
      "    ram_util_percent: 62.17499999999999\n",
      "  pid: 5318\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: -2.300000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.1\n",
      "    policy2: -8.789999999999981\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09355566492204635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04584699667976958\n",
      "    mean_inference_ms: 2.192018072293268\n",
      "    mean_raw_obs_processing_ms: 0.23485774719529087\n",
      "  time_since_restore: 51.15239381790161\n",
      "  time_this_iter_s: 16.97127103805542\n",
      "  time_total_s: 51.15239381790161\n",
      "  timers:\n",
      "    learn_throughput: 227.337\n",
      "    learn_time_ms: 13196.244\n",
      "    sample_throughput: 780.161\n",
      "    sample_time_ms: 3845.362\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1625108234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: e0548_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>RUNNING </td><td>172.30.1.40:5318</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         51.1524</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">  -3.69 </td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>RUNNING </td><td>172.30.1.40:5317</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         37.7615</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\"> -21.575</td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               -35.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00001:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-57-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.200000000000001\n",
      "  episode_reward_mean: -29.083333333333364\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 30c1148e916449e1884fd7c5abd4d048\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.0031135548294211426\n",
      "          total_loss: 100.55957317352295\n",
      "          vf_explained_var: 0.06382351368665695\n",
      "          vf_loss: 100.5626875559489\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2185085713863373\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0346004554691414\n",
      "          policy_loss: -0.06168335722759366\n",
      "          total_loss: 4.693903595209122\n",
      "          vf_explained_var: 0.3550139367580414\n",
      "          vf_loss: 4.74001677831014\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.68387096774194\n",
      "    ram_util_percent: 62.083870967741916\n",
      "  pid: 5317\n",
      "  policy_reward_max:\n",
      "    policy1: 18.0\n",
      "    policy2: 3.199999999999995\n",
      "  policy_reward_mean:\n",
      "    policy1: -21.65\n",
      "    policy2: -7.433333333333319\n",
      "  policy_reward_min:\n",
      "    policy1: -42.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09469259471654719\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0479456366044769\n",
      "    mean_inference_ms: 2.232087233705698\n",
      "    mean_raw_obs_processing_ms: 0.2391687613274048\n",
      "  time_since_restore: 59.233580112457275\n",
      "  time_this_iter_s: 21.472033977508545\n",
      "  time_total_s: 59.233580112457275\n",
      "  timers:\n",
      "    learn_throughput: 190.546\n",
      "    learn_time_ms: 15744.248\n",
      "    sample_throughput: 751.773\n",
      "    sample_time_ms: 3990.565\n",
      "    update_time_ms: 3.152\n",
      "  timestamp: 1625108243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: e0548_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>RUNNING </td><td>172.30.1.40:5318</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         51.1524</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\"> -3.69  </td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>RUNNING </td><td>172.30.1.40:5317</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         59.2336</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-29.0833</td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-57-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999996\n",
      "  episode_reward_mean: -1.604999999999989\n",
      "  episode_reward_min: -37.50000000000004\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 109393779bf04cd2a072197ec47e0f68\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.2709183792273204\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021722817871098716\n",
      "          policy_loss: -0.0721723132301122\n",
      "          total_loss: 29.253071626027424\n",
      "          vf_explained_var: 0.2923828065395355\n",
      "          vf_loss: 29.31058120727539\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.228888933857282\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025869210561116535\n",
      "          policy_loss: -0.07550248814125855\n",
      "          total_loss: 1.6480840494235356\n",
      "          vf_explained_var: 0.4637688100337982\n",
      "          vf_loss: 1.706124797463417\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.686363636363645\n",
      "    ram_util_percent: 62.01818181818181\n",
      "  pid: 5318\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: -2.300000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.965\n",
      "    policy2: -8.569999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09347876668689425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.045770248691130354\n",
      "    mean_inference_ms: 2.1880024457444267\n",
      "    mean_raw_obs_processing_ms: 0.23294683177617664\n",
      "  time_since_restore: 66.61125802993774\n",
      "  time_this_iter_s: 15.458864212036133\n",
      "  time_total_s: 66.61125802993774\n",
      "  timers:\n",
      "    learn_throughput: 234.13\n",
      "    learn_time_ms: 12813.389\n",
      "    sample_throughput: 783.28\n",
      "    sample_time_ms: 3830.049\n",
      "    update_time_ms: 2.42\n",
      "  timestamp: 1625108250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: e0548_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>RUNNING </td><td>172.30.1.40:5318</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         66.6113</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -1.605 </td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>RUNNING </td><td>172.30.1.40:5317</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         59.2336</td><td style=\"text-align: right;\"> 9000</td><td style=\"text-align: right;\">-29.0833</td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-57-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.000000000000019\n",
      "  episode_reward_mean: -35.76000000000003\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 30c1148e916449e1884fd7c5abd4d048\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.225\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.0036503668719281754\n",
      "          total_loss: 97.39595603942871\n",
      "          vf_explained_var: 0.050319839268922806\n",
      "          vf_loss: 97.39960606892903\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.1506167352199554\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022369494816909235\n",
      "          policy_loss: -0.05275215804188823\n",
      "          total_loss: 14.885810534159342\n",
      "          vf_explained_var: 0.19328035414218903\n",
      "          vf_loss: 14.923463066418966\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.96551724137931\n",
      "    ram_util_percent: 61.9551724137931\n",
      "  pid: 5317\n",
      "  policy_reward_max:\n",
      "    policy1: 13.0\n",
      "    policy2: 23.0\n",
      "  policy_reward_mean:\n",
      "    policy1: -32.03\n",
      "    policy2: -3.7299999999999893\n",
      "  policy_reward_min:\n",
      "    policy1: -51.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09406580736344651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04818544762217544\n",
      "    mean_inference_ms: 2.233567772686751\n",
      "    mean_raw_obs_processing_ms: 0.23786920181089372\n",
      "  time_since_restore: 79.30495810508728\n",
      "  time_this_iter_s: 20.071377992630005\n",
      "  time_total_s: 79.30495810508728\n",
      "  timers:\n",
      "    learn_throughput: 188.18\n",
      "    learn_time_ms: 15942.215\n",
      "    sample_throughput: 774.175\n",
      "    sample_time_ms: 3875.092\n",
      "    update_time_ms: 2.852\n",
      "  timestamp: 1625108263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: e0548_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>RUNNING </td><td>172.30.1.40:5318</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         66.6113</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -1.605</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>RUNNING </td><td>172.30.1.40:5317</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         79.305 </td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -35.76 </td><td style=\"text-align: right;\">                 3  </td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-57-46\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.599999999999966\n",
      "  episode_reward_mean: -0.6179999999999874\n",
      "  episode_reward_min: -22.50000000000003\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 109393779bf04cd2a072197ec47e0f68\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.258529340227445\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01582040269083033\n",
      "          policy_loss: -0.06465477589517832\n",
      "          total_loss: 20.755126516024273\n",
      "          vf_explained_var: 0.4199785888195038\n",
      "          vf_loss: 20.803763031959534\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2010985016822815\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01911787238592903\n",
      "          policy_loss: -0.06215822913994392\n",
      "          total_loss: 1.8530896206696827\n",
      "          vf_explained_var: 0.4000462293624878\n",
      "          vf_loss: 1.8958909561236699\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.3375\n",
      "    ram_util_percent: 61.95416666666667\n",
      "  pid: 5318\n",
      "  policy_reward_max:\n",
      "    policy1: 30.5\n",
      "    policy2: -2.2999999999999936\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.71\n",
      "    policy2: -8.327999999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -12.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09189495487410337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0452460532971432\n",
      "    mean_inference_ms: 2.1539229854017132\n",
      "    mean_raw_obs_processing_ms: 0.2287642175647667\n",
      "  time_since_restore: 82.87230777740479\n",
      "  time_this_iter_s: 16.26104974746704\n",
      "  time_total_s: 82.87230777740479\n",
      "  timers:\n",
      "    learn_throughput: 233.65\n",
      "    learn_time_ms: 12839.695\n",
      "    sample_throughput: 805.319\n",
      "    sample_time_ms: 3725.23\n",
      "    update_time_ms: 2.553\n",
      "  timestamp: 1625108266\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: e0548_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20990)\u001b[0m 2021-07-01 11:57:55,822\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=20990)\u001b[0m 2021-07-01 11:58:02,928\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00001:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-58-03\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -24.30000000000001\n",
      "  episode_reward_mean: -38.703000000000046\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 30c1148e916449e1884fd7c5abd4d048\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1125\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: 0.0017244585712129872\n",
      "          total_loss: 109.28126271565755\n",
      "          vf_explained_var: 0.10310747474431992\n",
      "          vf_loss: 109.27954006195068\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.0966935753822327\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015106024763857325\n",
      "          policy_loss: -0.04396301585560044\n",
      "          total_loss: 23.824143171310425\n",
      "          vf_explained_var: 0.19264298677444458\n",
      "          vf_loss: 23.852811177571613\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.80000000000001\n",
      "    ram_util_percent: 61.29310344827587\n",
      "  pid: 5317\n",
      "  policy_reward_max:\n",
      "    policy1: -17.0\n",
      "    policy2: 30.699999999999974\n",
      "  policy_reward_mean:\n",
      "    policy1: -40.715\n",
      "    policy2: 2.0120000000000084\n",
      "  policy_reward_min:\n",
      "    policy1: -55.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09331842174711154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.048189057067958875\n",
      "    mean_inference_ms: 2.223013698218882\n",
      "    mean_raw_obs_processing_ms: 0.23651214003653764\n",
      "  time_since_restore: 99.97380185127258\n",
      "  time_this_iter_s: 20.668843746185303\n",
      "  time_total_s: 99.97380185127258\n",
      "  timers:\n",
      "    learn_throughput: 186.609\n",
      "    learn_time_ms: 16076.425\n",
      "    sample_throughput: 767.372\n",
      "    sample_time_ms: 3909.444\n",
      "    update_time_ms: 2.87\n",
      "  timestamp: 1625108283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: e0548_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (1 PENDING, 2 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>RUNNING   </td><td>172.30.1.40:5317</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         99.9738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -38.703</td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>RUNNING   </td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>PENDING   </td><td>                </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8723</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">  -0.618</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=21614)\u001b[0m 2021-07-01 11:58:12,422\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=21614)\u001b[0m 2021-07-01 11:58:22,024\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-58-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.000000000000023\n",
      "  episode_reward_mean: -9.899999999999995\n",
      "  episode_reward_min: -35.40000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 165b12bdda7d4a6ba79c5cf4f1f16b99\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.3582803755998611\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028701205796096474\n",
      "          policy_loss: -0.059343935987271834\n",
      "          total_loss: 34.44962179660797\n",
      "          vf_explained_var: 0.11263073980808258\n",
      "          vf_loss: 34.5032262802124\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.3488942198455334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03857875464018434\n",
      "          policy_loss: -0.07234835260896944\n",
      "          total_loss: 1.975800510495901\n",
      "          vf_explained_var: 0.42328837513923645\n",
      "          vf_loss: 2.040433093905449\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.43333333333333\n",
      "    ram_util_percent: 62.13030303030303\n",
      "  pid: 20990\n",
      "  policy_reward_max:\n",
      "    policy1: 22.0\n",
      "    policy2: -3.400000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: -0.5875\n",
      "    policy2: -9.312499999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08451015218861517\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042059313113066266\n",
      "    mean_inference_ms: 1.958930808147867\n",
      "    mean_raw_obs_processing_ms: 0.20800066971290354\n",
      "  time_since_restore: 22.915868043899536\n",
      "  time_this_iter_s: 22.915868043899536\n",
      "  time_total_s: 22.915868043899536\n",
      "  timers:\n",
      "    learn_throughput: 227.314\n",
      "    learn_time_ms: 17596.809\n",
      "    sample_throughput: 755.894\n",
      "    sample_time_ms: 5291.749\n",
      "    update_time_ms: 5.396\n",
      "  timestamp: 1625108305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: e0548_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>RUNNING   </td><td>172.30.1.40:20990</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         22.9159</td><td style=\"text-align: right;\"> 4000</td><td style=\"text-align: right;\">  -9.9  </td><td style=\"text-align: right;\">                12  </td><td style=\"text-align: right;\">               -35.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>RUNNING   </td><td>                 </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8723</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">  -0.618</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         99.9738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -38.703</td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-58-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.700000000000019\n",
      "  episode_reward_mean: -7.424999999999992\n",
      "  episode_reward_min: -34.50000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: a5ea5be08c514c6a8d524ac08f699f01\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.06490051870840502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: .inf\n",
      "          policy_loss: 0.4366196487098932\n",
      "          total_loss: .inf\n",
      "          vf_explained_var: -0.060224324464797974\n",
      "          vf_loss: 50.42529737949371\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.3498334139585495\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.037792466464452446\n",
      "          policy_loss: -0.06785095878876746\n",
      "          total_loss: 1.7302709370851517\n",
      "          vf_explained_var: 0.4352650046348572\n",
      "          vf_loss: 1.7905634380877018\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.79444444444445\n",
      "    ram_util_percent: 61.25555555555555\n",
      "  pid: 21614\n",
      "  policy_reward_max:\n",
      "    policy1: 19.5\n",
      "    policy2: -6.699999999999995\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.1625\n",
      "    policy2: -9.58749999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09699632739019895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.048689935160898555\n",
      "    mean_inference_ms: 2.2936557186418387\n",
      "    mean_raw_obs_processing_ms: 0.2402656379787401\n",
      "  time_since_restore: 24.75477910041809\n",
      "  time_this_iter_s: 24.75477910041809\n",
      "  time_total_s: 24.75477910041809\n",
      "  timers:\n",
      "    learn_throughput: 207.821\n",
      "    learn_time_ms: 19247.327\n",
      "    sample_throughput: 727.91\n",
      "    sample_time_ms: 5495.184\n",
      "    update_time_ms: 2.54\n",
      "  timestamp: 1625108326\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: e0548_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>RUNNING   </td><td>172.30.1.40:20990</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         22.9159</td><td style=\"text-align: right;\"> 4000</td><td style=\"text-align: right;\">  -9.9  </td><td style=\"text-align: right;\">                12  </td><td style=\"text-align: right;\">               -35.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>RUNNING   </td><td>172.30.1.40:21614</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         24.7548</td><td style=\"text-align: right;\"> 4000</td><td style=\"text-align: right;\">  -7.425</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8723</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">  -0.618</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         99.9738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -38.703</td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000021\n",
      "  episode_reward_mean: -6.978749999999991\n",
      "  episode_reward_min: -35.40000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 165b12bdda7d4a6ba79c5cf4f1f16b99\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.3225731179118156\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029981161875184625\n",
      "          policy_loss: -0.06853678141487762\n",
      "          total_loss: 33.906187415122986\n",
      "          vf_explained_var: 0.1982439011335373\n",
      "          vf_loss: 33.9657301902771\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.3087973892688751\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03621761919930577\n",
      "          policy_loss: -0.076551110483706\n",
      "          total_loss: 2.056877516210079\n",
      "          vf_explained_var: 0.4060567021369934\n",
      "          vf_loss: 2.122563362121582\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.487500000000004\n",
      "    ram_util_percent: 60.900000000000006\n",
      "  pid: 20990\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -3.399999999999985\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.89375\n",
      "    policy2: -8.872499999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08782711003818369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04347014237852856\n",
      "    mean_inference_ms: 2.040240567038449\n",
      "    mean_raw_obs_processing_ms: 0.2158973069215715\n",
      "  time_since_restore: 45.36697196960449\n",
      "  time_this_iter_s: 22.451103925704956\n",
      "  time_total_s: 45.36697196960449\n",
      "  timers:\n",
      "    learn_throughput: 231.185\n",
      "    learn_time_ms: 17302.123\n",
      "    sample_throughput: 745.861\n",
      "    sample_time_ms: 5362.93\n",
      "    update_time_ms: 3.723\n",
      "  timestamp: 1625108328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: e0548_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0548_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-59-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.19999999999999\n",
      "  episode_reward_mean: -4.889999999999994\n",
      "  episode_reward_min: -30.00000000000002\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 165b12bdda7d4a6ba79c5cf4f1f16b99\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.293636180460453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023636360361706465\n",
      "          policy_loss: -0.06728518143063411\n",
      "          total_loss: 31.328189074993134\n",
      "          vf_explained_var: 0.25122109055519104\n",
      "          vf_loss: 31.384837925434113\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.285410724580288\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02820841653738171\n",
      "          policy_loss: -0.06863338357652538\n",
      "          total_loss: 2.6299745813012123\n",
      "          vf_explained_var: 0.3561617136001587\n",
      "          vf_loss: 2.6859141886234283\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.3551724137931\n",
      "    ram_util_percent: 61.38965517241381\n",
      "  pid: 20990\n",
      "  policy_reward_max:\n",
      "    policy1: 27.0\n",
      "    policy2: -0.10000000000000275\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.57\n",
      "    policy2: -8.459999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -21.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09045342321284396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04431593001367475\n",
      "    mean_inference_ms: 2.099804906698706\n",
      "    mean_raw_obs_processing_ms: 0.22099894431726333\n",
      "  time_since_restore: 65.90464496612549\n",
      "  time_this_iter_s: 20.537672996520996\n",
      "  time_total_s: 65.90464496612549\n",
      "  timers:\n",
      "    learn_throughput: 240.507\n",
      "    learn_time_ms: 16631.526\n",
      "    sample_throughput: 751.613\n",
      "    sample_time_ms: 5321.885\n",
      "    update_time_ms: 3.176\n",
      "  timestamp: 1625108348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: e0548_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>RUNNING   </td><td>172.30.1.40:20990</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         65.9046</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -4.89 </td><td style=\"text-align: right;\">                19.2</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>RUNNING   </td><td>172.30.1.40:21614</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         24.7548</td><td style=\"text-align: right;\"> 4000</td><td style=\"text-align: right;\">  -7.425</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8723</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">  -0.618</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         99.9738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -38.703</td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00003:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-59-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.700000000000019\n",
      "  episode_reward_mean: -29.692500000000035\n",
      "  episode_reward_min: -58.50000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: a5ea5be08c514c6a8d524ac08f699f01\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.03847502931260749\n",
      "          entropy_coeff: 0.0\n",
      "          kl: .inf\n",
      "          policy_loss: 0.11948691852740012\n",
      "          total_loss: .inf\n",
      "          vf_explained_var: -0.054587654769420624\n",
      "          vf_loss: 147.67479872703552\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.3244993463158607\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02826323953922838\n",
      "          policy_loss: -0.053914743184577674\n",
      "          total_loss: 2.1365653090178967\n",
      "          vf_explained_var: 0.3681047558784485\n",
      "          vf_loss: 2.1820010654628277\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.31818181818182\n",
      "    ram_util_percent: 61.38181818181818\n",
      "  pid: 21614\n",
      "  policy_reward_max:\n",
      "    policy1: 19.5\n",
      "    policy2: -4.500000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: -20.05\n",
      "    policy2: -9.642499999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -49.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09646916168733118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04828580406093487\n",
      "    mean_inference_ms: 2.288745666159378\n",
      "    mean_raw_obs_processing_ms: 0.23887829174174166\n",
      "  time_since_restore: 48.391234159469604\n",
      "  time_this_iter_s: 23.636455059051514\n",
      "  time_total_s: 48.391234159469604\n",
      "  timers:\n",
      "    learn_throughput: 213.509\n",
      "    learn_time_ms: 18734.557\n",
      "    sample_throughput: 733.83\n",
      "    sample_time_ms: 5450.855\n",
      "    update_time_ms: 2.257\n",
      "  timestamp: 1625108350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: e0548_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0548_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-59-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999964\n",
      "  episode_reward_mean: -2.3939999999999926\n",
      "  episode_reward_min: -30.00000000000002\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 165b12bdda7d4a6ba79c5cf4f1f16b99\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.2734674774110317\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01903907099040225\n",
      "          policy_loss: -0.059093777352245525\n",
      "          total_loss: 24.125164330005646\n",
      "          vf_explained_var: 0.3338729739189148\n",
      "          vf_loss: 24.171406745910645\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2636818550527096\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022577459807507694\n",
      "          policy_loss: -0.05941556964535266\n",
      "          total_loss: 2.2819810584187508\n",
      "          vf_explained_var: 0.40063759684562683\n",
      "          vf_loss: 2.3261568918824196\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.658620689655166\n",
      "    ram_util_percent: 61.89310344827586\n",
      "  pid: 20990\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 1.000000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.67\n",
      "    policy2: -8.063999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -20.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09178960848973433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04504546149066679\n",
      "    mean_inference_ms: 2.1375093211753198\n",
      "    mean_raw_obs_processing_ms: 0.2238510925381401\n",
      "  time_since_restore: 85.71413207054138\n",
      "  time_this_iter_s: 19.809487104415894\n",
      "  time_total_s: 85.71413207054138\n",
      "  timers:\n",
      "    learn_throughput: 247.206\n",
      "    learn_time_ms: 16180.841\n",
      "    sample_throughput: 764.151\n",
      "    sample_time_ms: 5234.57\n",
      "    update_time_ms: 2.883\n",
      "  timestamp: 1625108368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: e0548_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>RUNNING   </td><td>172.30.1.40:20990</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         85.7141</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -2.394 </td><td style=\"text-align: right;\">                19.5</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>RUNNING   </td><td>172.30.1.40:21614</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         48.3912</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-29.6925</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8723</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -0.618 </td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         99.9738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-38.703 </td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00003:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-59-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.700000000000019\n",
      "  episode_reward_mean: -45.675000000000075\n",
      "  episode_reward_min: -60.0000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: a5ea5be08c514c6a8d524ac08f699f01\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: .inf\n",
      "          policy_loss: 0.003162487060762942\n",
      "          total_loss: .inf\n",
      "          vf_explained_var: 0.09282225370407104\n",
      "          vf_loss: 88.48809289932251\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.3065645135939121\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022434738755691797\n",
      "          policy_loss: -0.03076306331786327\n",
      "          total_loss: 2.4649734273552895\n",
      "          vf_explained_var: 0.23755189776420593\n",
      "          vf_loss: 2.4856408536434174\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.64594594594595\n",
      "    ram_util_percent: 61.95675675675675\n",
      "  pid: 21614\n",
      "  policy_reward_max:\n",
      "    policy1: 19.5\n",
      "    policy2: -4.500000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: -35.895\n",
      "    policy2: -9.779999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -50.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09498024723005209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04757623589427558\n",
      "    mean_inference_ms: 2.255425812525371\n",
      "    mean_raw_obs_processing_ms: 0.23532063043893492\n",
      "  time_since_restore: 74.16555595397949\n",
      "  time_this_iter_s: 25.774321794509888\n",
      "  time_total_s: 74.16555595397949\n",
      "  timers:\n",
      "    learn_throughput: 205.931\n",
      "    learn_time_ms: 19423.951\n",
      "    sample_throughput: 756.426\n",
      "    sample_time_ms: 5288.026\n",
      "    update_time_ms: 2.203\n",
      "  timestamp: 1625108376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: e0548_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>RUNNING   </td><td>172.30.1.40:20990</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         85.7141</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  -2.394</td><td style=\"text-align: right;\">                19.5</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>RUNNING   </td><td>172.30.1.40:21614</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         74.1656</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -45.675</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               -60  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8723</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">  -0.618</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         99.9738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -38.703</td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_11-59-48\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.999999999999925\n",
      "  episode_reward_mean: 1.35300000000001\n",
      "  episode_reward_min: -18.299999999999983\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 165b12bdda7d4a6ba79c5cf4f1f16b99\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.2573885843157768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020924198208376765\n",
      "          policy_loss: -0.06594341617892496\n",
      "          total_loss: 31.756824374198914\n",
      "          vf_explained_var: 0.3943982422351837\n",
      "          vf_loss: 31.808643698692322\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2465833760797977\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016957093845121562\n",
      "          policy_loss: -0.053643331746570766\n",
      "          total_loss: 2.1821109652519226\n",
      "          vf_explained_var: 0.32917582988739014\n",
      "          vf_loss: 2.2185852341353893\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.69285714285714\n",
      "    ram_util_percent: 62.178571428571445\n",
      "  pid: 20990\n",
      "  policy_reward_max:\n",
      "    policy1: 34.0\n",
      "    policy2: 1.000000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.23\n",
      "    policy2: -7.876999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -10.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09072266266894616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04484239838835769\n",
      "    mean_inference_ms: 2.118285911428315\n",
      "    mean_raw_obs_processing_ms: 0.22111284670658182\n",
      "  time_since_restore: 105.78214883804321\n",
      "  time_this_iter_s: 20.06801676750183\n",
      "  time_total_s: 105.78214883804321\n",
      "  timers:\n",
      "    learn_throughput: 249.1\n",
      "    learn_time_ms: 16057.813\n",
      "    sample_throughput: 786.427\n",
      "    sample_time_ms: 5086.295\n",
      "    update_time_ms: 2.671\n",
      "  timestamp: 1625108388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: e0548_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>RUNNING   </td><td>172.30.1.40:20990</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        105.782 </td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.353</td><td style=\"text-align: right;\">                24  </td><td style=\"text-align: right;\">               -18.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>RUNNING   </td><td>172.30.1.40:21614</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         74.1656</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -45.675</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               -60  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8723</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">  -0.618</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         99.9738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -38.703</td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00003:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-00-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.90000000000006\n",
      "  episode_reward_mean: -58.479000000000106\n",
      "  episode_reward_min: -60.0000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: a5ea5be08c514c6a8d524ac08f699f01\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: 0.0031935407605487853\n",
      "          total_loss: 99.45838046073914\n",
      "          vf_explained_var: 0.11185324937105179\n",
      "          vf_loss: 99.45518708229065\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2973405234515667\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017603624670300633\n",
      "          policy_loss: -0.043790820927824825\n",
      "          total_loss: 2.275177426636219\n",
      "          vf_explained_var: 0.29466643929481506\n",
      "          vf_loss: 2.307085771113634\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.00857142857143\n",
      "    ram_util_percent: 61.18857142857143\n",
      "  pid: 21614\n",
      "  policy_reward_max:\n",
      "    policy1: -36.5\n",
      "    policy2: -8.89999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: -48.49\n",
      "    policy2: -9.988999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -50.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09229712284346527\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.046497487050257205\n",
      "    mean_inference_ms: 2.198217232310314\n",
      "    mean_raw_obs_processing_ms: 0.22874387372379246\n",
      "  time_since_restore: 98.72675514221191\n",
      "  time_this_iter_s: 24.561199188232422\n",
      "  time_total_s: 98.72675514221191\n",
      "  timers:\n",
      "    learn_throughput: 204.465\n",
      "    learn_time_ms: 19563.295\n",
      "    sample_throughput: 782.987\n",
      "    sample_time_ms: 5108.638\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1625108400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: e0548_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>RUNNING   </td><td>172.30.1.40:21614</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         98.7268</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -58.479</td><td style=\"text-align: right;\">               -45.9</td><td style=\"text-align: right;\">               -60  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8723</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">  -0.618</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         99.9738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -38.703</td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        105.782 </td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.353</td><td style=\"text-align: right;\">                24  </td><td style=\"text-align: right;\">               -18.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0548_00003:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-00-24\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -58.50000000000009\n",
      "  episode_reward_mean: -59.9250000000001\n",
      "  episode_reward_min: -60.0000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: a5ea5be08c514c6a8d524ac08f699f01\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3375\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.007534682372352108\n",
      "          total_loss: 150.69444930553436\n",
      "          vf_explained_var: 0.10564549267292023\n",
      "          vf_loss: 150.70198106765747\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2682304754853249\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015698419068939984\n",
      "          policy_loss: -0.035477222729241475\n",
      "          total_loss: 2.247704055160284\n",
      "          vf_explained_var: 0.3029800057411194\n",
      "          vf_loss: 2.2725848630070686\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.793939393939393\n",
      "    ram_util_percent: 60.74545454545454\n",
      "  pid: 21614\n",
      "  policy_reward_max:\n",
      "    policy1: -48.5\n",
      "    policy2: -9.99999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: -49.925\n",
      "    policy2: -9.999999999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -50.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08794346195269309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044622357991125\n",
      "    mean_inference_ms: 2.097750640730131\n",
      "    mean_raw_obs_processing_ms: 0.21798001794056554\n",
      "  time_since_restore: 122.00344514846802\n",
      "  time_this_iter_s: 23.276690006256104\n",
      "  time_total_s: 122.00344514846802\n",
      "  timers:\n",
      "    learn_throughput: 203.705\n",
      "    learn_time_ms: 19636.244\n",
      "    sample_throughput: 841.173\n",
      "    sample_time_ms: 4755.266\n",
      "    update_time_ms: 2.12\n",
      "  timestamp: 1625108424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: e0548_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>RUNNING   </td><td>172.30.1.40:21614</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        122.003 </td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -59.925</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               -60  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8723</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">  -0.618</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         99.9738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -38.703</td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        105.782 </td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.353</td><td style=\"text-align: right;\">                24  </td><td style=\"text-align: right;\">               -18.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 4/4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8723</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">  -0.618</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         99.9738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -38.703</td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        105.782 </td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.353</td><td style=\"text-align: right;\">                24  </td><td style=\"text-align: right;\">               -18.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0548_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        122.003 </td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -59.925</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               -60  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-01 12:00:24,393\tINFO tune.py:549 -- Total run time: 259.64 seconds (259.40 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f81a5d869a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs on a cluster.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Running stuff with tune, we can re-use the exact\n",
    "# same config that we used when working with RLlib directly!\n",
    "tune_config = config.copy()\n",
    "\n",
    "# Let's add our first hyperparameter search via our config.\n",
    "# How about we try two different learning rates? Let's say 0.00005 and 0.5 (ouch!).\n",
    "tune_config[\"lr\"] = tune.grid_search([0.0001, 0.5])  # <- 0.5? again: ouch!\n",
    "tune_config[\"train_batch_size\"] = tune.grid_search([3000, 4000])\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "# Tune will stop the run, once any single one of the criteria is matched (not all of them!).\n",
    "stop = {\n",
    "    # Note that the keys used here can be anything present in the above `rllib_trainer.train()` output dict.\n",
    "    \"training_iteration\": 5,\n",
    "    \"episode_reward_mean\": 20.0,\n",
    "}\n",
    "\n",
    "# \"PPO\" is a registered name that points to RLlib's PPOTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "\n",
    "# Run a simple experiment until one of the stopping criteria is met.\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=tune_config,\n",
    "    stop=stop,\n",
    "\n",
    "    # Note that no trainers will be returned from this call here.\n",
    "    # Tune will create n Trainers internally, run them in parallel and destroy them at the end.\n",
    "    # However, you can ...\n",
    "    checkpoint_at_end=True,  # ... create a checkpoint when done.\n",
    "    checkpoint_freq=10,  # ... create a checkpoint every 10 training iterations.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b886fb8-6ccd-4be2-80bb-fc0936808d11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why did we use 6 CPUs in the tune run above (3 CPUs per trial)?\n",
    "\n",
    "PPO-기본적으로 2개의 \"롤아웃\" 작업자(`num_workers = 2`)를 사용합니다. 이들은 자체 환경 카피를 갖고 병렬로 단계를 진행하는 레이 액터입니다. 이 두 \"롤아웃\" 작업자 외에 RLlib의 모든 Trainer에는 항상 \"로컬\" 작업자가 있으며, PPO의 경우 학습 업데이트를 처리합니다. 이를 통해 3개의 작업자 (2 개의 롤아웃 + 1 개의 로컬 학습자)가 제공되며 3 개의 CPU가 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a74ec7-a6c1-431d-83aa-35df56d93185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise No 2\n",
    "\n",
    "<hr />\n",
    "\n",
    "Using the `tune_config` that we have built so far, let's run another `tune.run()`, but apply the following changes to our setup this time:\n",
    "- Setup only 1 learning rate under the \"lr\" config key. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Setup only 1 train batch size under the \"train_batch_size\" config key. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Set `num_workers` to 5, which will allow us to run more environment \"rollouts\" in parallel and to collect training batches more quickly.\n",
    "- Set the `num_envs_per_worker` config parameter to 5. This will clone our env on each rollout worker, and thus parallelize action computing forward passes through our neural networks.\n",
    "\n",
    "Other than that, use the exact same args as in our `tune.run()` call in the previous cell.\n",
    "\n",
    "<hr />\n",
    "\n",
    "지금까지 빌드한 `tune_config`를 사용하여 다른 `tune.run()`을 실행하되 이번에는 설정에 다음 변경 사항을 적용합니다.\n",
    "- \"lr\" 구성 키 아래에 1 개의 학습률만 설정합니다. 이전 셀 (가장 높은 평균 보상을 산출 한 셀)의 실행에서 (겉보기에) 가장 좋은 값을 선택합니다.\n",
    "- \"train_batch_size\" 구성 키 아래에 기차 배치 크기를 1 만 설정합니다. 이전 셀 (가장 높은 평균 보상을 산출 한 셀)의 실행에서 (겉보기에) 가장 좋은 값을 선택합니다.\n",
    "- `num_workers`를 5로 설정하면 더 많은 환경 \"롤아웃\"을 병렬로 실행하고 훈련 배치를 더 빨리 수집 할 수 있습니다.\n",
    "- `num_envs_per_worker` 구성 매개 변수를 5로 설정합니다. 그러면 각 롤아웃 작업자에서 env가 복제되므로 신경망을 통해 전달되는 액션 컴퓨팅이 병렬화됩니다.\n",
    "\n",
    "그 외에는 이전 셀의`tune.run()`호출에서와 똑같은 인수를 사용하십시오.\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff184330-4229-4476-a9e0-1fdbaed948d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=21670)\u001b[0m 2021-07-01 12:14:57,619\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=21670)\u001b[0m 2021-07-01 12:14:57,620\tWARNING ppo.py:135 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=5 num_envs_per_worker=5 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 160.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=21670)\u001b[0m 2021-07-01 12:15:08,737\tINFO trainable.py:101 -- Trainable.setup took 11.119 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=21670)\u001b[0m 2021-07-01 12:15:08,738\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-15-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.100000000000028\n",
      "  episode_reward_mean: -6.767999999999992\n",
      "  episode_reward_min: -31.50000000000002\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 25\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.3568887673318386\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03012266621226445\n",
      "          policy_loss: -0.06361623853445053\n",
      "          total_loss: 29.52478688955307\n",
      "          vf_explained_var: 0.08373932540416718\n",
      "          vf_loss: 29.582378208637238\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.348378911614418\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03935076738707721\n",
      "          policy_loss: -0.07338983699446544\n",
      "          total_loss: 2.0010338835418224\n",
      "          vf_explained_var: 0.31972336769104004\n",
      "          vf_loss: 2.0665535926818848\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.041666666666664\n",
      "    ram_util_percent: 62.37083333333334\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 17.0\n",
      "    policy2: -3.399999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.44\n",
      "    policy2: -9.207999999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -21.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2523425202932417\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1438247491113888\n",
      "    mean_inference_ms: 2.790616906207541\n",
      "    mean_raw_obs_processing_ms: 1.017029092919012\n",
      "  time_since_restore: 16.722298860549927\n",
      "  time_this_iter_s: 16.722298860549927\n",
      "  time_total_s: 16.722298860549927\n",
      "  timers:\n",
      "    learn_throughput: 250.163\n",
      "    learn_time_ms: 15989.546\n",
      "    sample_throughput: 5572.901\n",
      "    sample_time_ms: 717.759\n",
      "    update_time_ms: 2.463\n",
      "  timestamp: 1625109325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         16.7223</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -6.768</td><td style=\"text-align: right;\">                 8.1</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-15-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999973\n",
      "  episode_reward_mean: -5.49199999999999\n",
      "  episode_reward_min: -31.50000000000002\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 75\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.3113512620329857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03275543975178152\n",
      "          policy_loss: -0.07503505887871142\n",
      "          total_loss: 23.290098428726196\n",
      "          vf_explained_var: 0.2802092730998993\n",
      "          vf_loss: 23.355306833982468\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.3091180697083473\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.036320072831586\n",
      "          policy_loss: -0.07106630102498457\n",
      "          total_loss: 2.0546911768615246\n",
      "          vf_explained_var: 0.35070037841796875\n",
      "          vf_loss: 2.11486142501235\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.921739130434784\n",
      "    ram_util_percent: 62.08260869565217\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: -3.399999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.32\n",
      "    policy2: -8.811999999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -21.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.25648422052005215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1453148997534357\n",
      "    mean_inference_ms: 2.814413752102239\n",
      "    mean_raw_obs_processing_ms: 1.0284125423152757\n",
      "  time_since_restore: 32.43605613708496\n",
      "  time_this_iter_s: 15.713757276535034\n",
      "  time_total_s: 32.43605613708496\n",
      "  timers:\n",
      "    learn_throughput: 258.359\n",
      "    learn_time_ms: 15482.343\n",
      "    sample_throughput: 5525.567\n",
      "    sample_time_ms: 723.908\n",
      "    update_time_ms: 2.274\n",
      "  timestamp: 1625109341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         32.4361</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">  -5.492</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-15-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999973\n",
      "  episode_reward_mean: -3.26099999999999\n",
      "  episode_reward_min: -31.50000000000002\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 100\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.2861355915665627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025299696542788297\n",
      "          policy_loss: -0.0661568658251781\n",
      "          total_loss: 22.09027224779129\n",
      "          vf_explained_var: 0.3724067211151123\n",
      "          vf_loss: 22.145043909549713\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2710602134466171\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02802169299684465\n",
      "          policy_loss: -0.06804478150297655\n",
      "          total_loss: 1.834354866296053\n",
      "          vf_explained_var: 0.340915322303772\n",
      "          vf_loss: 1.889789890497923\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.31428571428571\n",
      "    ram_util_percent: 62.27619047619047\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: -3.399999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.375\n",
      "    policy2: -8.635999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -21.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2577963017808361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14580873474278114\n",
      "    mean_inference_ms: 2.8283666171842436\n",
      "    mean_raw_obs_processing_ms: 1.0307771740672687\n",
      "  time_since_restore: 47.54720735549927\n",
      "  time_this_iter_s: 15.111151218414307\n",
      "  time_total_s: 47.54720735549927\n",
      "  timers:\n",
      "    learn_throughput: 264.687\n",
      "    learn_time_ms: 15112.21\n",
      "    sample_throughput: 5507.551\n",
      "    sample_time_ms: 726.276\n",
      "    update_time_ms: 2.279\n",
      "  timestamp: 1625109356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         47.5472</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -3.261</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-16-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999973\n",
      "  episode_reward_mean: 0.006000000000012518\n",
      "  episode_reward_min: -21.00000000000004\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 150\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.2525263242423534\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02081197005463764\n",
      "          policy_loss: -0.06522226356901228\n",
      "          total_loss: 24.721259891986847\n",
      "          vf_explained_var: 0.37142133712768555\n",
      "          vf_loss: 24.772433876991272\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.242830853909254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021919104154221714\n",
      "          policy_loss: -0.059169401036342606\n",
      "          total_loss: 3.66197320073843\n",
      "          vf_explained_var: 0.3569755554199219\n",
      "          vf_loss: 3.706347234547138\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.68333333333334\n",
      "    ram_util_percent: 62.24166666666667\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 6.500000000000005\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.795\n",
      "    policy2: -7.788999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2617890941701883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14755312927079728\n",
      "    mean_inference_ms: 2.8657365005269746\n",
      "    mean_raw_obs_processing_ms: 1.0432220444260576\n",
      "  time_since_restore: 64.10338139533997\n",
      "  time_this_iter_s: 16.5561740398407\n",
      "  time_total_s: 64.10338139533997\n",
      "  timers:\n",
      "    learn_throughput: 261.63\n",
      "    learn_time_ms: 15288.747\n",
      "    sample_throughput: 5510.897\n",
      "    sample_time_ms: 725.835\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1625109372\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         64.1034</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   0.006</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">                 -21</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-16-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.000000000000018\n",
      "  episode_reward_mean: 1.2750000000000128\n",
      "  episode_reward_min: -19.499999999999986\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 200\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.2251441702246666\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01720927143469453\n",
      "          policy_loss: -0.05809747794410214\n",
      "          total_loss: 24.986693501472473\n",
      "          vf_explained_var: 0.3580786883831024\n",
      "          vf_loss: 25.027366757392883\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2260788530111313\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018364832358201966\n",
      "          policy_loss: -0.05885816866066307\n",
      "          total_loss: 2.7606728225946426\n",
      "          vf_explained_var: 0.19816341996192932\n",
      "          vf_loss: 2.8009365648031235\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.28260869565217\n",
      "    ram_util_percent: 62.34347826086957\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 31.0\n",
      "    policy2: 6.500000000000005\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.745\n",
      "    policy2: -7.469999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2629285177334304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14845990181509744\n",
      "    mean_inference_ms: 2.8755228580284053\n",
      "    mean_raw_obs_processing_ms: 1.0404134083152112\n",
      "  time_since_restore: 80.42197442054749\n",
      "  time_this_iter_s: 16.31859302520752\n",
      "  time_total_s: 80.42197442054749\n",
      "  timers:\n",
      "    learn_throughput: 260.536\n",
      "    learn_time_ms: 15352.951\n",
      "    sample_throughput: 5548.993\n",
      "    sample_time_ms: 720.852\n",
      "    update_time_ms: 2.485\n",
      "  timestamp: 1625109389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          80.422</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.275</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-16-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.499999999999925\n",
      "  episode_reward_mean: 1.8540000000000116\n",
      "  episode_reward_min: -14.69999999999999\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 225\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.2090138867497444\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016028896439820528\n",
      "          policy_loss: -0.06064255003002472\n",
      "          total_loss: 29.62217015028\n",
      "          vf_explained_var: 0.4218500256538391\n",
      "          vf_loss: 29.66658365726471\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.2001393847167492\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01732496003387496\n",
      "          policy_loss: -0.04931415824103169\n",
      "          total_loss: 3.438452772796154\n",
      "          vf_explained_var: 0.28902414441108704\n",
      "          vf_loss: 3.4702253937721252\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.14615384615385\n",
      "    ram_util_percent: 62.20384615384615\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 33.0\n",
      "    policy2: 3.200000000000009\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.17\n",
      "    policy2: -7.3159999999999865\n",
      "  policy_reward_min:\n",
      "    policy1: -8.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2648629062597317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1492779682954974\n",
      "    mean_inference_ms: 2.8935655027604583\n",
      "    mean_raw_obs_processing_ms: 1.0427016582639421\n",
      "  time_since_restore: 98.20840263366699\n",
      "  time_this_iter_s: 17.786428213119507\n",
      "  time_total_s: 98.20840263366699\n",
      "  timers:\n",
      "    learn_throughput: 256.108\n",
      "    learn_time_ms: 15618.418\n",
      "    sample_throughput: 5431.402\n",
      "    sample_time_ms: 736.458\n",
      "    update_time_ms: 2.695\n",
      "  timestamp: 1625109407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         98.2084</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   1.854</td><td style=\"text-align: right;\">                28.5</td><td style=\"text-align: right;\">               -14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-17-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.499999999999925\n",
      "  episode_reward_mean: 3.2040000000000113\n",
      "  episode_reward_min: -15.299999999999978\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 275\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.1845766603946686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01669146100175567\n",
      "          policy_loss: -0.05655819133971818\n",
      "          total_loss: 20.38133680820465\n",
      "          vf_explained_var: 0.4689350724220276\n",
      "          vf_loss: 20.420994997024536\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.1807509735226631\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01652803207980469\n",
      "          policy_loss: -0.06039647577563301\n",
      "          total_loss: 2.1545243076980114\n",
      "          vf_explained_var: 0.30730026960372925\n",
      "          vf_loss: 2.198186155408621\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.48518518518518\n",
      "    ram_util_percent: 62.22962962962964\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 33.0\n",
      "    policy2: 3.200000000000009\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.685\n",
      "    policy2: -7.480999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -8.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.27466528407509627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15370510176041927\n",
      "    mean_inference_ms: 3.003650594522901\n",
      "    mean_raw_obs_processing_ms: 1.087114094378046\n",
      "  time_since_restore: 117.38578367233276\n",
      "  time_this_iter_s: 19.17738103866577\n",
      "  time_total_s: 117.38578367233276\n",
      "  timers:\n",
      "    learn_throughput: 250.313\n",
      "    learn_time_ms: 15979.972\n",
      "    sample_throughput: 5148.657\n",
      "    sample_time_ms: 776.902\n",
      "    update_time_ms: 2.654\n",
      "  timestamp: 1625109426\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         117.386</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   3.204</td><td style=\"text-align: right;\">                28.5</td><td style=\"text-align: right;\">               -15.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.499999999999925\n",
      "  episode_reward_mean: 3.4200000000000097\n",
      "  episode_reward_min: -15.299999999999978\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 300\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.168278556317091\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015423265751451254\n",
      "          policy_loss: -0.05428478046087548\n",
      "          total_loss: 30.142649054527283\n",
      "          vf_explained_var: 0.33004117012023926\n",
      "          vf_loss: 30.18131709098816\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.160992071032524\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017098574520787224\n",
      "          policy_loss: -0.05513965553836897\n",
      "          total_loss: 2.5669192373752594\n",
      "          vf_explained_var: 0.21063627302646637\n",
      "          vf_loss: 2.604746639728546\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.75217391304348\n",
      "    ram_util_percent: 62.06956521739131\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 34.0\n",
      "    policy2: 4.30000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.78\n",
      "    policy2: -7.359999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -8.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2783391401659884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15549552552405277\n",
      "    mean_inference_ms: 3.050774753766734\n",
      "    mean_raw_obs_processing_ms: 1.1076571101699566\n",
      "  time_since_restore: 133.69289875030518\n",
      "  time_this_iter_s: 16.307115077972412\n",
      "  time_total_s: 133.69289875030518\n",
      "  timers:\n",
      "    learn_throughput: 251.082\n",
      "    learn_time_ms: 15931.031\n",
      "    sample_throughput: 5209.475\n",
      "    sample_time_ms: 767.832\n",
      "    update_time_ms: 2.842\n",
      "  timestamp: 1625109442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         133.693</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">    3.42</td><td style=\"text-align: right;\">                28.5</td><td style=\"text-align: right;\">               -15.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-17-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 36.599999999999945\n",
      "  episode_reward_mean: 5.7450000000000045\n",
      "  episode_reward_min: -14.099999999999978\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 350\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.1381555162370205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015697246795753017\n",
      "          policy_loss: -0.05019960334175266\n",
      "          total_loss: 31.474104523658752\n",
      "          vf_explained_var: 0.3831806480884552\n",
      "          vf_loss: 31.508410453796387\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.1443698592483997\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017474685271736234\n",
      "          policy_loss: -0.052579218812752515\n",
      "          total_loss: 2.390397612005472\n",
      "          vf_explained_var: 0.35046765208244324\n",
      "          vf_loss: 2.4252837151288986\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.04347826086956\n",
      "    ram_util_percent: 62.2608695652174\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 4.30000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 13.05\n",
      "    policy2: -7.304999999999989\n",
      "  policy_reward_min:\n",
      "    policy1: -8.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28552339222551004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15812267958065707\n",
      "    mean_inference_ms: 3.128740809166518\n",
      "    mean_raw_obs_processing_ms: 1.1400159482055332\n",
      "  time_since_restore: 149.9911596775055\n",
      "  time_this_iter_s: 16.298260927200317\n",
      "  time_total_s: 149.9911596775055\n",
      "  timers:\n",
      "    learn_throughput: 252.225\n",
      "    learn_time_ms: 15858.85\n",
      "    sample_throughput: 5034.649\n",
      "    sample_time_ms: 794.494\n",
      "    update_time_ms: 2.772\n",
      "  timestamp: 1625109459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         149.991</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   5.745</td><td style=\"text-align: right;\">                36.6</td><td style=\"text-align: right;\">               -14.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-17-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 36.599999999999945\n",
      "  episode_reward_mean: 8.300999999999995\n",
      "  episode_reward_min: -11.999999999999973\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 400\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.116125252097845\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016089638404082507\n",
      "          policy_loss: -0.050306459015700966\n",
      "          total_loss: 33.64773416519165\n",
      "          vf_explained_var: 0.2923678755760193\n",
      "          vf_loss: 33.68174958229065\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.1364811062812805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01656447834102437\n",
      "          policy_loss: -0.05233997156028636\n",
      "          total_loss: 3.3204622343182564\n",
      "          vf_explained_var: 0.2527543902397156\n",
      "          vf_loss: 3.3560307547450066\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.828\n",
      "    ram_util_percent: 61.864\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 10.900000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 15.705\n",
      "    policy2: -7.403999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -6.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2878379294227482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.159107748161331\n",
      "    mean_inference_ms: 3.155603961232012\n",
      "    mean_raw_obs_processing_ms: 1.1496330724390476\n",
      "  time_since_restore: 166.91136050224304\n",
      "  time_this_iter_s: 16.92020082473755\n",
      "  time_total_s: 166.91136050224304\n",
      "  timers:\n",
      "    learn_throughput: 251.644\n",
      "    learn_time_ms: 15895.479\n",
      "    sample_throughput: 5104.111\n",
      "    sample_time_ms: 783.682\n",
      "    update_time_ms: 2.743\n",
      "  timestamp: 1625109476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         166.911</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   8.301</td><td style=\"text-align: right;\">                36.6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-18-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.599999999999888\n",
      "  episode_reward_mean: 8.588999999999992\n",
      "  episode_reward_min: -11.999999999999973\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 425\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.0902166105806828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017312415409833193\n",
      "          policy_loss: -0.05841508507728577\n",
      "          total_loss: 27.08051198720932\n",
      "          vf_explained_var: 0.41637498140335083\n",
      "          vf_loss: 27.121398210525513\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.122117042541504\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015966680773999542\n",
      "          policy_loss: -0.05041519217775203\n",
      "          total_loss: 2.0159237757325172\n",
      "          vf_explained_var: 0.3730037808418274\n",
      "          vf_loss: 2.0501726642251015\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.74166666666667\n",
      "    ram_util_percent: 61.94166666666666\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: 10.900000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 16.235\n",
      "    policy2: -7.645999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -6.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28675970406735524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15805042117831886\n",
      "    mean_inference_ms: 3.1379834805744156\n",
      "    mean_raw_obs_processing_ms: 1.139728921729239\n",
      "  time_since_restore: 184.12242555618286\n",
      "  time_this_iter_s: 17.21106505393982\n",
      "  time_total_s: 184.12242555618286\n",
      "  timers:\n",
      "    learn_throughput: 250.815\n",
      "    learn_time_ms: 15948.001\n",
      "    sample_throughput: 5124.599\n",
      "    sample_time_ms: 780.549\n",
      "    update_time_ms: 2.801\n",
      "  timestamp: 1625109493\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         184.122</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   8.589</td><td style=\"text-align: right;\">                27.6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-18-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.09999999999995\n",
      "  episode_reward_mean: 10.145999999999987\n",
      "  episode_reward_min: -11.69999999999998\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 475\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.0569895319640636\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01619165571173653\n",
      "          policy_loss: -0.05843629679293372\n",
      "          total_loss: 39.908546686172485\n",
      "          vf_explained_var: 0.31136554479599\n",
      "          vf_loss: 39.950589179992676\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.1041331589221954\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01824884893721901\n",
      "          policy_loss: -0.05607768660411239\n",
      "          total_loss: 2.6027070730924606\n",
      "          vf_explained_var: 0.30271369218826294\n",
      "          vf_loss: 2.640307780355215\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.891304347826086\n",
      "    ram_util_percent: 62.05217391304346\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: 10.900000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 17.66\n",
      "    policy2: -7.513999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -7.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2862534756850889\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15859810276870717\n",
      "    mean_inference_ms: 3.1382174487914267\n",
      "    mean_raw_obs_processing_ms: 1.1389596604915218\n",
      "  time_since_restore: 200.40566849708557\n",
      "  time_this_iter_s: 16.28324294090271\n",
      "  time_total_s: 200.40566849708557\n",
      "  timers:\n",
      "    learn_throughput: 250.251\n",
      "    learn_time_ms: 15983.932\n",
      "    sample_throughput: 4995.293\n",
      "    sample_time_ms: 800.754\n",
      "    update_time_ms: 2.967\n",
      "  timestamp: 1625109509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         200.406</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  10.146</td><td style=\"text-align: right;\">                29.1</td><td style=\"text-align: right;\">               -11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-18-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.09999999999995\n",
      "  episode_reward_mean: 12.422999999999979\n",
      "  episode_reward_min: -7.799999999999984\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 500\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 1.0248748362064362\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017148243845440447\n",
      "          policy_loss: -0.058404994721058756\n",
      "          total_loss: 39.854022204875946\n",
      "          vf_explained_var: 0.3219492733478546\n",
      "          vf_loss: 39.895064771175385\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.0880036614835262\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01604377166950144\n",
      "          policy_loss: -0.049085192324128\n",
      "          total_loss: 4.0701765567064285\n",
      "          vf_explained_var: 0.2978519797325134\n",
      "          vf_loss: 4.1030173152685165\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.119230769230775\n",
      "    ram_util_percent: 62.1423076923077\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: 2.099999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 19.915\n",
      "    policy2: -7.491999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -7.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28541781318072706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1592401430074332\n",
      "    mean_inference_ms: 3.137860871149116\n",
      "    mean_raw_obs_processing_ms: 1.140649497058465\n",
      "  time_since_restore: 218.1642894744873\n",
      "  time_this_iter_s: 17.758620977401733\n",
      "  time_total_s: 218.1642894744873\n",
      "  timers:\n",
      "    learn_throughput: 246.182\n",
      "    learn_time_ms: 16248.146\n",
      "    sample_throughput: 4992.148\n",
      "    sample_time_ms: 801.258\n",
      "    update_time_ms: 2.953\n",
      "  timestamp: 1625109527\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         218.164</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  12.423</td><td style=\"text-align: right;\">                29.1</td><td style=\"text-align: right;\">                -7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-19-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.09999999999995\n",
      "  episode_reward_mean: 14.432999999999963\n",
      "  episode_reward_min: -5.3999999999999915\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 550\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.9973515067249537\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01615451308316551\n",
      "          policy_loss: -0.05109630296647083\n",
      "          total_loss: 31.19097602367401\n",
      "          vf_explained_var: 0.37627577781677246\n",
      "          vf_loss: 31.225715935230255\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.0669565834105015\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01797909798915498\n",
      "          policy_loss: -0.06255728175165132\n",
      "          total_loss: 3.007676161825657\n",
      "          vf_explained_var: 0.20673231780529022\n",
      "          vf_loss: 3.052029550075531\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.879999999999995\n",
      "    ram_util_percent: 62.05200000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 38.0\n",
      "    policy2: 12.000000000000014\n",
      "  policy_reward_mean:\n",
      "    policy1: 21.54\n",
      "    policy2: -7.106999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -7.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2873937185745601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16097910136566773\n",
      "    mean_inference_ms: 3.1613149786845747\n",
      "    mean_raw_obs_processing_ms: 1.1520943341353223\n",
      "  time_since_restore: 235.63589549064636\n",
      "  time_this_iter_s: 17.471606016159058\n",
      "  time_total_s: 235.63589549064636\n",
      "  timers:\n",
      "    learn_throughput: 245.094\n",
      "    learn_time_ms: 16320.299\n",
      "    sample_throughput: 4873.154\n",
      "    sample_time_ms: 820.824\n",
      "    update_time_ms: 2.916\n",
      "  timestamp: 1625109545\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         235.636</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  14.433</td><td style=\"text-align: right;\">                29.1</td><td style=\"text-align: right;\">                -5.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-19-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.099999999999923\n",
      "  episode_reward_mean: 15.878999999999968\n",
      "  episode_reward_min: 2.99482660892636e-14\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 600\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.9702014122158289\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015383271791506559\n",
      "          policy_loss: -0.05491414025891572\n",
      "          total_loss: 36.91341292858124\n",
      "          vf_explained_var: 0.3397338390350342\n",
      "          vf_loss: 36.95275259017944\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.0402205344289541\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015771437232615426\n",
      "          policy_loss: -0.04964466445380822\n",
      "          total_loss: 4.621232561767101\n",
      "          vf_explained_var: 0.17645229399204254\n",
      "          vf_loss: 4.654908649623394\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.22608695652173\n",
      "    ram_util_percent: 62.217391304347856\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 38.0\n",
      "    policy2: 12.000000000000014\n",
      "  policy_reward_mean:\n",
      "    policy1: 22.48\n",
      "    policy2: -6.600999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: 1.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28714026737214676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1616730833459269\n",
      "    mean_inference_ms: 3.1709805140726246\n",
      "    mean_raw_obs_processing_ms: 1.1553389234148397\n",
      "  time_since_restore: 251.82340621948242\n",
      "  time_this_iter_s: 16.18751072883606\n",
      "  time_total_s: 251.82340621948242\n",
      "  timers:\n",
      "    learn_throughput: 245.364\n",
      "    learn_time_ms: 16302.333\n",
      "    sample_throughput: 4847.961\n",
      "    sample_time_ms: 825.089\n",
      "    update_time_ms: 3.012\n",
      "  timestamp: 1625109561\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         251.823</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  15.879</td><td style=\"text-align: right;\">                29.1</td><td style=\"text-align: right;\">         2.99483e-14</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-19-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0999999999999\n",
      "  episode_reward_mean: 16.493999999999957\n",
      "  episode_reward_min: -2.999999999999973\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 625\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.9621274322271347\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014050655881874263\n",
      "          policy_loss: -0.047218539693858474\n",
      "          total_loss: 37.175280690193176\n",
      "          vf_explained_var: 0.38614946603775024\n",
      "          vf_loss: 37.20827263593674\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.042516179382801\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01578679942758754\n",
      "          policy_loss: -0.03467702434863895\n",
      "          total_loss: 3.083782583475113\n",
      "          vf_explained_var: 0.17003682255744934\n",
      "          vf_loss: 3.102475479245186\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.11923076923077\n",
      "    ram_util_percent: 62.23076923076923\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 41.0\n",
      "    policy2: 8.70000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 23.095\n",
      "    policy2: -6.6009999999999875\n",
      "  policy_reward_min:\n",
      "    policy1: 5.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29071497323452583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16291980124677413\n",
      "    mean_inference_ms: 3.212311770639517\n",
      "    mean_raw_obs_processing_ms: 1.170950844414698\n",
      "  time_since_restore: 269.8802032470703\n",
      "  time_this_iter_s: 18.05679702758789\n",
      "  time_total_s: 269.8802032470703\n",
      "  timers:\n",
      "    learn_throughput: 245.922\n",
      "    learn_time_ms: 16265.338\n",
      "    sample_throughput: 4489.806\n",
      "    sample_time_ms: 890.907\n",
      "    update_time_ms: 2.882\n",
      "  timestamp: 1625109579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">          269.88</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  16.494</td><td style=\"text-align: right;\">                32.1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-19-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.6999999999999\n",
      "  episode_reward_mean: 18.242999999999952\n",
      "  episode_reward_min: -2.999999999999973\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 675\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.9346323776990175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01375378860393539\n",
      "          policy_loss: -0.0499732798198238\n",
      "          total_loss: 29.570641458034515\n",
      "          vf_explained_var: 0.42696326971054077\n",
      "          vf_loss: 29.606688916683197\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 1.0121796373277903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014833728200756013\n",
      "          policy_loss: -0.05274756799917668\n",
      "          total_loss: 5.540087565779686\n",
      "          vf_explained_var: 0.1390731930732727\n",
      "          vf_loss: 5.577815994620323\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.544000000000004\n",
      "    ram_util_percent: 62.20399999999999\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 41.0\n",
      "    policy2: 12.000000000000016\n",
      "  policy_reward_mean:\n",
      "    policy1: 24.085\n",
      "    policy2: -5.841999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: 5.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2966539648056558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16542227321678138\n",
      "    mean_inference_ms: 3.2942934673021202\n",
      "    mean_raw_obs_processing_ms: 1.2029230768286423\n",
      "  time_since_restore: 287.62244844436646\n",
      "  time_this_iter_s: 17.742245197296143\n",
      "  time_total_s: 287.62244844436646\n",
      "  timers:\n",
      "    learn_throughput: 247.79\n",
      "    learn_time_ms: 16142.7\n",
      "    sample_throughput: 4598.125\n",
      "    sample_time_ms: 869.92\n",
      "    update_time_ms: 2.892\n",
      "  timestamp: 1625109597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         287.622</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  18.243</td><td style=\"text-align: right;\">                32.7</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-20-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.6999999999999\n",
      "  episode_reward_mean: 18.926999999999946\n",
      "  episode_reward_min: -2.999999999999973\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 700\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.9122906383126974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013836637837812304\n",
      "          policy_loss: -0.043922056909650564\n",
      "          total_loss: 59.79106044769287\n",
      "          vf_explained_var: 0.24767276644706726\n",
      "          vf_loss: 59.82097339630127\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.9547248724848032\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012564581498736516\n",
      "          policy_loss: -0.03633102570893243\n",
      "          total_loss: 20.17772427201271\n",
      "          vf_explained_var: 0.21614736318588257\n",
      "          vf_loss: 20.201333954930305\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.964\n",
      "    ram_util_percent: 62.128\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 41.0\n",
      "    policy2: 35.10000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 23.9\n",
      "    policy2: -4.972999999999989\n",
      "  policy_reward_min:\n",
      "    policy1: -21.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30015551670258667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16734191987441493\n",
      "    mean_inference_ms: 3.3498084733413567\n",
      "    mean_raw_obs_processing_ms: 1.2252462367036057\n",
      "  time_since_restore: 305.3126504421234\n",
      "  time_this_iter_s: 17.690201997756958\n",
      "  time_total_s: 305.3126504421234\n",
      "  timers:\n",
      "    learn_throughput: 246.262\n",
      "    learn_time_ms: 16242.843\n",
      "    sample_throughput: 4406.255\n",
      "    sample_time_ms: 907.8\n",
      "    update_time_ms: 2.718\n",
      "  timestamp: 1625109614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         305.313</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  18.927</td><td style=\"text-align: right;\">                32.7</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-20-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.4999999999999\n",
      "  episode_reward_mean: 19.904999999999944\n",
      "  episode_reward_min: 3.2999999999999394\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 750\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.9015404060482979\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013370373228099197\n",
      "          policy_loss: -0.043550039059482515\n",
      "          total_loss: 38.45504206418991\n",
      "          vf_explained_var: 0.38878071308135986\n",
      "          vf_loss: 38.48505401611328\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.9642647467553616\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013386637438088655\n",
      "          policy_loss: -0.04056001373101026\n",
      "          total_loss: 10.4579236805439\n",
      "          vf_explained_var: 0.18228058516979218\n",
      "          vf_loss: 10.48492956161499\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.950000000000003\n",
      "    ram_util_percent: 62.21818181818182\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 44.5\n",
      "    policy2: 46.09999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 23.69\n",
      "    policy2: -3.784999999999991\n",
      "  policy_reward_min:\n",
      "    policy1: -29.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30443096761096355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16911354756290797\n",
      "    mean_inference_ms: 3.3949806250991914\n",
      "    mean_raw_obs_processing_ms: 1.2451271771669463\n",
      "  time_since_restore: 321.0050857067108\n",
      "  time_this_iter_s: 15.692435264587402\n",
      "  time_total_s: 321.0050857067108\n",
      "  timers:\n",
      "    learn_throughput: 247.228\n",
      "    learn_time_ms: 16179.366\n",
      "    sample_throughput: 4391.916\n",
      "    sample_time_ms: 910.764\n",
      "    update_time_ms: 2.732\n",
      "  timestamp: 1625109630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         321.005</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  19.905</td><td style=\"text-align: right;\">                34.5</td><td style=\"text-align: right;\">                 3.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-20-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.39999999999989\n",
      "  episode_reward_mean: 20.789999999999942\n",
      "  episode_reward_min: -4.19999999999998\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 800\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.893436374142766\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015498742839554325\n",
      "          policy_loss: -0.054675333318300545\n",
      "          total_loss: 45.45559549331665\n",
      "          vf_explained_var: 0.32242220640182495\n",
      "          vf_loss: 45.49457883834839\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.9165618978440762\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012814274406991899\n",
      "          policy_loss: -0.03449971579539124\n",
      "          total_loss: 17.700226187705994\n",
      "          vf_explained_var: 0.2424086183309555\n",
      "          vf_loss: 17.72175134718418\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.0111111111111\n",
      "    ram_util_percent: 62.15555555555555\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 44.5\n",
      "    policy2: 46.09999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 23.805\n",
      "    policy2: -3.0149999999999926\n",
      "  policy_reward_min:\n",
      "    policy1: -29.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30605043107390256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16989981910962204\n",
      "    mean_inference_ms: 3.4168230826386843\n",
      "    mean_raw_obs_processing_ms: 1.2518513471782833\n",
      "  time_since_restore: 339.40694856643677\n",
      "  time_this_iter_s: 18.401862859725952\n",
      "  time_total_s: 339.40694856643677\n",
      "  timers:\n",
      "    learn_throughput: 245.061\n",
      "    learn_time_ms: 16322.437\n",
      "    sample_throughput: 4369.111\n",
      "    sample_time_ms: 915.518\n",
      "    update_time_ms: 2.792\n",
      "  timestamp: 1625109649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         339.407</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">   20.79</td><td style=\"text-align: right;\">                35.4</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-21-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.39999999999989\n",
      "  episode_reward_mean: 20.138999999999943\n",
      "  episode_reward_min: -14.999999999999973\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 825\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.8833394609391689\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013538038620026782\n",
      "          policy_loss: -0.03531770597328432\n",
      "          total_loss: 81.41321158409119\n",
      "          vf_explained_var: 0.31171923875808716\n",
      "          vf_loss: 81.43482375144958\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.895959883928299\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012261409225175157\n",
      "          policy_loss: -0.03439321229234338\n",
      "          total_loss: 31.940586864948273\n",
      "          vf_explained_var: 0.28092697262763977\n",
      "          vf_loss: 31.96256572008133\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.8\n",
      "    ram_util_percent: 62.065384615384616\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 41.0\n",
      "    policy2: 39.5\n",
      "  policy_reward_mean:\n",
      "    policy1: 22.945\n",
      "    policy2: -2.8059999999999934\n",
      "  policy_reward_min:\n",
      "    policy1: -33.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3061220614024594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16960949001839737\n",
      "    mean_inference_ms: 3.4126861203836136\n",
      "    mean_raw_obs_processing_ms: 1.250852849482918\n",
      "  time_since_restore: 357.8996524810791\n",
      "  time_this_iter_s: 18.492703914642334\n",
      "  time_total_s: 357.8996524810791\n",
      "  timers:\n",
      "    learn_throughput: 243.461\n",
      "    learn_time_ms: 16429.714\n",
      "    sample_throughput: 4271.633\n",
      "    sample_time_ms: 936.41\n",
      "    update_time_ms: 2.746\n",
      "  timestamp: 1625109667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">           357.9</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  20.139</td><td style=\"text-align: right;\">                35.4</td><td style=\"text-align: right;\">                 -15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-21-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 36.599999999999916\n",
      "  episode_reward_mean: 21.305999999999937\n",
      "  episode_reward_min: -14.999999999999973\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 875\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.8898424785584211\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013627707463456318\n",
      "          policy_loss: -0.05538107693428174\n",
      "          total_loss: 47.56302630901337\n",
      "          vf_explained_var: 0.38327717781066895\n",
      "          vf_loss: 47.604610085487366\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.8960614465177059\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012046989926602691\n",
      "          policy_loss: -0.032079543569125235\n",
      "          total_loss: 13.113495290279388\n",
      "          vf_explained_var: 0.209801584482193\n",
      "          vf_loss: 13.133377313613892\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.992000000000004\n",
      "    ram_util_percent: 62.232\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 40.0\n",
      "    policy2: 39.5\n",
      "  policy_reward_mean:\n",
      "    policy1: 23.155\n",
      "    policy2: -1.8489999999999935\n",
      "  policy_reward_min:\n",
      "    policy1: -33.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3051803075400492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16910590021015687\n",
      "    mean_inference_ms: 3.3983694172676975\n",
      "    mean_raw_obs_processing_ms: 1.2474945565785638\n",
      "  time_since_restore: 375.1338095664978\n",
      "  time_this_iter_s: 17.2341570854187\n",
      "  time_total_s: 375.1338095664978\n",
      "  timers:\n",
      "    learn_throughput: 241.786\n",
      "    learn_time_ms: 16543.567\n",
      "    sample_throughput: 4357.707\n",
      "    sample_time_ms: 917.914\n",
      "    update_time_ms: 2.638\n",
      "  timestamp: 1625109684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         375.134</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  21.306</td><td style=\"text-align: right;\">                36.6</td><td style=\"text-align: right;\">                 -15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-21-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 36.599999999999916\n",
      "  episode_reward_mean: 21.81899999999993\n",
      "  episode_reward_min: -14.999999999999973\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 900\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.8714092690497637\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012820605625165626\n",
      "          policy_loss: -0.03884829790331423\n",
      "          total_loss: 46.085367918014526\n",
      "          vf_explained_var: 0.350822389125824\n",
      "          vf_loss: 46.11123466491699\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.8766209315508604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013341097044758499\n",
      "          policy_loss: -0.03952687082346529\n",
      "          total_loss: 8.958369210362434\n",
      "          vf_explained_var: 0.1791197806596756\n",
      "          vf_loss: 8.984388276934624\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.995999999999995\n",
      "    ram_util_percent: 62.172\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.0\n",
      "    policy2: 39.5\n",
      "  policy_reward_mean:\n",
      "    policy1: 23.8\n",
      "    policy2: -1.9809999999999928\n",
      "  policy_reward_min:\n",
      "    policy1: -33.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30439077790863445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1690241374679256\n",
      "    mean_inference_ms: 3.3907458464720643\n",
      "    mean_raw_obs_processing_ms: 1.2466500986883347\n",
      "  time_since_restore: 393.0475764274597\n",
      "  time_this_iter_s: 17.913766860961914\n",
      "  time_total_s: 393.0475764274597\n",
      "  timers:\n",
      "    learn_throughput: 241.676\n",
      "    learn_time_ms: 16551.108\n",
      "    sample_throughput: 4323.495\n",
      "    sample_time_ms: 925.177\n",
      "    update_time_ms: 2.736\n",
      "  timestamp: 1625109702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         393.048</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  21.819</td><td style=\"text-align: right;\">                36.6</td><td style=\"text-align: right;\">                 -15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-21-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.99999999999991\n",
      "  episode_reward_mean: 22.259999999999927\n",
      "  episode_reward_min: -4.4999999999999725\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 950\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.8506576735526323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013460650690831244\n",
      "          policy_loss: -0.04159090976463631\n",
      "          total_loss: 39.805975914001465\n",
      "          vf_explained_var: 0.4023236036300659\n",
      "          vf_loss: 39.83393895626068\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.8627097737044096\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015238345105899498\n",
      "          policy_loss: -0.04721771764161531\n",
      "          total_loss: 6.065625622868538\n",
      "          vf_explained_var: 0.18587130308151245\n",
      "          vf_loss: 6.097414508461952\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.1875\n",
      "    ram_util_percent: 62.025\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.0\n",
      "    policy2: 16.399999999999988\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.155\n",
      "    policy2: -3.8949999999999902\n",
      "  policy_reward_min:\n",
      "    policy1: 2.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3033224944981233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1683036330885539\n",
      "    mean_inference_ms: 3.3731390680097504\n",
      "    mean_raw_obs_processing_ms: 1.2422169123485967\n",
      "  time_since_restore: 409.52071261405945\n",
      "  time_this_iter_s: 16.47313618659973\n",
      "  time_total_s: 409.52071261405945\n",
      "  timers:\n",
      "    learn_throughput: 242.879\n",
      "    learn_time_ms: 16469.091\n",
      "    sample_throughput: 4406.9\n",
      "    sample_time_ms: 907.668\n",
      "    update_time_ms: 2.65\n",
      "  timestamp: 1625109719\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         409.521</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">   22.26</td><td style=\"text-align: right;\">                  36</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-22-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.79999999999993\n",
      "  episode_reward_mean: 22.23899999999993\n",
      "  episode_reward_min: -3.899999999999974\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.8361954726278782\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013801533525111154\n",
      "          policy_loss: -0.042492512468015775\n",
      "          total_loss: 39.49115043878555\n",
      "          vf_explained_var: 0.3885577917098999\n",
      "          vf_loss: 39.5196692943573\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.8440099004656076\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013723007985390723\n",
      "          policy_loss: -0.04427486436907202\n",
      "          total_loss: 8.256306126713753\n",
      "          vf_explained_var: 0.2442253828048706\n",
      "          vf_loss: 8.286686509847641\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.231818181818184\n",
      "    ram_util_percent: 61.81818181818182\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 41.5\n",
      "    policy2: 17.50000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.2\n",
      "    policy2: -3.960999999999991\n",
      "  policy_reward_min:\n",
      "    policy1: 5.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30124177955136067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16762553562695193\n",
      "    mean_inference_ms: 3.3497544428457173\n",
      "    mean_raw_obs_processing_ms: 1.2323177962495917\n",
      "  time_since_restore: 425.4709095954895\n",
      "  time_this_iter_s: 15.950196981430054\n",
      "  time_total_s: 425.4709095954895\n",
      "  timers:\n",
      "    learn_throughput: 243.128\n",
      "    learn_time_ms: 16452.217\n",
      "    sample_throughput: 4438.103\n",
      "    sample_time_ms: 901.286\n",
      "    update_time_ms: 2.603\n",
      "  timestamp: 1625109735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         425.471</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  22.239</td><td style=\"text-align: right;\">                34.8</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-22-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.599999999999916\n",
      "  episode_reward_mean: 23.150999999999918\n",
      "  episode_reward_min: -3.899999999999974\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1025\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.8104434404522181\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01443797501269728\n",
      "          policy_loss: -0.04361645434983075\n",
      "          total_loss: 54.72861862182617\n",
      "          vf_explained_var: 0.3610352575778961\n",
      "          vf_loss: 54.757617235183716\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.8309159930795431\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013314227806404233\n",
      "          policy_loss: -0.03626749271643348\n",
      "          total_loss: 6.761329770088196\n",
      "          vf_explained_var: 0.1408863663673401\n",
      "          vf_loss: 6.784116715192795\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.608000000000004\n",
      "    ram_util_percent: 62.047999999999995\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 17.50000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.915\n",
      "    policy2: -4.763999999999991\n",
      "  policy_reward_min:\n",
      "    policy1: 5.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30084319135055765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1670861027035871\n",
      "    mean_inference_ms: 3.340785143180733\n",
      "    mean_raw_obs_processing_ms: 1.2278419220455012\n",
      "  time_since_restore: 442.58334970474243\n",
      "  time_this_iter_s: 17.11244010925293\n",
      "  time_total_s: 442.58334970474243\n",
      "  timers:\n",
      "    learn_throughput: 243.415\n",
      "    learn_time_ms: 16432.837\n",
      "    sample_throughput: 4843.308\n",
      "    sample_time_ms: 825.882\n",
      "    update_time_ms: 2.666\n",
      "  timestamp: 1625109752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         442.583</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">  23.151</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-22-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.599999999999916\n",
      "  episode_reward_mean: 23.078999999999922\n",
      "  episode_reward_min: 8.10000000000003\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1075\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.7827772051095963\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01187054009642452\n",
      "          policy_loss: -0.04296684722066857\n",
      "          total_loss: 58.35486912727356\n",
      "          vf_explained_var: 0.3712236285209656\n",
      "          vf_loss: 58.38581693172455\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.8152655549347401\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008422455444815569\n",
      "          policy_loss: -0.028378242437611334\n",
      "          total_loss: 16.14551293104887\n",
      "          vf_explained_var: 0.35482656955718994\n",
      "          vf_loss: 16.165363423526287\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.46153846153846\n",
      "    ram_util_percent: 62.12692307692308\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 55.99999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.37\n",
      "    policy2: -4.2909999999999915\n",
      "  policy_reward_min:\n",
      "    policy1: -36.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2998441804934525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1664963277723783\n",
      "    mean_inference_ms: 3.328427610290595\n",
      "    mean_raw_obs_processing_ms: 1.2213883720962433\n",
      "  time_since_restore: 460.6251904964447\n",
      "  time_this_iter_s: 18.04184079170227\n",
      "  time_total_s: 460.6251904964447\n",
      "  timers:\n",
      "    learn_throughput: 243.027\n",
      "    learn_time_ms: 16459.077\n",
      "    sample_throughput: 4825.485\n",
      "    sample_time_ms: 828.932\n",
      "    update_time_ms: 2.85\n",
      "  timestamp: 1625109770\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         460.625</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  23.079</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                 8.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-23-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.599999999999916\n",
      "  episode_reward_mean: 23.51099999999992\n",
      "  episode_reward_min: 8.10000000000003\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1100\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.755469124764204\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012736326403683051\n",
      "          policy_loss: -0.04194001277210191\n",
      "          total_loss: 69.02539026737213\n",
      "          vf_explained_var: 0.3581518232822418\n",
      "          vf_loss: 69.05443394184113\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.7591878082603216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01084563639597036\n",
      "          policy_loss: -0.032865810295334086\n",
      "          total_loss: 26.65272694826126\n",
      "          vf_explained_var: 0.4809296131134033\n",
      "          vf_loss: 26.67461133003235\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.49166666666667\n",
      "    ram_util_percent: 62.104166666666664\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 55.99999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.735\n",
      "    policy2: -3.223999999999991\n",
      "  policy_reward_min:\n",
      "    policy1: -36.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2991555958996221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16643393764402714\n",
      "    mean_inference_ms: 3.3240934612124455\n",
      "    mean_raw_obs_processing_ms: 1.2187633189591982\n",
      "  time_since_restore: 478.09564757347107\n",
      "  time_this_iter_s: 17.470457077026367\n",
      "  time_total_s: 478.09564757347107\n",
      "  timers:\n",
      "    learn_throughput: 242.933\n",
      "    learn_time_ms: 16465.471\n",
      "    sample_throughput: 4993.182\n",
      "    sample_time_ms: 801.092\n",
      "    update_time_ms: 2.971\n",
      "  timestamp: 1625109788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         478.096</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  23.511</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                 8.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-23-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.199999999999925\n",
      "  episode_reward_mean: 23.77199999999992\n",
      "  episode_reward_min: 8.10000000000003\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1150\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.7568658664822578\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011726018390618265\n",
      "          policy_loss: -0.0369270934315864\n",
      "          total_loss: 37.519820392131805\n",
      "          vf_explained_var: 0.498402863740921\n",
      "          vf_loss: 37.544874131679535\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.784734720364213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012868296063970774\n",
      "          policy_loss: -0.03936969648930244\n",
      "          total_loss: 9.859687820076942\n",
      "          vf_explained_var: 0.21718358993530273\n",
      "          vf_loss: 9.88602864742279\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.034615384615385\n",
      "    ram_util_percent: 62.21923076923077\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 43.5\n",
      "    policy2: 55.99999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.555\n",
      "    policy2: -1.7829999999999913\n",
      "  policy_reward_min:\n",
      "    policy1: -36.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29957186274161657\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16636288859393347\n",
      "    mean_inference_ms: 3.325963114660163\n",
      "    mean_raw_obs_processing_ms: 1.2182176157382305\n",
      "  time_since_restore: 496.32689356803894\n",
      "  time_this_iter_s: 18.23124599456787\n",
      "  time_total_s: 496.32689356803894\n",
      "  timers:\n",
      "    learn_throughput: 238.964\n",
      "    learn_time_ms: 16738.915\n",
      "    sample_throughput: 5118.762\n",
      "    sample_time_ms: 781.439\n",
      "    update_time_ms: 3.001\n",
      "  timestamp: 1625109806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         496.327</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  23.772</td><td style=\"text-align: right;\">                37.2</td><td style=\"text-align: right;\">                 8.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-23-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.199999999999925\n",
      "  episode_reward_mean: 24.143999999999913\n",
      "  episode_reward_min: 6.600000000000022\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.7550983149558306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013223929272498935\n",
      "          policy_loss: -0.05174273130251095\n",
      "          total_loss: 46.10219955444336\n",
      "          vf_explained_var: 0.2923959493637085\n",
      "          vf_loss: 46.140552282333374\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.773777574300766\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015490151999983937\n",
      "          policy_loss: -0.044109482725616544\n",
      "          total_loss: 5.510032311081886\n",
      "          vf_explained_var: 0.17955386638641357\n",
      "          vf_loss: 5.53845788538456\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.608\n",
      "    ram_util_percent: 62.308\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 35.09999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.445\n",
      "    policy2: -3.300999999999993\n",
      "  policy_reward_min:\n",
      "    policy1: -10.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2998162680174701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16684812422138265\n",
      "    mean_inference_ms: 3.3309050447973276\n",
      "    mean_raw_obs_processing_ms: 1.2175793590046657\n",
      "  time_since_restore: 513.2293696403503\n",
      "  time_this_iter_s: 16.9024760723114\n",
      "  time_total_s: 513.2293696403503\n",
      "  timers:\n",
      "    learn_throughput: 241.315\n",
      "    learn_time_ms: 16575.869\n",
      "    sample_throughput: 5032.174\n",
      "    sample_time_ms: 794.885\n",
      "    update_time_ms: 2.945\n",
      "  timestamp: 1625109823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         513.229</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  24.144</td><td style=\"text-align: right;\">                37.2</td><td style=\"text-align: right;\">                 6.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-24-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.199999999999925\n",
      "  episode_reward_mean: 25.013999999999925\n",
      "  episode_reward_min: 6.600000000000022\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1225\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.7478128559887409\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014537624316290021\n",
      "          policy_loss: -0.04249158198945224\n",
      "          total_loss: 58.531182169914246\n",
      "          vf_explained_var: 0.4047550559043884\n",
      "          vf_loss: 58.55895435810089\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.7455529663711786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011800238949945197\n",
      "          policy_loss: -0.037765214205137454\n",
      "          total_loss: 12.203456282615662\n",
      "          vf_explained_var: 0.22140374779701233\n",
      "          vf_loss: 12.229273721575737\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.43478260869565\n",
      "    ram_util_percent: 62.32608695652174\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 19.69999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.81\n",
      "    policy2: -3.795999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: -1.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3001940477765845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16678774662317244\n",
      "    mean_inference_ms: 3.333324505078986\n",
      "    mean_raw_obs_processing_ms: 1.217242701072748\n",
      "  time_since_restore: 529.8716366291046\n",
      "  time_this_iter_s: 16.642266988754272\n",
      "  time_total_s: 529.8716366291046\n",
      "  timers:\n",
      "    learn_throughput: 243.946\n",
      "    learn_time_ms: 16397.103\n",
      "    sample_throughput: 5072.444\n",
      "    sample_time_ms: 788.575\n",
      "    update_time_ms: 2.993\n",
      "  timestamp: 1625109840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         529.872</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  25.014</td><td style=\"text-align: right;\">                37.2</td><td style=\"text-align: right;\">                 6.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-24-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.6999999999999\n",
      "  episode_reward_mean: 25.439999999999912\n",
      "  episode_reward_min: 6.600000000000022\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1275\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.7493421267718077\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011357261042576283\n",
      "          policy_loss: -0.032791847304906696\n",
      "          total_loss: 43.244035959243774\n",
      "          vf_explained_var: 0.46728771924972534\n",
      "          vf_loss: 43.265329122543335\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.724886367097497\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013962425582576543\n",
      "          policy_loss: -0.03742250188952312\n",
      "          total_loss: 5.542097851634026\n",
      "          vf_explained_var: 0.1960243433713913\n",
      "          vf_loss: 5.565383307635784\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.92916666666667\n",
      "    ram_util_percent: 62.04583333333333\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 25.19999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.675\n",
      "    policy2: -3.2349999999999928\n",
      "  policy_reward_min:\n",
      "    policy1: -1.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3003727199578073\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16689444509781276\n",
      "    mean_inference_ms: 3.3367764490603293\n",
      "    mean_raw_obs_processing_ms: 1.2170210554812988\n",
      "  time_since_restore: 546.7721195220947\n",
      "  time_this_iter_s: 16.900482892990112\n",
      "  time_total_s: 546.7721195220947\n",
      "  timers:\n",
      "    learn_throughput: 244.574\n",
      "    learn_time_ms: 16354.937\n",
      "    sample_throughput: 5013.446\n",
      "    sample_time_ms: 797.854\n",
      "    update_time_ms: 2.965\n",
      "  timestamp: 1625109857\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         546.772</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">   25.44</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                 6.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-24-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.69999999999992\n",
      "  episode_reward_mean: 26.24399999999991\n",
      "  episode_reward_min: 13.199999999999976\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1300\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.7520488016307354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014453867683187127\n",
      "          policy_loss: -0.046342017958522774\n",
      "          total_loss: 47.91534101963043\n",
      "          vf_explained_var: 0.31279441714286804\n",
      "          vf_loss: 47.94704806804657\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.7050630934536457\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012352562043815851\n",
      "          policy_loss: -0.03343265340663493\n",
      "          total_loss: 6.555551096796989\n",
      "          vf_explained_var: 0.20724938809871674\n",
      "          vf_loss: 6.576476842164993\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.972\n",
      "    ram_util_percent: 61.996\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 25.19999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.49\n",
      "    policy2: -3.2459999999999924\n",
      "  policy_reward_min:\n",
      "    policy1: 4.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3004605828275483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16715643516136494\n",
      "    mean_inference_ms: 3.34054374819573\n",
      "    mean_raw_obs_processing_ms: 1.2187691879259615\n",
      "  time_since_restore: 564.3374843597412\n",
      "  time_this_iter_s: 17.565364837646484\n",
      "  time_total_s: 564.3374843597412\n",
      "  timers:\n",
      "    learn_throughput: 245.311\n",
      "    learn_time_ms: 16305.839\n",
      "    sample_throughput: 4921.234\n",
      "    sample_time_ms: 812.804\n",
      "    update_time_ms: 2.869\n",
      "  timestamp: 1625109874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         564.337</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  26.244</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                13.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-24-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.69999999999992\n",
      "  episode_reward_mean: 24.782999999999916\n",
      "  episode_reward_min: -15.000000000000016\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1350\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.733967112377286\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010923820751486346\n",
      "          policy_loss: -0.03539090038975701\n",
      "          total_loss: 48.204896569252014\n",
      "          vf_explained_var: 0.4383675754070282\n",
      "          vf_loss: 48.22922682762146\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.6909687649458647\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011891787929926068\n",
      "          policy_loss: -0.039980609697522596\n",
      "          total_loss: 5.963369995355606\n",
      "          vf_explained_var: 0.20221267640590668\n",
      "          vf_loss: 5.991310119628906\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.650000000000006\n",
      "    ram_util_percent: 62.107692307692304\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 44.0\n",
      "    policy2: 13.100000000000012\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.26\n",
      "    policy2: -3.476999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30191440291019794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16740104561209734\n",
      "    mean_inference_ms: 3.355706549002816\n",
      "    mean_raw_obs_processing_ms: 1.2241630593308068\n",
      "  time_since_restore: 582.0647473335266\n",
      "  time_this_iter_s: 17.7272629737854\n",
      "  time_total_s: 582.0647473335266\n",
      "  timers:\n",
      "    learn_throughput: 243.721\n",
      "    learn_time_ms: 16412.211\n",
      "    sample_throughput: 4810.06\n",
      "    sample_time_ms: 831.59\n",
      "    update_time_ms: 2.962\n",
      "  timestamp: 1625109892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         582.065</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">  24.783</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                 -15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-25-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 41.69999999999991\n",
      "  episode_reward_mean: 23.297999999999917\n",
      "  episode_reward_min: -15.000000000000016\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.7258352134376764\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012346163217443973\n",
      "          policy_loss: -0.03469091281294823\n",
      "          total_loss: 41.05376237630844\n",
      "          vf_explained_var: 0.3798743188381195\n",
      "          vf_loss: 41.075952887535095\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.6712769251316786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012428994989022613\n",
      "          policy_loss: -0.03857807628810406\n",
      "          total_loss: 9.681242361664772\n",
      "          vf_explained_var: 0.2266920506954193\n",
      "          vf_loss: 9.707236289978027\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.812\n",
      "    ram_util_percent: 62.144\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 44.0\n",
      "    policy2: 20.799999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.06\n",
      "    policy2: -2.761999999999993\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30199043900324773\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16756710839585956\n",
      "    mean_inference_ms: 3.359302903008388\n",
      "    mean_raw_obs_processing_ms: 1.2242203969607077\n",
      "  time_since_restore: 599.6468985080719\n",
      "  time_this_iter_s: 17.582151174545288\n",
      "  time_total_s: 599.6468985080719\n",
      "  timers:\n",
      "    learn_throughput: 241.372\n",
      "    learn_time_ms: 16571.934\n",
      "    sample_throughput: 4793.792\n",
      "    sample_time_ms: 834.413\n",
      "    update_time_ms: 3.023\n",
      "  timestamp: 1625109910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         599.647</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  23.298</td><td style=\"text-align: right;\">                41.7</td><td style=\"text-align: right;\">                 -15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 41.9999999999999\n",
      "  episode_reward_mean: 24.611999999999917\n",
      "  episode_reward_min: -15.000000000000016\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1425\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.7090427149087191\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011347666091751307\n",
      "          policy_loss: -0.03905441757524386\n",
      "          total_loss: 54.887192249298096\n",
      "          vf_explained_var: 0.3882172107696533\n",
      "          vf_loss: 54.91475749015808\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.6565224099904299\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011524360248586163\n",
      "          policy_loss: -0.03812630122411065\n",
      "          total_loss: 10.499200209975243\n",
      "          vf_explained_var: 0.1467559039592743\n",
      "          vf_loss: 10.525658011436462\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.49999999999999\n",
      "    ram_util_percent: 62.119230769230775\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 20.799999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.22\n",
      "    policy2: -2.607999999999994\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3019049653532296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1671757262435393\n",
      "    mean_inference_ms: 3.3548074564672583\n",
      "    mean_raw_obs_processing_ms: 1.2213505561878342\n",
      "  time_since_restore: 618.0548794269562\n",
      "  time_this_iter_s: 18.407980918884277\n",
      "  time_total_s: 618.0548794269562\n",
      "  timers:\n",
      "    learn_throughput: 239.57\n",
      "    learn_time_ms: 16696.569\n",
      "    sample_throughput: 4763.859\n",
      "    sample_time_ms: 839.655\n",
      "    update_time_ms: 2.975\n",
      "  timestamp: 1625109928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         618.055</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">  24.612</td><td style=\"text-align: right;\">                  42</td><td style=\"text-align: right;\">                 -15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-25-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.19999999999991\n",
      "  episode_reward_mean: 25.700999999999922\n",
      "  episode_reward_min: 6.599999999999946\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1475\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.6513062454760075\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010608064330881462\n",
      "          policy_loss: -0.03765065909828991\n",
      "          total_loss: 83.23577332496643\n",
      "          vf_explained_var: 0.3778274357318878\n",
      "          vf_loss: 83.26268267631531\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.6078086476773024\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009730464720632881\n",
      "          policy_loss: -0.031714811513666064\n",
      "          total_loss: 49.27068841457367\n",
      "          vf_explained_var: 0.33860546350479126\n",
      "          vf_loss: 49.29255157709122\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.53928571428572\n",
      "    ram_util_percent: 62.08571428571428\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 54.0\n",
      "    policy2: 57.09999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.395\n",
      "    policy2: -0.6939999999999956\n",
      "  policy_reward_min:\n",
      "    policy1: -31.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3010663319355525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16673336419194637\n",
      "    mean_inference_ms: 3.343794111577474\n",
      "    mean_raw_obs_processing_ms: 1.2165013725947063\n",
      "  time_since_restore: 637.3672695159912\n",
      "  time_this_iter_s: 19.312390089035034\n",
      "  time_total_s: 637.3672695159912\n",
      "  timers:\n",
      "    learn_throughput: 237.634\n",
      "    learn_time_ms: 16832.608\n",
      "    sample_throughput: 4811.047\n",
      "    sample_time_ms: 831.42\n",
      "    update_time_ms: 2.788\n",
      "  timestamp: 1625109947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         637.367</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">  25.701</td><td style=\"text-align: right;\">                46.2</td><td style=\"text-align: right;\">                 6.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-26-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.19999999999991\n",
      "  episode_reward_mean: 26.54399999999992\n",
      "  episode_reward_min: 10.799999999999931\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1500\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.6578108109533787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01142730432911776\n",
      "          policy_loss: -0.04072327809990384\n",
      "          total_loss: 93.50556802749634\n",
      "          vf_explained_var: 0.29439038038253784\n",
      "          vf_loss: 93.53472149372101\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.5860246401280165\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011576366785448045\n",
      "          policy_loss: -0.03494589318870567\n",
      "          total_loss: 47.66449809074402\n",
      "          vf_explained_var: 0.2546466290950775\n",
      "          vf_loss: 47.687722980976105\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.26785714285715\n",
      "    ram_util_percent: 63.08571428571429\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 54.0\n",
      "    policy2: 57.09999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.215\n",
      "    policy2: 0.32900000000000423\n",
      "  policy_reward_min:\n",
      "    policy1: -31.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3006975227646643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16685858019915728\n",
      "    mean_inference_ms: 3.342477107514944\n",
      "    mean_raw_obs_processing_ms: 1.215623620098355\n",
      "  time_since_restore: 654.9548842906952\n",
      "  time_this_iter_s: 17.58761477470398\n",
      "  time_total_s: 654.9548842906952\n",
      "  timers:\n",
      "    learn_throughput: 237.652\n",
      "    learn_time_ms: 16831.349\n",
      "    sample_throughput: 4742.239\n",
      "    sample_time_ms: 843.483\n",
      "    update_time_ms: 2.763\n",
      "  timestamp: 1625109965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         654.955</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">  26.544</td><td style=\"text-align: right;\">                46.2</td><td style=\"text-align: right;\">                10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-26-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 41.099999999999916\n",
      "  episode_reward_mean: 26.807999999999925\n",
      "  episode_reward_min: 11.699999999999978\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1550\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.6529938206076622\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01205019476765301\n",
      "          policy_loss: -0.03559972153743729\n",
      "          total_loss: 74.10644471645355\n",
      "          vf_explained_var: 0.39646294713020325\n",
      "          vf_loss: 74.12984478473663\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.5691890455782413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00941241392865777\n",
      "          policy_loss: -0.020127179275732487\n",
      "          total_loss: 29.879907846450806\n",
      "          vf_explained_var: 0.274535596370697\n",
      "          vf_loss: 29.890504956245422\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.211111111111116\n",
      "    ram_util_percent: 60.52222222222222\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 47.0\n",
      "    policy2: 43.899999999999984\n",
      "  policy_reward_mean:\n",
      "    policy1: 24.73\n",
      "    policy2: 2.0780000000000047\n",
      "  policy_reward_min:\n",
      "    policy1: -28.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3027510065555923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16761151907143915\n",
      "    mean_inference_ms: 3.3619160818281824\n",
      "    mean_raw_obs_processing_ms: 1.2223602767070356\n",
      "  time_since_restore: 674.3022603988647\n",
      "  time_this_iter_s: 19.347376108169556\n",
      "  time_total_s: 674.3022603988647\n",
      "  timers:\n",
      "    learn_throughput: 236.49\n",
      "    learn_time_ms: 16914.016\n",
      "    sample_throughput: 4586.166\n",
      "    sample_time_ms: 872.188\n",
      "    update_time_ms: 2.86\n",
      "  timestamp: 1625109984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         674.302</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  26.808</td><td style=\"text-align: right;\">                41.1</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-26-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 42.29999999999994\n",
      "  episode_reward_mean: 27.401999999999926\n",
      "  episode_reward_min: 9.300000000000015\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1600\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.6171208284795284\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010537848487729207\n",
      "          policy_loss: -0.038302931119687855\n",
      "          total_loss: 98.75880146026611\n",
      "          vf_explained_var: 0.3327019214630127\n",
      "          vf_loss: 98.78643572330475\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.5386306596919894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008467600346193649\n",
      "          policy_loss: -0.02356571286509279\n",
      "          total_loss: 54.52508932352066\n",
      "          vf_explained_var: 0.36963915824890137\n",
      "          vf_loss: 54.54008162021637\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.904\n",
      "    ram_util_percent: 60.912000000000006\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 65.89999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.115\n",
      "    policy2: 2.287000000000005\n",
      "  policy_reward_min:\n",
      "    policy1: -47.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.305489422720424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16925214300898075\n",
      "    mean_inference_ms: 3.395105180446744\n",
      "    mean_raw_obs_processing_ms: 1.2345613582754862\n",
      "  time_since_restore: 691.905041217804\n",
      "  time_this_iter_s: 17.60278081893921\n",
      "  time_total_s: 691.905041217804\n",
      "  timers:\n",
      "    learn_throughput: 235.874\n",
      "    learn_time_ms: 16958.197\n",
      "    sample_throughput: 4454.685\n",
      "    sample_time_ms: 897.931\n",
      "    update_time_ms: 2.963\n",
      "  timestamp: 1625110002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         691.905</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">  27.402</td><td style=\"text-align: right;\">                42.3</td><td style=\"text-align: right;\">                 9.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 42.29999999999994\n",
      "  episode_reward_mean: 26.759999999999923\n",
      "  episode_reward_min: -5.399999999999974\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1625\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.608578335493803\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010056228551547974\n",
      "          policy_loss: -0.039720732427667826\n",
      "          total_loss: 126.71490669250488\n",
      "          vf_explained_var: 0.3913487195968628\n",
      "          vf_loss: 126.74444818496704\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.5102438563480973\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00933224796608556\n",
      "          policy_loss: -0.027798937604529783\n",
      "          total_loss: 76.74751496315002\n",
      "          vf_explained_var: 0.32228463888168335\n",
      "          vf_loss: 76.76586449146271\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.108695652173914\n",
      "    ram_util_percent: 61.08695652173913\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 65.89999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 22.35\n",
      "    policy2: 4.410000000000006\n",
      "  policy_reward_min:\n",
      "    policy1: -48.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30644083959044294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.169343620019851\n",
      "    mean_inference_ms: 3.4015342466885796\n",
      "    mean_raw_obs_processing_ms: 1.2364816305514\n",
      "  time_since_restore: 708.0036602020264\n",
      "  time_this_iter_s: 16.098618984222412\n",
      "  time_total_s: 708.0036602020264\n",
      "  timers:\n",
      "    learn_throughput: 236.576\n",
      "    learn_time_ms: 16907.902\n",
      "    sample_throughput: 4473.832\n",
      "    sample_time_ms: 894.088\n",
      "    update_time_ms: 2.901\n",
      "  timestamp: 1625110018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         708.004</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">   26.76</td><td style=\"text-align: right;\">                42.3</td><td style=\"text-align: right;\">                -5.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-27-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 41.0999999999999\n",
      "  episode_reward_mean: 26.768999999999924\n",
      "  episode_reward_min: -5.399999999999974\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1675\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.6082373224198818\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010465923434821889\n",
      "          policy_loss: -0.03888287820154801\n",
      "          total_loss: 90.99619698524475\n",
      "          vf_explained_var: 0.40143096446990967\n",
      "          vf_loss: 91.02448320388794\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.5081617161631584\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009218808394507505\n",
      "          policy_loss: -0.025678436213638633\n",
      "          total_loss: 51.7008490562439\n",
      "          vf_explained_var: 0.34195399284362793\n",
      "          vf_loss: 51.71719288825989\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.199999999999996\n",
      "    ram_util_percent: 61.17391304347827\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 65.89999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 21.93\n",
      "    policy2: 4.839000000000006\n",
      "  policy_reward_min:\n",
      "    policy1: -48.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3064106748305724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16926024128807768\n",
      "    mean_inference_ms: 3.4007609084622965\n",
      "    mean_raw_obs_processing_ms: 1.2362815240244098\n",
      "  time_since_restore: 723.6076731681824\n",
      "  time_this_iter_s: 15.604012966156006\n",
      "  time_total_s: 723.6076731681824\n",
      "  timers:\n",
      "    learn_throughput: 238.266\n",
      "    learn_time_ms: 16787.961\n",
      "    sample_throughput: 4522.428\n",
      "    sample_time_ms: 884.481\n",
      "    update_time_ms: 2.871\n",
      "  timestamp: 1625110034\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         723.608</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">  26.769</td><td style=\"text-align: right;\">                41.1</td><td style=\"text-align: right;\">                -5.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-27-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 41.0999999999999\n",
      "  episode_reward_mean: 26.354999999999926\n",
      "  episode_reward_min: -5.399999999999974\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1700\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.6330260615795851\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011587152344873175\n",
      "          policy_loss: -0.04267140389129054\n",
      "          total_loss: 55.799885392189026\n",
      "          vf_explained_var: 0.3577066659927368\n",
      "          vf_loss: 55.83082449436188\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.5296872034668922\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011486400966532528\n",
      "          policy_loss: -0.0348560765851289\n",
      "          total_loss: 13.34864354133606\n",
      "          vf_explained_var: 0.2138759195804596\n",
      "          vf_loss: 13.371869593858719\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.695454545454545\n",
      "    ram_util_percent: 61.449999999999996\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 64.8\n",
      "  policy_reward_mean:\n",
      "    policy1: 22.935\n",
      "    policy2: 3.4200000000000057\n",
      "  policy_reward_min:\n",
      "    policy1: -48.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30548156515142666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16912846536992324\n",
      "    mean_inference_ms: 3.395094335269156\n",
      "    mean_raw_obs_processing_ms: 1.2340512564376684\n",
      "  time_since_restore: 739.5415270328522\n",
      "  time_this_iter_s: 15.9338538646698\n",
      "  time_total_s: 739.5415270328522\n",
      "  timers:\n",
      "    learn_throughput: 240.317\n",
      "    learn_time_ms: 16644.661\n",
      "    sample_throughput: 4625.912\n",
      "    sample_time_ms: 864.694\n",
      "    update_time_ms: 2.877\n",
      "  timestamp: 1625110050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         739.542</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  26.355</td><td style=\"text-align: right;\">                41.1</td><td style=\"text-align: right;\">                -5.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-27-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.099999999999895\n",
      "  episode_reward_mean: 26.16299999999992\n",
      "  episode_reward_min: 10.20000000000001\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1750\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.6044534202665091\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009882264173938893\n",
      "          policy_loss: -0.03455116726399865\n",
      "          total_loss: 63.919331073760986\n",
      "          vf_explained_var: 0.4511594772338867\n",
      "          vf_loss: 63.943875193595886\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.5062406668439507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009181985529721715\n",
      "          policy_loss: -0.03183251991868019\n",
      "          total_loss: 21.839143604040146\n",
      "          vf_explained_var: 0.23653055727481842\n",
      "          vf_loss: 21.861679077148438\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.1\n",
      "    ram_util_percent: 61.458333333333336\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 57.099999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.175\n",
      "    policy2: -0.011999999999991556\n",
      "  policy_reward_min:\n",
      "    policy1: -38.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3048385886459185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16841624150156412\n",
      "    mean_inference_ms: 3.3838633047464874\n",
      "    mean_raw_obs_processing_ms: 1.229796507027756\n",
      "  time_since_restore: 756.2932560443878\n",
      "  time_this_iter_s: 16.751729011535645\n",
      "  time_total_s: 756.2932560443878\n",
      "  timers:\n",
      "    learn_throughput: 241.428\n",
      "    learn_time_ms: 16568.08\n",
      "    sample_throughput: 4741.235\n",
      "    sample_time_ms: 843.662\n",
      "    update_time_ms: 2.866\n",
      "  timestamp: 1625110067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         756.293</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">  26.163</td><td style=\"text-align: right;\">                44.1</td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-28-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.099999999999945\n",
      "  episode_reward_mean: 26.936999999999916\n",
      "  episode_reward_min: 10.199999999999916\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1800\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5773807624354959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01096820883685723\n",
      "          policy_loss: -0.03606245148694143\n",
      "          total_loss: 67.6617283821106\n",
      "          vf_explained_var: 0.41563931107521057\n",
      "          vf_loss: 67.68668520450592\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.4704292006790638\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009178821186651476\n",
      "          policy_loss: -0.02501678332919255\n",
      "          total_loss: 30.163585007190704\n",
      "          vf_explained_var: 0.4221709668636322\n",
      "          vf_loss: 30.17930844426155\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.7\n",
      "    ram_util_percent: 61.71538461538463\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 58.199999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.06\n",
      "    policy2: -1.1229999999999927\n",
      "  policy_reward_min:\n",
      "    policy1: -33.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30348491200618255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16804364382420467\n",
      "    mean_inference_ms: 3.3726050683351145\n",
      "    mean_raw_obs_processing_ms: 1.2248349979799755\n",
      "  time_since_restore: 774.1336119174957\n",
      "  time_this_iter_s: 17.84035587310791\n",
      "  time_total_s: 774.1336119174957\n",
      "  timers:\n",
      "    learn_throughput: 241.083\n",
      "    learn_time_ms: 16591.798\n",
      "    sample_throughput: 4725.934\n",
      "    sample_time_ms: 846.393\n",
      "    update_time_ms: 2.803\n",
      "  timestamp: 1625110085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         774.134</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  26.937</td><td style=\"text-align: right;\">                44.1</td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-28-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.099999999999945\n",
      "  episode_reward_mean: 27.71399999999992\n",
      "  episode_reward_min: 9.599999999999929\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1825\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.575436856597662\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010584337898762897\n",
      "          policy_loss: -0.029876374581363052\n",
      "          total_loss: 99.357541680336\n",
      "          vf_explained_var: 0.4079066514968872\n",
      "          vf_loss: 99.37670063972473\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.4560162900015712\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008694539152202196\n",
      "          policy_loss: -0.01949935482116416\n",
      "          total_loss: 55.57815456390381\n",
      "          vf_explained_var: 0.33946335315704346\n",
      "          vf_loss: 55.58885049819946\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.84814814814814\n",
      "    ram_util_percent: 62.17407407407408\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 47.0\n",
      "    policy2: 66.99999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.23\n",
      "    policy2: 1.4840000000000049\n",
      "  policy_reward_min:\n",
      "    policy1: -50.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30416178703039193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16802063068207837\n",
      "    mean_inference_ms: 3.375402348241983\n",
      "    mean_raw_obs_processing_ms: 1.22546286801246\n",
      "  time_since_restore: 793.5487608909607\n",
      "  time_this_iter_s: 19.415148973464966\n",
      "  time_total_s: 793.5487608909607\n",
      "  timers:\n",
      "    learn_throughput: 240.062\n",
      "    learn_time_ms: 16662.397\n",
      "    sample_throughput: 4563.312\n",
      "    sample_time_ms: 876.556\n",
      "    update_time_ms: 2.766\n",
      "  timestamp: 1625110104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         793.549</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">  27.714</td><td style=\"text-align: right;\">                44.1</td><td style=\"text-align: right;\">                 9.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-28-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.099999999999945\n",
      "  episode_reward_mean: 27.014999999999926\n",
      "  episode_reward_min: -4.73232564246473e-14\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1875\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5624528033658862\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010137731253053062\n",
      "          policy_loss: -0.029504501959308982\n",
      "          total_loss: 90.40749025344849\n",
      "          vf_explained_var: 0.4197303056716919\n",
      "          vf_loss: 90.42673194408417\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.4554482949897647\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007074807974277064\n",
      "          policy_loss: -0.02916519221616909\n",
      "          total_loss: 46.94156736135483\n",
      "          vf_explained_var: 0.3546808362007141\n",
      "          vf_loss: 46.96356850862503\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.417391304347824\n",
      "    ram_util_percent: 62.20000000000002\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 69.19999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 22.88\n",
      "    policy2: 4.135000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -53.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3047280715087971\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16826477191696923\n",
      "    mean_inference_ms: 3.379726968143288\n",
      "    mean_raw_obs_processing_ms: 1.2269735062818854\n",
      "  time_since_restore: 809.2912349700928\n",
      "  time_this_iter_s: 15.74247407913208\n",
      "  time_total_s: 809.2912349700928\n",
      "  timers:\n",
      "    learn_throughput: 245.352\n",
      "    learn_time_ms: 16303.117\n",
      "    sample_throughput: 4551.249\n",
      "    sample_time_ms: 878.88\n",
      "    update_time_ms: 2.739\n",
      "  timestamp: 1625110120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         809.291</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  27.015</td><td style=\"text-align: right;\">                44.1</td><td style=\"text-align: right;\">        -4.73233e-14</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-28-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 43.49999999999994\n",
      "  episode_reward_mean: 26.942999999999916\n",
      "  episode_reward_min: -4.73232564246473e-14\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1900\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5556779690086842\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010142985032871366\n",
      "          policy_loss: -0.03942727667163126\n",
      "          total_loss: 108.74274230003357\n",
      "          vf_explained_var: 0.32439902424812317\n",
      "          vf_loss: 108.77189946174622\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.45478449761867523\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008491732442053035\n",
      "          policy_loss: -0.024101989285554737\n",
      "          total_loss: 53.33566117286682\n",
      "          vf_explained_var: 0.367654025554657\n",
      "          vf_loss: 53.35116624832153\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.43478260869565\n",
      "    ram_util_percent: 62.28695652173916\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 69.19999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 23.435\n",
      "    policy2: 3.508000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -53.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3046110596940207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16849996433781023\n",
      "    mean_inference_ms: 3.380637715933454\n",
      "    mean_raw_obs_processing_ms: 1.2276384552254505\n",
      "  time_since_restore: 825.3122160434723\n",
      "  time_this_iter_s: 16.020981073379517\n",
      "  time_total_s: 825.3122160434723\n",
      "  timers:\n",
      "    learn_throughput: 247.442\n",
      "    learn_time_ms: 16165.401\n",
      "    sample_throughput: 4646.458\n",
      "    sample_time_ms: 860.871\n",
      "    update_time_ms: 2.681\n",
      "  timestamp: 1625110136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         825.312</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">  26.943</td><td style=\"text-align: right;\">                43.5</td><td style=\"text-align: right;\">        -4.73233e-14</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.99999999999989\n",
      "  episode_reward_mean: 26.97299999999991\n",
      "  episode_reward_min: 11.099999999999975\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1950\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5412654019892216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01096715868334286\n",
      "          policy_loss: -0.037435822290717624\n",
      "          total_loss: 50.55959916114807\n",
      "          vf_explained_var: 0.5152097344398499\n",
      "          vf_loss: 50.58593064546585\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.46067621279507875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009136672699241899\n",
      "          policy_loss: -0.032512452627997845\n",
      "          total_loss: 10.31050269305706\n",
      "          vf_explained_var: 0.2257208228111267\n",
      "          vf_loss: 10.333764418959618\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.170833333333334\n",
      "    ram_util_percent: 62.487500000000004\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 57.099999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.82\n",
      "    policy2: 0.15300000000000535\n",
      "  policy_reward_min:\n",
      "    policy1: -40.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3051096805485987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16850045944936018\n",
      "    mean_inference_ms: 3.3810512713755863\n",
      "    mean_raw_obs_processing_ms: 1.2277569280992864\n",
      "  time_since_restore: 842.6167390346527\n",
      "  time_this_iter_s: 17.30452299118042\n",
      "  time_total_s: 842.6167390346527\n",
      "  timers:\n",
      "    learn_throughput: 250.377\n",
      "    learn_time_ms: 15975.927\n",
      "    sample_throughput: 4729.563\n",
      "    sample_time_ms: 845.744\n",
      "    update_time_ms: 2.716\n",
      "  timestamp: 1625110153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         842.617</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  26.973</td><td style=\"text-align: right;\">                  45</td><td style=\"text-align: right;\">                11.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.29999999999989\n",
      "  episode_reward_mean: 27.608999999999913\n",
      "  episode_reward_min: 11.099999999999923\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2000\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5460654096677899\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011444914096500725\n",
      "          policy_loss: -0.03660273423884064\n",
      "          total_loss: 51.23738205432892\n",
      "          vf_explained_var: 0.42363882064819336\n",
      "          vf_loss: 51.262396693229675\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.446755844168365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009469673153944314\n",
      "          policy_loss: -0.03308580274460837\n",
      "          total_loss: 20.762616097927094\n",
      "          vf_explained_var: 0.30733707547187805\n",
      "          vf_loss: 20.786113917827606\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.861538461538466\n",
      "    ram_util_percent: 62.71923076923077\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 57.099999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.94\n",
      "    policy2: -0.3309999999999939\n",
      "  policy_reward_min:\n",
      "    policy1: -40.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3057062744189167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16935401296550245\n",
      "    mean_inference_ms: 3.3935417364598264\n",
      "    mean_raw_obs_processing_ms: 1.2320882634608559\n",
      "  time_since_restore: 860.6139221191406\n",
      "  time_this_iter_s: 17.997183084487915\n",
      "  time_total_s: 860.6139221191406\n",
      "  timers:\n",
      "    learn_throughput: 249.632\n",
      "    learn_time_ms: 16023.591\n",
      "    sample_throughput: 4778.7\n",
      "    sample_time_ms: 837.048\n",
      "    update_time_ms: 2.706\n",
      "  timestamp: 1625110171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         860.614</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">  27.609</td><td style=\"text-align: right;\">                45.3</td><td style=\"text-align: right;\">                11.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-29-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.29999999999989\n",
      "  episode_reward_mean: 27.25799999999991\n",
      "  episode_reward_min: -0.9000000000000057\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2025\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5393119771033525\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010714047908550128\n",
      "          policy_loss: -0.031208670377964154\n",
      "          total_loss: 58.7618545293808\n",
      "          vf_explained_var: 0.42772674560546875\n",
      "          vf_loss: 58.782214522361755\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.4380191331729293\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009252861636923626\n",
      "          policy_loss: -0.03444632171886042\n",
      "          total_loss: 15.574363395571709\n",
      "          vf_explained_var: 0.23401284217834473\n",
      "          vf_loss: 15.599441424012184\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.37407407407408\n",
      "    ram_util_percent: 62.73333333333334\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 57.099999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.27\n",
      "    policy2: -0.01199999999999461\n",
      "  policy_reward_min:\n",
      "    policy1: -40.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30696420664887364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16963902017822613\n",
      "    mean_inference_ms: 3.4037137268026942\n",
      "    mean_raw_obs_processing_ms: 1.2351481387163716\n",
      "  time_since_restore: 879.5649852752686\n",
      "  time_this_iter_s: 18.95106315612793\n",
      "  time_total_s: 879.5649852752686\n",
      "  timers:\n",
      "    learn_throughput: 245.789\n",
      "    learn_time_ms: 16274.092\n",
      "    sample_throughput: 4588.987\n",
      "    sample_time_ms: 871.652\n",
      "    update_time_ms: 2.712\n",
      "  timestamp: 1625110190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         879.565</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  27.258</td><td style=\"text-align: right;\">                45.3</td><td style=\"text-align: right;\">                -0.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-30-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.29999999999989\n",
      "  episode_reward_mean: 26.672999999999917\n",
      "  episode_reward_min: -0.9000000000000057\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2075\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5260745612904429\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010782082536024973\n",
      "          policy_loss: -0.038400133460527286\n",
      "          total_loss: 45.476948857307434\n",
      "          vf_explained_var: 0.48442479968070984\n",
      "          vf_loss: 45.504432022571564\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.41812095791101456\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008664376626256853\n",
      "          policy_loss: -0.02162455624784343\n",
      "          total_loss: 18.977315574884415\n",
      "          vf_explained_var: 0.30201810598373413\n",
      "          vf_loss: 18.99016734957695\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.93846153846154\n",
      "    ram_util_percent: 62.90384615384615\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 27.400000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.795\n",
      "    policy2: -0.12199999999999538\n",
      "  policy_reward_min:\n",
      "    policy1: -10.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30863840480989063\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17047787215644894\n",
      "    mean_inference_ms: 3.4235726914580233\n",
      "    mean_raw_obs_processing_ms: 1.2424841047757709\n",
      "  time_since_restore: 897.3985652923584\n",
      "  time_this_iter_s: 17.833580017089844\n",
      "  time_total_s: 897.3985652923584\n",
      "  timers:\n",
      "    learn_throughput: 242.78\n",
      "    learn_time_ms: 16475.805\n",
      "    sample_throughput: 4483.647\n",
      "    sample_time_ms: 892.131\n",
      "    update_time_ms: 2.828\n",
      "  timestamp: 1625110208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         897.399</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">  26.673</td><td style=\"text-align: right;\">                45.3</td><td style=\"text-align: right;\">                -0.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 42.899999999999906\n",
      "  episode_reward_mean: 26.120999999999913\n",
      "  episode_reward_min: -0.9000000000000057\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2100\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5207427088171244\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009903548183501698\n",
      "          policy_loss: -0.030531662399880588\n",
      "          total_loss: 64.98694276809692\n",
      "          vf_explained_var: 0.3759056627750397\n",
      "          vf_loss: 65.00744581222534\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.41075793374329805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007620429270900786\n",
      "          policy_loss: -0.0228262135933619\n",
      "          total_loss: 22.633049935102463\n",
      "          vf_explained_var: 0.3303641676902771\n",
      "          vf_loss: 22.648159861564636\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.66521739130435\n",
      "    ram_util_percent: 63.304347826086975\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 37.29999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.99\n",
      "    policy2: 0.13100000000000458\n",
      "  policy_reward_min:\n",
      "    policy1: -10.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3091731918705001\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17109854517134\n",
      "    mean_inference_ms: 3.435203736640309\n",
      "    mean_raw_obs_processing_ms: 1.2473994205349777\n",
      "  time_since_restore: 914.1098833084106\n",
      "  time_this_iter_s: 16.711318016052246\n",
      "  time_total_s: 914.1098833084106\n",
      "  timers:\n",
      "    learn_throughput: 242.077\n",
      "    learn_time_ms: 16523.65\n",
      "    sample_throughput: 4340.145\n",
      "    sample_time_ms: 921.628\n",
      "    update_time_ms: 2.906\n",
      "  timestamp: 1625110225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">          914.11</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  26.121</td><td style=\"text-align: right;\">                42.9</td><td style=\"text-align: right;\">                -0.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-30-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.699999999999896\n",
      "  episode_reward_mean: 27.581999999999912\n",
      "  episode_reward_min: 9.299999999999978\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2150\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.532847173511982\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011965885641984642\n",
      "          policy_loss: -0.0397716409934219\n",
      "          total_loss: 47.60154092311859\n",
      "          vf_explained_var: 0.44051337242126465\n",
      "          vf_loss: 47.62919735908508\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.3938567154109478\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008902138724806719\n",
      "          policy_loss: -0.0227295879740268\n",
      "          total_loss: 14.092758163809776\n",
      "          vf_explained_var: 0.26613736152648926\n",
      "          vf_loss: 14.106474414467812\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.888461538461534\n",
      "    ram_util_percent: 63.41923076923077\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 37.29999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.54\n",
      "    policy2: -0.9579999999999947\n",
      "  policy_reward_min:\n",
      "    policy1: -7.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31051254439492115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17143087528900922\n",
      "    mean_inference_ms: 3.446651031963885\n",
      "    mean_raw_obs_processing_ms: 1.251179478899189\n",
      "  time_since_restore: 932.3384981155396\n",
      "  time_this_iter_s: 18.228614807128906\n",
      "  time_total_s: 932.3384981155396\n",
      "  timers:\n",
      "    learn_throughput: 240.155\n",
      "    learn_time_ms: 16655.919\n",
      "    sample_throughput: 4269.255\n",
      "    sample_time_ms: 936.932\n",
      "    update_time_ms: 2.986\n",
      "  timestamp: 1625110243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         932.338</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">  27.582</td><td style=\"text-align: right;\">                44.7</td><td style=\"text-align: right;\">                 9.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-31-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.699999999999896\n",
      "  episode_reward_mean: 28.91999999999991\n",
      "  episode_reward_min: 9.299999999999978\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2200\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5267959600314498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011378162686014548\n",
      "          policy_loss: -0.04015610304486472\n",
      "          total_loss: 51.2565575838089\n",
      "          vf_explained_var: 0.38757336139678955\n",
      "          vf_loss: 51.28519296646118\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.3880249308422208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007887807267252356\n",
      "          policy_loss: -0.021769274433609098\n",
      "          total_loss: 12.505044236779213\n",
      "          vf_explained_var: 0.25827574729919434\n",
      "          vf_loss: 12.518827050924301\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.064\n",
      "    ram_util_percent: 62.08\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 30.7\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.46\n",
      "    policy2: -0.5399999999999961\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3101760856376154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17162844106346264\n",
      "    mean_inference_ms: 3.4493391902985473\n",
      "    mean_raw_obs_processing_ms: 1.251589169772224\n",
      "  time_since_restore: 949.4835090637207\n",
      "  time_this_iter_s: 17.145010948181152\n",
      "  time_total_s: 949.4835090637207\n",
      "  timers:\n",
      "    learn_throughput: 241.198\n",
      "    learn_time_ms: 16583.863\n",
      "    sample_throughput: 4257.338\n",
      "    sample_time_ms: 939.554\n",
      "    update_time_ms: 2.96\n",
      "  timestamp: 1625110261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         949.484</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">   28.92</td><td style=\"text-align: right;\">                44.7</td><td style=\"text-align: right;\">                 9.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-31-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 43.79999999999989\n",
      "  episode_reward_mean: 28.49399999999991\n",
      "  episode_reward_min: 10.199999999999973\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2225\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.518477720208466\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0101266161800595\n",
      "          policy_loss: -0.028769243916030973\n",
      "          total_loss: 58.48160147666931\n",
      "          vf_explained_var: 0.4852888584136963\n",
      "          vf_loss: 58.500118136405945\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.3960338346660137\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008548306548618712\n",
      "          policy_loss: -0.025280075264163315\n",
      "          total_loss: 12.38359522819519\n",
      "          vf_explained_var: 0.2638653516769409\n",
      "          vf_loss: 12.400220036506653\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.548148148148144\n",
      "    ram_util_percent: 60.951851851851856\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 25.199999999999967\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.76\n",
      "    policy2: -1.2659999999999956\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31036366992390135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17136959749499872\n",
      "    mean_inference_ms: 3.4470343422919565\n",
      "    mean_raw_obs_processing_ms: 1.2498077713470201\n",
      "  time_since_restore: 968.4429938793182\n",
      "  time_this_iter_s: 18.959484815597534\n",
      "  time_total_s: 968.4429938793182\n",
      "  timers:\n",
      "    learn_throughput: 241.459\n",
      "    learn_time_ms: 16565.938\n",
      "    sample_throughput: 4386.7\n",
      "    sample_time_ms: 911.847\n",
      "    update_time_ms: 2.991\n",
      "  timestamp: 1625110280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         968.443</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">  28.494</td><td style=\"text-align: right;\">                43.8</td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 43.79999999999989\n",
      "  episode_reward_mean: 28.77299999999991\n",
      "  episode_reward_min: 10.199999999999973\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2275\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5084627205505967\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010730282490840182\n",
      "          policy_loss: -0.04745615113642998\n",
      "          total_loss: 43.285601019859314\n",
      "          vf_explained_var: 0.46620872616767883\n",
      "          vf_loss: 43.322192907333374\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.38719790428876877\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009677585170720704\n",
      "          policy_loss: -0.025918253522831947\n",
      "          total_loss: 7.5673821568489075\n",
      "          vf_explained_var: 0.26801180839538574\n",
      "          vf_loss: 7.583501860499382\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.45\n",
      "    ram_util_percent: 61.48846153846154\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 25.199999999999967\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.27\n",
      "    policy2: -1.4969999999999937\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.309954034490971\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1711368737134037\n",
      "    mean_inference_ms: 3.442193303403203\n",
      "    mean_raw_obs_processing_ms: 1.2477989228472934\n",
      "  time_since_restore: 986.4683079719543\n",
      "  time_this_iter_s: 18.02531409263611\n",
      "  time_total_s: 986.4683079719543\n",
      "  timers:\n",
      "    learn_throughput: 238.239\n",
      "    learn_time_ms: 16789.832\n",
      "    sample_throughput: 4365.828\n",
      "    sample_time_ms: 916.207\n",
      "    update_time_ms: 3.006\n",
      "  timestamp: 1625110298\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         986.468</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  28.773</td><td style=\"text-align: right;\">                43.8</td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-31-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.899999999999906\n",
      "  episode_reward_mean: 28.511999999999908\n",
      "  episode_reward_min: 11.999999999999925\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2300\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.5033513372763991\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011761668807594106\n",
      "          policy_loss: -0.037840775679796934\n",
      "          total_loss: 49.8784402012825\n",
      "          vf_explained_var: 0.3748028874397278\n",
      "          vf_loss: 49.904372692108154\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.3766078343614936\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007265429521794431\n",
      "          policy_loss: -0.019802608236204833\n",
      "          total_loss: 11.682046070694923\n",
      "          vf_explained_var: 0.33684828877449036\n",
      "          vf_loss: 11.69449257850647\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.25555555555556\n",
      "    ram_util_percent: 62.01111111111111\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 32.899999999999956\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.295\n",
      "    policy2: -1.782999999999994\n",
      "  policy_reward_min:\n",
      "    policy1: -9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.309605818374954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17131261968640998\n",
      "    mean_inference_ms: 3.4429423392412004\n",
      "    mean_raw_obs_processing_ms: 1.2482823436925359\n",
      "  time_since_restore: 1005.4973692893982\n",
      "  time_this_iter_s: 19.029061317443848\n",
      "  time_total_s: 1005.4973692893982\n",
      "  timers:\n",
      "    learn_throughput: 234.366\n",
      "    learn_time_ms: 17067.295\n",
      "    sample_throughput: 4255.995\n",
      "    sample_time_ms: 939.851\n",
      "    update_time_ms: 2.967\n",
      "  timestamp: 1625110317\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">          1005.5</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">  28.512</td><td style=\"text-align: right;\">                45.9</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-32-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.899999999999906\n",
      "  episode_reward_mean: 28.925999999999913\n",
      "  episode_reward_min: 0.30000000000000937\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2350\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.499114696867764\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01040422500227578\n",
      "          policy_loss: -0.030269727867562324\n",
      "          total_loss: 39.088170289993286\n",
      "          vf_explained_var: 0.5453683137893677\n",
      "          vf_loss: 39.10790550708771\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.387932307086885\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009937457638443448\n",
      "          policy_loss: -0.029949122807011008\n",
      "          total_loss: 3.124899946153164\n",
      "          vf_explained_var: 0.23211070895195007\n",
      "          vf_loss: 3.1447874009609222\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.97037037037036\n",
      "    ram_util_percent: 62.522222222222226\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 32.899999999999956\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.39\n",
      "    policy2: -1.463999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3101304107247952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17129697721937515\n",
      "    mean_inference_ms: 3.4444953063456296\n",
      "    mean_raw_obs_processing_ms: 1.2487576286914988\n",
      "  time_since_restore: 1024.655724287033\n",
      "  time_this_iter_s: 19.158354997634888\n",
      "  time_total_s: 1024.655724287033\n",
      "  timers:\n",
      "    learn_throughput: 231.588\n",
      "    learn_time_ms: 17272.065\n",
      "    sample_throughput: 4343.5\n",
      "    sample_time_ms: 920.916\n",
      "    update_time_ms: 2.851\n",
      "  timestamp: 1625110336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1024.66</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  28.926</td><td style=\"text-align: right;\">                45.9</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-32-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.599999999999895\n",
      "  episode_reward_mean: 28.841999999999906\n",
      "  episode_reward_min: 0.30000000000000937\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2400\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.4971405565738678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009180304448818788\n",
      "          policy_loss: -0.03316359795280732\n",
      "          total_loss: 40.95639330148697\n",
      "          vf_explained_var: 0.4539433717727661\n",
      "          vf_loss: 40.98026245832443\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.37088790349662304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008918736304622144\n",
      "          policy_loss: -0.03166651012725197\n",
      "          total_loss: 4.122450947761536\n",
      "          vf_explained_var: 0.24284228682518005\n",
      "          vf_loss: 4.145087294280529\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.284615384615385\n",
      "    ram_util_percent: 62.20384615384616\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 20.799999999999986\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.23\n",
      "    policy2: -2.3879999999999955\n",
      "  policy_reward_min:\n",
      "    policy1: 7.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3102887093661092\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1719049826552117\n",
      "    mean_inference_ms: 3.452289288858076\n",
      "    mean_raw_obs_processing_ms: 1.2516761302931736\n",
      "  time_since_restore: 1042.8029832839966\n",
      "  time_this_iter_s: 18.1472589969635\n",
      "  time_total_s: 1042.8029832839966\n",
      "  timers:\n",
      "    learn_throughput: 231.436\n",
      "    learn_time_ms: 17283.363\n",
      "    sample_throughput: 4323.228\n",
      "    sample_time_ms: 925.235\n",
      "    update_time_ms: 2.705\n",
      "  timestamp: 1625110354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">          1042.8</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">  28.842</td><td style=\"text-align: right;\">                45.6</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-32-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.599999999999895\n",
      "  episode_reward_mean: 28.529999999999905\n",
      "  episode_reward_min: 0.30000000000000937\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2425\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.4604586251080036\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00968187366379425\n",
      "          policy_loss: -0.03634499783220235\n",
      "          total_loss: 54.27969813346863\n",
      "          vf_explained_var: 0.4854733347892761\n",
      "          vf_loss: 54.30624163150787\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.37462913431227207\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010249417726299725\n",
      "          policy_loss: -0.024299891607370228\n",
      "          total_loss: 3.6870671957731247\n",
      "          vf_explained_var: 0.2284083217382431\n",
      "          vf_loss: 3.7009894773364067\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.468\n",
      "    ram_util_percent: 61.412\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 8.700000000000012\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.05\n",
      "    policy2: -2.5199999999999965\n",
      "  policy_reward_min:\n",
      "    policy1: 7.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3107237105177423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17189093553281773\n",
      "    mean_inference_ms: 3.4541800346047697\n",
      "    mean_raw_obs_processing_ms: 1.2513681679743203\n",
      "  time_since_restore: 1060.178943157196\n",
      "  time_this_iter_s: 17.375959873199463\n",
      "  time_total_s: 1060.178943157196\n",
      "  timers:\n",
      "    learn_throughput: 232.995\n",
      "    learn_time_ms: 17167.765\n",
      "    sample_throughput: 4528.631\n",
      "    sample_time_ms: 883.269\n",
      "    update_time_ms: 2.722\n",
      "  timestamp: 1625110372\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1060.18</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">   28.53</td><td style=\"text-align: right;\">                45.6</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-33-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.299999999999905\n",
      "  episode_reward_mean: 29.345999999999908\n",
      "  episode_reward_min: 6.899999999999972\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2475\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.45990800578147173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009412181709194556\n",
      "          policy_loss: -0.03621422732248902\n",
      "          total_loss: 47.44175672531128\n",
      "          vf_explained_var: 0.4747450351715088\n",
      "          vf_loss: 47.4684419631958\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.3703594971448183\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01099903235444799\n",
      "          policy_loss: -0.027013294224161655\n",
      "          total_loss: 2.251761559396982\n",
      "          vf_explained_var: 0.28433239459991455\n",
      "          vf_loss: 2.2676383666694164\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.565384615384616\n",
      "    ram_util_percent: 61.665384615384625\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 5.399999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.185\n",
      "    policy2: -2.8389999999999964\n",
      "  policy_reward_min:\n",
      "    policy1: 10.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3102728402292507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17170273202350705\n",
      "    mean_inference_ms: 3.4504742122546794\n",
      "    mean_raw_obs_processing_ms: 1.249488790555637\n",
      "  time_since_restore: 1078.3220512866974\n",
      "  time_this_iter_s: 18.143108129501343\n",
      "  time_total_s: 1078.3220512866974\n",
      "  timers:\n",
      "    learn_throughput: 232.315\n",
      "    learn_time_ms: 17217.995\n",
      "    sample_throughput: 4627.422\n",
      "    sample_time_ms: 864.412\n",
      "    update_time_ms: 2.715\n",
      "  timestamp: 1625110390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1078.32</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">  29.346</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                 6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-33-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.99999999999989\n",
      "  episode_reward_mean: 29.91899999999991\n",
      "  episode_reward_min: 9.299999999999946\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2500\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.45996512845158577\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009708321245852858\n",
      "          policy_loss: -0.029493181966245174\n",
      "          total_loss: 57.936488032341\n",
      "          vf_explained_var: 0.32221248745918274\n",
      "          vf_loss: 57.956151843070984\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.35013770032674074\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0099830822146032\n",
      "          policy_loss: -0.026882497710175812\n",
      "          total_loss: 3.5753841176629066\n",
      "          vf_explained_var: 0.24602019786834717\n",
      "          vf_loss: 3.592158764600754\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.296\n",
      "    ram_util_percent: 62.19200000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 5.399999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.67\n",
      "    policy2: -2.750999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: 10.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3097905891727045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17182349753481552\n",
      "    mean_inference_ms: 3.4515056734741334\n",
      "    mean_raw_obs_processing_ms: 1.2491326273553174\n",
      "  time_since_restore: 1096.1676774024963\n",
      "  time_this_iter_s: 17.84562611579895\n",
      "  time_total_s: 1096.1676774024963\n",
      "  timers:\n",
      "    learn_throughput: 230.829\n",
      "    learn_time_ms: 17328.862\n",
      "    sample_throughput: 4612.158\n",
      "    sample_time_ms: 867.273\n",
      "    update_time_ms: 2.639\n",
      "  timestamp: 1625110408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1096.17</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  29.919</td><td style=\"text-align: right;\">                  51</td><td style=\"text-align: right;\">                 9.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-33-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.99999999999989\n",
      "  episode_reward_mean: 29.843999999999905\n",
      "  episode_reward_min: 4.7999999999999705\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2550\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.4333848813548684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011029368717572652\n",
      "          policy_loss: -0.03199455395224504\n",
      "          total_loss: 50.58541393280029\n",
      "          vf_explained_var: 0.49820467829704285\n",
      "          vf_loss: 50.606241106987\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.3532142862677574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006538082525366917\n",
      "          policy_loss: -0.01778130783350207\n",
      "          total_loss: 10.191692389547825\n",
      "          vf_explained_var: 0.26180487871170044\n",
      "          vf_loss: 10.202854007482529\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.50370370370371\n",
      "    ram_util_percent: 62.17037037037038\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 30.699999999999974\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.21\n",
      "    policy2: -2.365999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: 6.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30993687676442877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17156726384257492\n",
      "    mean_inference_ms: 3.454617940743649\n",
      "    mean_raw_obs_processing_ms: 1.2477921443516513\n",
      "  time_since_restore: 1114.9334774017334\n",
      "  time_this_iter_s: 18.76579999923706\n",
      "  time_total_s: 1114.9334774017334\n",
      "  timers:\n",
      "    learn_throughput: 229.888\n",
      "    learn_time_ms: 17399.785\n",
      "    sample_throughput: 4705.016\n",
      "    sample_time_ms: 850.156\n",
      "    update_time_ms: 2.567\n",
      "  timestamp: 1625110426\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1114.93</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">  29.844</td><td style=\"text-align: right;\">                  51</td><td style=\"text-align: right;\">                 4.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-34-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.99999999999989\n",
      "  episode_reward_mean: 30.506999999999913\n",
      "  episode_reward_min: 4.7999999999999705\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2600\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.4466161960735917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009506653674179688\n",
      "          policy_loss: -0.02882041031261906\n",
      "          total_loss: 48.38812077045441\n",
      "          vf_explained_var: 0.38340672850608826\n",
      "          vf_loss: 48.40731596946716\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.351255533285439\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009815590339712799\n",
      "          policy_loss: -0.028634858666919172\n",
      "          total_loss: 2.761278957128525\n",
      "          vf_explained_var: 0.23387324810028076\n",
      "          vf_loss: 2.779975574463606\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.87916666666667\n",
      "    ram_util_percent: 62.291666666666664\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 30.699999999999974\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.015\n",
      "    policy2: -1.507999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: 6.0\n",
      "    policy2: -8.899999999999984\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30908877925819633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17148811482968576\n",
      "    mean_inference_ms: 3.450715289119239\n",
      "    mean_raw_obs_processing_ms: 1.2453686450828663\n",
      "  time_since_restore: 1131.769010066986\n",
      "  time_this_iter_s: 16.835532665252686\n",
      "  time_total_s: 1131.769010066986\n",
      "  timers:\n",
      "    learn_throughput: 230.256\n",
      "    learn_time_ms: 17372.002\n",
      "    sample_throughput: 4723.484\n",
      "    sample_time_ms: 846.833\n",
      "    update_time_ms: 2.582\n",
      "  timestamp: 1625110443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1131.77</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  30.507</td><td style=\"text-align: right;\">                  45</td><td style=\"text-align: right;\">                 4.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-34-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.99999999999989\n",
      "  episode_reward_mean: 31.460999999999903\n",
      "  episode_reward_min: 13.49999999999991\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2625\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.41558741219341755\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008428301807725802\n",
      "          policy_loss: -0.03117535766796209\n",
      "          total_loss: 43.22935688495636\n",
      "          vf_explained_var: 0.5371391177177429\n",
      "          vf_loss: 43.25199830532074\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.3483879864215851\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008356184305739589\n",
      "          policy_loss: -0.029392297088634223\n",
      "          total_loss: 3.4770147427916527\n",
      "          vf_explained_var: 0.22906893491744995\n",
      "          vf_loss: 3.4979463294148445\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.13478260869566\n",
      "    ram_util_percent: 62.57826086956523\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 30.699999999999974\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.31\n",
      "    policy2: -1.8489999999999975\n",
      "  policy_reward_min:\n",
      "    policy1: 6.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30909602884929294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1711277871153176\n",
      "    mean_inference_ms: 3.4466164678859443\n",
      "    mean_raw_obs_processing_ms: 1.2431675755055245\n",
      "  time_since_restore: 1148.3361790180206\n",
      "  time_this_iter_s: 16.567168951034546\n",
      "  time_total_s: 1148.3361790180206\n",
      "  timers:\n",
      "    learn_throughput: 233.37\n",
      "    learn_time_ms: 17140.151\n",
      "    sample_throughput: 4764.842\n",
      "    sample_time_ms: 839.482\n",
      "    update_time_ms: 2.564\n",
      "  timestamp: 1625110460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1148.34</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">  31.461</td><td style=\"text-align: right;\">                  45</td><td style=\"text-align: right;\">                13.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-34-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.89999999999989\n",
      "  episode_reward_mean: 31.664999999999896\n",
      "  episode_reward_min: 16.499999999999922\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2675\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.41459635086357594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008136734672007151\n",
      "          policy_loss: -0.02910262248769868\n",
      "          total_loss: 54.96034324169159\n",
      "          vf_explained_var: 0.47449517250061035\n",
      "          vf_loss: 54.98120844364166\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.340872960165143\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009517946746200323\n",
      "          policy_loss: -0.03249837990733795\n",
      "          total_loss: 3.6005714684724808\n",
      "          vf_explained_var: 0.28839391469955444\n",
      "          vf_loss: 3.623432956635952\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.87083333333334\n",
      "    ram_util_percent: 62.77916666666667\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 54.5\n",
      "    policy2: 10.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.195\n",
      "    policy2: -1.5299999999999978\n",
      "  policy_reward_min:\n",
      "    policy1: 9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30840285857669186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17076925780254673\n",
      "    mean_inference_ms: 3.4379313319345943\n",
      "    mean_raw_obs_processing_ms: 1.239824545892293\n",
      "  time_since_restore: 1164.6098592281342\n",
      "  time_this_iter_s: 16.273680210113525\n",
      "  time_total_s: 1164.6098592281342\n",
      "  timers:\n",
      "    learn_throughput: 235.689\n",
      "    learn_time_ms: 16971.502\n",
      "    sample_throughput: 4801.775\n",
      "    sample_time_ms: 833.025\n",
      "    update_time_ms: 2.545\n",
      "  timestamp: 1625110476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1164.61</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  31.665</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-34-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.89999999999989\n",
      "  episode_reward_mean: 31.4129999999999\n",
      "  episode_reward_min: 10.79999999999995\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2700\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.4151736833155155\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009356792696053162\n",
      "          policy_loss: -0.02684480749303475\n",
      "          total_loss: 46.228831708431244\n",
      "          vf_explained_var: 0.4356357157230377\n",
      "          vf_loss: 46.24620372056961\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.33128465712070465\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008832008665194735\n",
      "          policy_loss: -0.02795064845122397\n",
      "          total_loss: 4.050120480358601\n",
      "          vf_explained_var: 0.29511362314224243\n",
      "          vf_loss: 4.069128677248955\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.050000000000004\n",
      "    ram_util_percent: 62.50833333333333\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 54.5\n",
      "    policy2: 10.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.635\n",
      "    policy2: -1.2219999999999982\n",
      "  policy_reward_min:\n",
      "    policy1: 6.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30764860104821856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1707350540181604\n",
      "    mean_inference_ms: 3.4327355644079103\n",
      "    mean_raw_obs_processing_ms: 1.2383143272248258\n",
      "  time_since_restore: 1181.4146511554718\n",
      "  time_this_iter_s: 16.804791927337646\n",
      "  time_total_s: 1181.4146511554718\n",
      "  timers:\n",
      "    learn_throughput: 238.442\n",
      "    learn_time_ms: 16775.555\n",
      "    sample_throughput: 4959.304\n",
      "    sample_time_ms: 806.565\n",
      "    update_time_ms: 2.547\n",
      "  timestamp: 1625110493\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1181.41</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">  31.413</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-35-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.89999999999989\n",
      "  episode_reward_mean: 31.427999999999912\n",
      "  episode_reward_min: 10.79999999999995\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2750\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.39950852654874325\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008149424218572676\n",
      "          policy_loss: -0.031159012636635453\n",
      "          total_loss: 37.72542840242386\n",
      "          vf_explained_var: 0.550269365310669\n",
      "          vf_loss: 37.74833619594574\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.3302033357322216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00973176745173987\n",
      "          policy_loss: -0.032659729971783236\n",
      "          total_loss: 4.670669429004192\n",
      "          vf_explained_var: 0.2720407247543335\n",
      "          vf_loss: 4.693475633859634\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.93846153846154\n",
      "    ram_util_percent: 61.55769230769231\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 54.5\n",
      "    policy2: 10.900000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.595\n",
      "    policy2: -1.1669999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: 6.5\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3076740526063285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1702495634030366\n",
      "    mean_inference_ms: 3.426467794452301\n",
      "    mean_raw_obs_processing_ms: 1.2358029988575958\n",
      "  time_since_restore: 1200.2721109390259\n",
      "  time_this_iter_s: 18.857459783554077\n",
      "  time_total_s: 1200.2721109390259\n",
      "  timers:\n",
      "    learn_throughput: 238.917\n",
      "    learn_time_ms: 16742.227\n",
      "    sample_throughput: 4940.908\n",
      "    sample_time_ms: 809.568\n",
      "    update_time_ms: 2.65\n",
      "  timestamp: 1625110512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         1200.27</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  31.428</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-35-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.5999999999999\n",
      "  episode_reward_mean: 31.0379999999999\n",
      "  episode_reward_min: 12.899999999999928\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2800\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.40106165409088135\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008346112736035138\n",
      "          policy_loss: -0.02956718095811084\n",
      "          total_loss: 45.721173882484436\n",
      "          vf_explained_var: 0.4126655161380768\n",
      "          vf_loss: 45.74229061603546\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.3124107886105776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009589052831870504\n",
      "          policy_loss: -0.027018377237254754\n",
      "          total_loss: 4.563468150794506\n",
      "          vf_explained_var: 0.3075399696826935\n",
      "          vf_loss: 4.580777615308762\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.77407407407408\n",
      "    ram_util_percent: 62.86296296296296\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 10.900000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.105\n",
      "    policy2: -0.06699999999999699\n",
      "  policy_reward_min:\n",
      "    policy1: 7.5\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30802940712906085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17076113515153718\n",
      "    mean_inference_ms: 3.433019341701373\n",
      "    mean_raw_obs_processing_ms: 1.2382793607113032\n",
      "  time_since_restore: 1219.2136437892914\n",
      "  time_this_iter_s: 18.941532850265503\n",
      "  time_total_s: 1219.2136437892914\n",
      "  timers:\n",
      "    learn_throughput: 237.903\n",
      "    learn_time_ms: 16813.56\n",
      "    sample_throughput: 4892.092\n",
      "    sample_time_ms: 817.646\n",
      "    update_time_ms: 2.645\n",
      "  timestamp: 1625110531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1219.21</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">  31.038</td><td style=\"text-align: right;\">                45.6</td><td style=\"text-align: right;\">                12.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-35-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.099999999999895\n",
      "  episode_reward_mean: 31.709999999999905\n",
      "  episode_reward_min: 15.299999999999908\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2825\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3888206984847784\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008587263888330199\n",
      "          policy_loss: -0.028097990056267008\n",
      "          total_loss: 51.13039267063141\n",
      "          vf_explained_var: 0.5088303089141846\n",
      "          vf_loss: 51.14979660511017\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.3214776162058115\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009156754807918333\n",
      "          policy_loss: -0.030065261147683486\n",
      "          total_loss: 3.9698247611522675\n",
      "          vf_explained_var: 0.27312523126602173\n",
      "          vf_loss: 3.990618795156479\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.1875\n",
      "    ram_util_percent: 62.61249999999999\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 56.5\n",
      "    policy2: 9.80000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.645\n",
      "    policy2: 0.06500000000000357\n",
      "  policy_reward_min:\n",
      "    policy1: 7.5\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30849044462303077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17070854033981497\n",
      "    mean_inference_ms: 3.4357643087095346\n",
      "    mean_raw_obs_processing_ms: 1.238384016548616\n",
      "  time_since_restore: 1235.40540766716\n",
      "  time_this_iter_s: 16.191763877868652\n",
      "  time_total_s: 1235.40540766716\n",
      "  timers:\n",
      "    learn_throughput: 239.575\n",
      "    learn_time_ms: 16696.231\n",
      "    sample_throughput: 4899.054\n",
      "    sample_time_ms: 816.484\n",
      "    update_time_ms: 2.634\n",
      "  timestamp: 1625110547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1235.41</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   31.71</td><td style=\"text-align: right;\">                53.1</td><td style=\"text-align: right;\">                15.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-36-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.099999999999895\n",
      "  episode_reward_mean: 31.403999999999904\n",
      "  episode_reward_min: 11.699999999999932\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2875\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3801783425733447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00835933315102011\n",
      "          policy_loss: -0.025995594056439586\n",
      "          total_loss: 42.57580357789993\n",
      "          vf_explained_var: 0.5453577637672424\n",
      "          vf_loss: 42.59333539009094\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2971887942403555\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008088284550467506\n",
      "          policy_loss: -0.02210830760304816\n",
      "          total_loss: 3.5566072687506676\n",
      "          vf_explained_var: 0.31437504291534424\n",
      "          vf_loss: 3.570526137948036\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.787499999999994\n",
      "    ram_util_percent: 61.57916666666667\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 56.5\n",
      "    policy2: 12.000000000000007\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.525\n",
      "    policy2: 0.8790000000000032\n",
      "  policy_reward_min:\n",
      "    policy1: 7.5\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.308340786864593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17067722257538173\n",
      "    mean_inference_ms: 3.434646255822272\n",
      "    mean_raw_obs_processing_ms: 1.23844932069825\n",
      "  time_since_restore: 1252.771544456482\n",
      "  time_this_iter_s: 17.3661367893219\n",
      "  time_total_s: 1252.771544456482\n",
      "  timers:\n",
      "    learn_throughput: 240.761\n",
      "    learn_time_ms: 16613.991\n",
      "    sample_throughput: 4870.119\n",
      "    sample_time_ms: 821.335\n",
      "    update_time_ms: 2.519\n",
      "  timestamp: 1625110565\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1252.77</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">  31.404</td><td style=\"text-align: right;\">                53.1</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.099999999999895\n",
      "  episode_reward_mean: 31.214999999999907\n",
      "  episode_reward_min: 11.699999999999932\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2900\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.40327494870871305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012119251972762868\n",
      "          policy_loss: -0.029202532081399113\n",
      "          total_loss: 53.28054475784302\n",
      "          vf_explained_var: 0.36442506313323975\n",
      "          vf_loss: 53.297476291656494\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.28393321111798286\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006221509844181128\n",
      "          policy_loss: -0.022390015015844256\n",
      "          total_loss: 12.812160477042198\n",
      "          vf_explained_var: 0.30435457825660706\n",
      "          vf_loss: 12.828251153230667\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.900000000000002\n",
      "    ram_util_percent: 62.26666666666667\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 56.5\n",
      "    policy2: 46.09999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.215\n",
      "    policy2: 1.0000000000000027\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -7.799999999999989\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30764349473169067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17066892042523749\n",
      "    mean_inference_ms: 3.430270042429626\n",
      "    mean_raw_obs_processing_ms: 1.2378374313761722\n",
      "  time_since_restore: 1269.0784113407135\n",
      "  time_this_iter_s: 16.306866884231567\n",
      "  time_total_s: 1269.0784113407135\n",
      "  timers:\n",
      "    learn_throughput: 242.517\n",
      "    learn_time_ms: 16493.686\n",
      "    sample_throughput: 5076.796\n",
      "    sample_time_ms: 787.899\n",
      "    update_time_ms: 2.508\n",
      "  timestamp: 1625110581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1269.08</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  31.215</td><td style=\"text-align: right;\">                53.1</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-36-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.99999999999989\n",
      "  episode_reward_mean: 31.166999999999906\n",
      "  episode_reward_min: 11.699999999999932\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2950\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.37697332352399826\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009029634325997904\n",
      "          policy_loss: -0.022354075117618777\n",
      "          total_loss: 41.856142938137054\n",
      "          vf_explained_var: 0.5243224501609802\n",
      "          vf_loss: 41.86935383081436\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.28996666288003325\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007911351858638227\n",
      "          policy_loss: -0.030268052738392726\n",
      "          total_loss: 6.973582029342651\n",
      "          vf_explained_var: 0.23750758171081543\n",
      "          vf_loss: 6.99583987891674\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.44545454545455\n",
      "    ram_util_percent: 62.08636363636362\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 46.09999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.145\n",
      "    policy2: 1.0220000000000025\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3078198402852674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17029298650123187\n",
      "    mean_inference_ms: 3.4256970754328857\n",
      "    mean_raw_obs_processing_ms: 1.2370221351700008\n",
      "  time_since_restore: 1284.6384165287018\n",
      "  time_this_iter_s: 15.560005187988281\n",
      "  time_total_s: 1284.6384165287018\n",
      "  timers:\n",
      "    learn_throughput: 247.514\n",
      "    learn_time_ms: 16160.705\n",
      "    sample_throughput: 4997.462\n",
      "    sample_time_ms: 800.406\n",
      "    update_time_ms: 2.462\n",
      "  timestamp: 1625110597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1284.64</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">  31.167</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-36-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.99999999999989\n",
      "  episode_reward_mean: 33.19499999999991\n",
      "  episode_reward_min: 15.599999999999929\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3000\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.38701165188103914\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009140318594290875\n",
      "          policy_loss: -0.02292408555513248\n",
      "          total_loss: 42.5873920917511\n",
      "          vf_explained_var: 0.4088112711906433\n",
      "          vf_loss: 42.6010617017746\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.29278439562767744\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009790790441911668\n",
      "          policy_loss: -0.02747689833631739\n",
      "          total_loss: 4.511995241045952\n",
      "          vf_explained_var: 0.22999729216098785\n",
      "          vf_loss: 4.5295589864254\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.36521739130435\n",
      "    ram_util_percent: 61.46521739130433\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 16.399999999999984\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.075\n",
      "    policy2: 0.1200000000000015\n",
      "  policy_reward_min:\n",
      "    policy1: 10.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3071498096822551\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1701983107041807\n",
      "    mean_inference_ms: 3.419280474217319\n",
      "    mean_raw_obs_processing_ms: 1.235430870331794\n",
      "  time_since_restore: 1301.248678445816\n",
      "  time_this_iter_s: 16.610261917114258\n",
      "  time_total_s: 1301.248678445816\n",
      "  timers:\n",
      "    learn_throughput: 247.801\n",
      "    learn_time_ms: 16141.954\n",
      "    sample_throughput: 5021.75\n",
      "    sample_time_ms: 796.535\n",
      "    update_time_ms: 2.519\n",
      "  timestamp: 1625110613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1301.25</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  33.195</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-37-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.99999999999989\n",
      "  episode_reward_mean: 33.74399999999991\n",
      "  episode_reward_min: 15.599999999999929\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3025\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3619242087006569\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008744300954276696\n",
      "          policy_loss: -0.02794624667149037\n",
      "          total_loss: 38.97281110286713\n",
      "          vf_explained_var: 0.543172299861908\n",
      "          vf_loss: 38.99190402030945\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.29013697197660804\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007978274283232167\n",
      "          policy_loss: -0.019548727446817793\n",
      "          total_loss: 5.191368900239468\n",
      "          vf_explained_var: 0.2663329839706421\n",
      "          vf_loss: 5.202839732170105\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.327999999999996\n",
      "    ram_util_percent: 62.05999999999999\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 15.300000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.745\n",
      "    policy2: -0.00099999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: 13.5\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3071357943863253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1698090157315267\n",
      "    mean_inference_ms: 3.414401808593243\n",
      "    mean_raw_obs_processing_ms: 1.232936802771917\n",
      "  time_since_restore: 1318.7442173957825\n",
      "  time_this_iter_s: 17.49553894996643\n",
      "  time_total_s: 1318.7442173957825\n",
      "  timers:\n",
      "    learn_throughput: 246.267\n",
      "    learn_time_ms: 16242.533\n",
      "    sample_throughput: 5071.339\n",
      "    sample_time_ms: 788.746\n",
      "    update_time_ms: 2.511\n",
      "  timestamp: 1625110631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1318.74</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">  33.744</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-37-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.1999999999999\n",
      "  episode_reward_mean: 32.56499999999991\n",
      "  episode_reward_min: 14.099999999999952\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3075\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.37443354818969965\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00910986056260299\n",
      "          policy_loss: -0.02700778329744935\n",
      "          total_loss: 41.13952624797821\n",
      "          vf_explained_var: 0.5260223150253296\n",
      "          vf_loss: 41.157310128211975\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2911763684824109\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00902573563507758\n",
      "          policy_loss: -0.03251861402532086\n",
      "          total_loss: 4.10200584679842\n",
      "          vf_explained_var: 0.318596750497818\n",
      "          vf_loss: 4.125385835766792\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.40909090909091\n",
      "    ram_util_percent: 62.290909090909075\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 10.90000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.18\n",
      "    policy2: 1.3850000000000038\n",
      "  policy_reward_min:\n",
      "    policy1: 6.5\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30614102183358377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16929899394389913\n",
      "    mean_inference_ms: 3.402282469107693\n",
      "    mean_raw_obs_processing_ms: 1.228450866201718\n",
      "  time_since_restore: 1333.7123854160309\n",
      "  time_this_iter_s: 14.968168020248413\n",
      "  time_total_s: 1333.7123854160309\n",
      "  timers:\n",
      "    learn_throughput: 248.14\n",
      "    learn_time_ms: 16119.955\n",
      "    sample_throughput: 5122.854\n",
      "    sample_time_ms: 780.815\n",
      "    update_time_ms: 2.492\n",
      "  timestamp: 1625110646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1333.71</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  32.565</td><td style=\"text-align: right;\">                46.2</td><td style=\"text-align: right;\">                14.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-37-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 31.937999999999906\n",
      "  episode_reward_min: 7.500000000000012\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3100\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3730168053880334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009844091939157806\n",
      "          policy_loss: -0.031390396296046674\n",
      "          total_loss: 48.65114152431488\n",
      "          vf_explained_var: 0.4138753414154053\n",
      "          vf_loss: 48.6725652217865\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2697904263623059\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007749586147838272\n",
      "          policy_loss: -0.020133722689934075\n",
      "          total_loss: 4.680341348052025\n",
      "          vf_explained_var: 0.3219701051712036\n",
      "          vf_loss: 4.692628659307957\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.895238095238092\n",
      "    ram_util_percent: 62.36190476190476\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 10.90000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.575\n",
      "    policy2: 1.3630000000000038\n",
      "  policy_reward_min:\n",
      "    policy1: 6.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30511253806692096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16913953644844126\n",
      "    mean_inference_ms: 3.394635229793963\n",
      "    mean_raw_obs_processing_ms: 1.226109006459434\n",
      "  time_since_restore: 1348.4005675315857\n",
      "  time_this_iter_s: 14.68818211555481\n",
      "  time_total_s: 1348.4005675315857\n",
      "  timers:\n",
      "    learn_throughput: 251.323\n",
      "    learn_time_ms: 15915.748\n",
      "    sample_throughput: 5172.243\n",
      "    sample_time_ms: 773.359\n",
      "    update_time_ms: 2.479\n",
      "  timestamp: 1625110660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">          1348.4</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">  31.938</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-37-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 32.27999999999991\n",
      "  episode_reward_min: 7.500000000000012\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3150\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3438447033986449\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007547226006863639\n",
      "          policy_loss: -0.03104245100985281\n",
      "          total_loss: 34.29871141910553\n",
      "          vf_explained_var: 0.6018577814102173\n",
      "          vf_loss: 34.32211208343506\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2678225776180625\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007401190770906396\n",
      "          policy_loss: -0.016851759719429538\n",
      "          total_loss: 4.345980122685432\n",
      "          vf_explained_var: 0.2661077678203583\n",
      "          vf_loss: 4.355338118970394\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.419047619047618\n",
      "    ram_util_percent: 61.599999999999994\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 14.200000000000014\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.675\n",
      "    policy2: 1.6050000000000026\n",
      "  policy_reward_min:\n",
      "    policy1: 6.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30448738159839667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16844939120889607\n",
      "    mean_inference_ms: 3.382397547275134\n",
      "    mean_raw_obs_processing_ms: 1.2216989627121828\n",
      "  time_since_restore: 1363.5711443424225\n",
      "  time_this_iter_s: 15.170576810836792\n",
      "  time_total_s: 1363.5711443424225\n",
      "  timers:\n",
      "    learn_throughput: 257.038\n",
      "    learn_time_ms: 15561.928\n",
      "    sample_throughput: 5270.212\n",
      "    sample_time_ms: 758.983\n",
      "    update_time_ms: 2.311\n",
      "  timestamp: 1625110676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1363.57</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">   32.28</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-38-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.799999999999905\n",
      "  episode_reward_mean: 33.128999999999905\n",
      "  episode_reward_min: 14.39999999999996\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3200\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3706224812194705\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010421168219181709\n",
      "          policy_loss: -0.035137413564370945\n",
      "          total_loss: 36.567643105983734\n",
      "          vf_explained_var: 0.46790772676467896\n",
      "          vf_loss: 36.59222894906998\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2481109625659883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006050431082258001\n",
      "          policy_loss: -0.01098754265694879\n",
      "          total_loss: 10.586247101426125\n",
      "          vf_explained_var: 0.24629154801368713\n",
      "          vf_loss: 10.591108553111553\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.490909090909096\n",
      "    ram_util_percent: 61.6909090909091\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 30.69999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.545\n",
      "    policy2: 2.584000000000002\n",
      "  policy_reward_min:\n",
      "    policy1: 6.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30293358093177014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16800082406670314\n",
      "    mean_inference_ms: 3.368715079067223\n",
      "    mean_raw_obs_processing_ms: 1.2171887371918848\n",
      "  time_since_restore: 1378.9952392578125\n",
      "  time_this_iter_s: 15.424094915390015\n",
      "  time_total_s: 1378.9952392578125\n",
      "  timers:\n",
      "    learn_throughput: 261.976\n",
      "    learn_time_ms: 15268.588\n",
      "    sample_throughput: 5709.701\n",
      "    sample_time_ms: 700.562\n",
      "    update_time_ms: 2.366\n",
      "  timestamp: 1625110691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">            1379</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">  33.129</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                14.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-38-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.799999999999905\n",
      "  episode_reward_mean: 33.2609999999999\n",
      "  episode_reward_min: 14.39999999999996\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3225\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3481900170445442\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008395638724323362\n",
      "          policy_loss: -0.02565316326217726\n",
      "          total_loss: 37.0484356880188\n",
      "          vf_explained_var: 0.574562668800354\n",
      "          vf_loss: 37.065587759017944\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2526213862001896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007762861074297689\n",
      "          policy_loss: -0.0290262529742904\n",
      "          total_loss: 4.57161033898592\n",
      "          vf_explained_var: 0.283589243888855\n",
      "          vf_loss: 4.5927765518426895\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.004166666666663\n",
      "    ram_util_percent: 61.99583333333334\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 30.69999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.875\n",
      "    policy2: 2.3860000000000015\n",
      "  policy_reward_min:\n",
      "    policy1: 6.0\n",
      "    policy2: -6.69999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3029211939521941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16763781128528202\n",
      "    mean_inference_ms: 3.3640466966413864\n",
      "    mean_raw_obs_processing_ms: 1.214862817050955\n",
      "  time_since_restore: 1395.6096620559692\n",
      "  time_this_iter_s: 16.61442279815674\n",
      "  time_total_s: 1395.6096620559692\n",
      "  timers:\n",
      "    learn_throughput: 261.263\n",
      "    learn_time_ms: 15310.223\n",
      "    sample_throughput: 5703.017\n",
      "    sample_time_ms: 701.383\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1625110708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1395.61</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  33.261</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                14.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-38-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.09999999999991\n",
      "  episode_reward_mean: 32.036999999999914\n",
      "  episode_reward_min: 10.799999999999939\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3275\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3373169479891658\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008734833361813799\n",
      "          policy_loss: -0.0339308048132807\n",
      "          total_loss: 29.622411727905273\n",
      "          vf_explained_var: 0.6293529272079468\n",
      "          vf_loss: 29.647498726844788\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2511029541492462\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007517124984588008\n",
      "          policy_loss: -0.027707472851034254\n",
      "          total_loss: 4.5239273980259895\n",
      "          vf_explained_var: 0.2974714934825897\n",
      "          vf_loss: 4.54402382671833\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.491304347826095\n",
      "    ram_util_percent: 61.76086956521741\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.0\n",
      "    policy2: 30.69999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.41\n",
      "    policy2: 1.6270000000000013\n",
      "  policy_reward_min:\n",
      "    policy1: 6.5\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3023884337713505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16742987683109134\n",
      "    mean_inference_ms: 3.3577543649245825\n",
      "    mean_raw_obs_processing_ms: 1.212504085549736\n",
      "  time_since_restore: 1411.855232000351\n",
      "  time_this_iter_s: 16.245569944381714\n",
      "  time_total_s: 1411.855232000351\n",
      "  timers:\n",
      "    learn_throughput: 263.24\n",
      "    learn_time_ms: 15195.265\n",
      "    sample_throughput: 5679.524\n",
      "    sample_time_ms: 704.284\n",
      "    update_time_ms: 2.426\n",
      "  timestamp: 1625110724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1411.86</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">  32.037</td><td style=\"text-align: right;\">                47.1</td><td style=\"text-align: right;\">                10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-39-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.4999999999999\n",
      "  episode_reward_mean: 32.38199999999991\n",
      "  episode_reward_min: 10.799999999999939\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3300\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.34206255059689283\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007730686294962652\n",
      "          policy_loss: -0.026720883310190402\n",
      "          total_loss: 40.78951561450958\n",
      "          vf_explained_var: 0.41514626145362854\n",
      "          vf_loss: 40.80840760469437\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22668148344382644\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008169190376065671\n",
      "          policy_loss: -0.02514680486638099\n",
      "          total_loss: 4.867424339056015\n",
      "          vf_explained_var: 0.34419748187065125\n",
      "          vf_loss: 4.884299769997597\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.988\n",
      "    ram_util_percent: 61.632\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 12.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.305\n",
      "    policy2: 1.077000000000002\n",
      "  policy_reward_min:\n",
      "    policy1: 11.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30192799151530364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16758088448425312\n",
      "    mean_inference_ms: 3.356513987987171\n",
      "    mean_raw_obs_processing_ms: 1.212608563215362\n",
      "  time_since_restore: 1428.9732358455658\n",
      "  time_this_iter_s: 17.118003845214844\n",
      "  time_total_s: 1428.9732358455658\n",
      "  timers:\n",
      "    learn_throughput: 262.043\n",
      "    learn_time_ms: 15264.67\n",
      "    sample_throughput: 5588.248\n",
      "    sample_time_ms: 715.788\n",
      "    update_time_ms: 2.479\n",
      "  timestamp: 1625110741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1428.97</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  32.382</td><td style=\"text-align: right;\">                49.5</td><td style=\"text-align: right;\">                10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-39-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.4999999999999\n",
      "  episode_reward_mean: 32.41199999999991\n",
      "  episode_reward_min: 10.799999999999939\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3350\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3181705502793193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00733953551389277\n",
      "          policy_loss: -0.026901803154032677\n",
      "          total_loss: 38.14650648832321\n",
      "          vf_explained_var: 0.5793315768241882\n",
      "          vf_loss: 38.165977120399475\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23590162489563227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007136044732760638\n",
      "          policy_loss: -0.01958273455966264\n",
      "          total_loss: 3.540800631046295\n",
      "          vf_explained_var: 0.3416520953178406\n",
      "          vf_loss: 3.553158111870289\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.92916666666667\n",
      "    ram_util_percent: 62.025\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 10.900000000000016\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.785\n",
      "    policy2: 1.6270000000000033\n",
      "  policy_reward_min:\n",
      "    policy1: 11.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30233759879191163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1674462559069616\n",
      "    mean_inference_ms: 3.3562447773016326\n",
      "    mean_raw_obs_processing_ms: 1.2123766850665398\n",
      "  time_since_restore: 1445.9351778030396\n",
      "  time_this_iter_s: 16.961941957473755\n",
      "  time_total_s: 1445.9351778030396\n",
      "  timers:\n",
      "    learn_throughput: 259.579\n",
      "    learn_time_ms: 15409.595\n",
      "    sample_throughput: 5624.035\n",
      "    sample_time_ms: 711.233\n",
      "    update_time_ms: 2.476\n",
      "  timestamp: 1625110758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         1445.94</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">  32.412</td><td style=\"text-align: right;\">                49.5</td><td style=\"text-align: right;\">                10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-39-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 43.79999999999991\n",
      "  episode_reward_mean: 32.8589999999999\n",
      "  episode_reward_min: 18.29999999999989\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3400\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3206331096589565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007782569402479567\n",
      "          policy_loss: -0.023343132925219834\n",
      "          total_loss: 35.257325530052185\n",
      "          vf_explained_var: 0.5374382138252258\n",
      "          vf_loss: 35.272788286209106\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2257610373198986\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007928537292173132\n",
      "          policy_loss: -0.015732352971099317\n",
      "          total_loss: 4.044005990028381\n",
      "          vf_explained_var: 0.332594633102417\n",
      "          vf_loss: 4.051710739731789\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.888000000000005\n",
      "    ram_util_percent: 62.316\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.0\n",
      "    policy2: 12.000000000000016\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.1\n",
      "    policy2: 1.7590000000000026\n",
      "  policy_reward_min:\n",
      "    policy1: 11.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3017264389669122\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1675152743762007\n",
      "    mean_inference_ms: 3.352723066544523\n",
      "    mean_raw_obs_processing_ms: 1.211530006854717\n",
      "  time_since_restore: 1463.324316740036\n",
      "  time_this_iter_s: 17.38913893699646\n",
      "  time_total_s: 1463.324316740036\n",
      "  timers:\n",
      "    learn_throughput: 258.348\n",
      "    learn_time_ms: 15482.976\n",
      "    sample_throughput: 5589.496\n",
      "    sample_time_ms: 715.628\n",
      "    update_time_ms: 2.476\n",
      "  timestamp: 1625110776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1463.32</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">  32.859</td><td style=\"text-align: right;\">                43.8</td><td style=\"text-align: right;\">                18.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-39-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.5999999999999\n",
      "  episode_reward_mean: 33.17999999999991\n",
      "  episode_reward_min: 16.499999999999922\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3425\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.30223307851701975\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01105104829184711\n",
      "          policy_loss: -0.03933554410468787\n",
      "          total_loss: 61.824514746665955\n",
      "          vf_explained_var: 0.5247037410736084\n",
      "          vf_loss: 61.85266041755676\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.223191833589226\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0038454778150480706\n",
      "          policy_loss: -0.011687955469824374\n",
      "          total_loss: 18.459231540560722\n",
      "          vf_explained_var: 0.3494098484516144\n",
      "          vf_loss: 18.467025622725487\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_agent_steps_trained: 688000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.73478260869565\n",
      "    ram_util_percent: 61.54347826086955\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.0\n",
      "    policy2: 56.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.025\n",
      "    policy2: 2.155000000000002\n",
      "  policy_reward_min:\n",
      "    policy1: -29.0\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30190487369678015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1671968438520188\n",
      "    mean_inference_ms: 3.3496744946274273\n",
      "    mean_raw_obs_processing_ms: 1.2097865864925774\n",
      "  time_since_restore: 1479.8100876808167\n",
      "  time_this_iter_s: 16.48577094078064\n",
      "  time_total_s: 1479.8100876808167\n",
      "  timers:\n",
      "    learn_throughput: 260.11\n",
      "    learn_time_ms: 15378.123\n",
      "    sample_throughput: 5558.938\n",
      "    sample_time_ms: 719.562\n",
      "    update_time_ms: 2.487\n",
      "  timestamp: 1625110792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1479.81</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">   33.18</td><td style=\"text-align: right;\">                45.6</td><td style=\"text-align: right;\">                16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-40-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.6999999999999\n",
      "  episode_reward_mean: 33.32399999999991\n",
      "  episode_reward_min: 7.499999999999927\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3475\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.32161636743694544\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00792305514914915\n",
      "          policy_loss: -0.02969865084742196\n",
      "          total_loss: 37.84160125255585\n",
      "          vf_explained_var: 0.5959841012954712\n",
      "          vf_loss: 37.86327821016312\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2431740746833384\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016921613772865385\n",
      "          policy_loss: -0.027782924007624388\n",
      "          total_loss: 5.004935309290886\n",
      "          vf_explained_var: 0.2977822422981262\n",
      "          vf_loss: 5.024151600897312\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.12173913043478\n",
      "    ram_util_percent: 61.76521739130433\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.5\n",
      "    policy2: 56.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.795\n",
      "    policy2: 2.529000000000002\n",
      "  policy_reward_min:\n",
      "    policy1: -29.0\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30121980785282765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16682551649172936\n",
      "    mean_inference_ms: 3.340917992993561\n",
      "    mean_raw_obs_processing_ms: 1.2065828435675454\n",
      "  time_since_restore: 1495.540690422058\n",
      "  time_this_iter_s: 15.730602741241455\n",
      "  time_total_s: 1495.540690422058\n",
      "  timers:\n",
      "    learn_throughput: 258.83\n",
      "    learn_time_ms: 15454.134\n",
      "    sample_throughput: 5557.507\n",
      "    sample_time_ms: 719.747\n",
      "    update_time_ms: 2.519\n",
      "  timestamp: 1625110808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1495.54</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">  33.324</td><td style=\"text-align: right;\">                47.7</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-40-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.6999999999999\n",
      "  episode_reward_mean: 34.022999999999904\n",
      "  episode_reward_min: 7.499999999999927\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3500\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3082009954378009\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0075245433108648285\n",
      "          policy_loss: -0.021467555314302444\n",
      "          total_loss: 53.491371154785156\n",
      "          vf_explained_var: 0.43768495321273804\n",
      "          vf_loss: 53.505221247673035\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23131753643974662\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01255671882245224\n",
      "          policy_loss: -0.022163924091728404\n",
      "          total_loss: 6.070770263671875\n",
      "          vf_explained_var: 0.2727735638618469\n",
      "          vf_loss: 6.0865772515535355\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_agent_steps_trained: 704000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.085714285714282\n",
      "    ram_util_percent: 62.07619047619048\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.5\n",
      "    policy2: 56.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.395\n",
      "    policy2: 2.6280000000000023\n",
      "  policy_reward_min:\n",
      "    policy1: -29.0\n",
      "    policy2: -6.699999999999991\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3003101645523595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1667469266388384\n",
      "    mean_inference_ms: 3.335322797218387\n",
      "    mean_raw_obs_processing_ms: 1.2049195216950594\n",
      "  time_since_restore: 1510.2566752433777\n",
      "  time_this_iter_s: 14.71598482131958\n",
      "  time_total_s: 1510.2566752433777\n",
      "  timers:\n",
      "    learn_throughput: 258.844\n",
      "    learn_time_ms: 15453.327\n",
      "    sample_throughput: 5529.396\n",
      "    sample_time_ms: 723.406\n",
      "    update_time_ms: 2.497\n",
      "  timestamp: 1625110823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1510.26</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">  34.023</td><td style=\"text-align: right;\">                47.7</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-40-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.99999999999989\n",
      "  episode_reward_mean: 34.72799999999991\n",
      "  episode_reward_min: 7.499999999999927\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3550\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2936189705505967\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006276743049966171\n",
      "          policy_loss: -0.021215364278759807\n",
      "          total_loss: 41.04600924253464\n",
      "          vf_explained_var: 0.6234234571456909\n",
      "          vf_loss: 41.06086963415146\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.24515516916289926\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01746990010724403\n",
      "          policy_loss: -0.025900604436174035\n",
      "          total_loss: 2.923016019165516\n",
      "          vf_explained_var: 0.27347368001937866\n",
      "          vf_loss: 2.940072476863861\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.2\n",
      "    ram_util_percent: 62.045454545454525\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 18.599999999999973\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.2\n",
      "    policy2: 1.5280000000000014\n",
      "  policy_reward_min:\n",
      "    policy1: 6.0\n",
      "    policy2: -5.59999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2999850886169896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1661764939469681\n",
      "    mean_inference_ms: 3.3268878714618415\n",
      "    mean_raw_obs_processing_ms: 1.2014355535362657\n",
      "  time_since_restore: 1525.8733353614807\n",
      "  time_this_iter_s: 15.616660118103027\n",
      "  time_total_s: 1525.8733353614807\n",
      "  timers:\n",
      "    learn_throughput: 258.095\n",
      "    learn_time_ms: 15498.147\n",
      "    sample_throughput: 5538.523\n",
      "    sample_time_ms: 722.214\n",
      "    update_time_ms: 2.589\n",
      "  timestamp: 1625110838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1525.87</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">  34.728</td><td style=\"text-align: right;\">                  51</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-40-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.99999999999989\n",
      "  episode_reward_mean: 35.06399999999991\n",
      "  episode_reward_min: 16.799999999999905\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3600\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3105206345207989\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0076787130237789825\n",
      "          policy_loss: -0.019555308695998974\n",
      "          total_loss: 39.540516912937164\n",
      "          vf_explained_var: 0.5051263570785522\n",
      "          vf_loss: 39.5522980093956\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.24564399663358927\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014264068711781874\n",
      "          policy_loss: -0.03308245258813258\n",
      "          total_loss: 3.302666924893856\n",
      "          vf_explained_var: 0.27629610896110535\n",
      "          vf_loss: 3.328528180718422\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_agent_steps_trained: 720000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.25909090909091\n",
      "    ram_util_percent: 61.83636363636364\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 18.599999999999973\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.075\n",
      "    policy2: 0.9889999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: 6.0\n",
      "    policy2: -4.5000000000000036\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2992725402055184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16612729824851263\n",
      "    mean_inference_ms: 3.3224101043363987\n",
      "    mean_raw_obs_processing_ms: 1.20011293902882\n",
      "  time_since_restore: 1541.4820613861084\n",
      "  time_this_iter_s: 15.608726024627686\n",
      "  time_total_s: 1541.4820613861084\n",
      "  timers:\n",
      "    learn_throughput: 258.295\n",
      "    learn_time_ms: 15486.198\n",
      "    sample_throughput: 5315.533\n",
      "    sample_time_ms: 752.511\n",
      "    update_time_ms: 2.678\n",
      "  timestamp: 1625110854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1541.48</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">  35.064</td><td style=\"text-align: right;\">                  51</td><td style=\"text-align: right;\">                16.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-41-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.09999999999991\n",
      "  episode_reward_mean: 34.9229999999999\n",
      "  episode_reward_min: 16.799999999999905\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3625\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.29324008291587234\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007221027131890878\n",
      "          policy_loss: -0.017683978308923542\n",
      "          total_loss: 37.96734815835953\n",
      "          vf_explained_var: 0.610059916973114\n",
      "          vf_loss: 37.97772002220154\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22930576233193278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01225955474365037\n",
      "          policy_loss: -0.025540432863635942\n",
      "          total_loss: 3.8662231862545013\n",
      "          vf_explained_var: 0.264079213142395\n",
      "          vf_loss: 3.885557159781456\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.0\n",
      "    ram_util_percent: 62.16363636363638\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 10.900000000000015\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.78\n",
      "    policy2: 1.143\n",
      "  policy_reward_min:\n",
      "    policy1: 15.0\n",
      "    policy2: -6.699999999999985\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2997958232936251\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16596429418433586\n",
      "    mean_inference_ms: 3.3233691224403774\n",
      "    mean_raw_obs_processing_ms: 1.1999494402607995\n",
      "  time_since_restore: 1556.920806646347\n",
      "  time_this_iter_s: 15.438745260238647\n",
      "  time_total_s: 1556.920806646347\n",
      "  timers:\n",
      "    learn_throughput: 260.443\n",
      "    learn_time_ms: 15358.453\n",
      "    sample_throughput: 5244.489\n",
      "    sample_time_ms: 762.705\n",
      "    update_time_ms: 2.638\n",
      "  timestamp: 1625110870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1556.92</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">  34.923</td><td style=\"text-align: right;\">                50.1</td><td style=\"text-align: right;\">                16.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-41-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.09999999999991\n",
      "  episode_reward_mean: 34.35599999999991\n",
      "  episode_reward_min: 20.099999999999895\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3675\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.30507426522672176\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008362106789718382\n",
      "          policy_loss: -0.03387977530655917\n",
      "          total_loss: 30.464163541793823\n",
      "          vf_explained_var: 0.6516702175140381\n",
      "          vf_loss: 30.489576995372772\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22310831816866994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012112680575228296\n",
      "          policy_loss: -0.02295917007722892\n",
      "          total_loss: 3.5282256826758385\n",
      "          vf_explained_var: 0.33941614627838135\n",
      "          vf_loss: 3.5450527742505074\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_agent_steps_trained: 736000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.134999999999998\n",
      "    ram_util_percent: 61.42\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 16.400000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.08\n",
      "    policy2: 2.2760000000000002\n",
      "  policy_reward_min:\n",
      "    policy1: 16.0\n",
      "    policy2: -6.699999999999985\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29962115509461357\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16583542709735394\n",
      "    mean_inference_ms: 3.3210744998423807\n",
      "    mean_raw_obs_processing_ms: 1.199417267260288\n",
      "  time_since_restore: 1571.0803878307343\n",
      "  time_this_iter_s: 14.159581184387207\n",
      "  time_total_s: 1571.0803878307343\n",
      "  timers:\n",
      "    learn_throughput: 263.817\n",
      "    learn_time_ms: 15162.006\n",
      "    sample_throughput: 5329.844\n",
      "    sample_time_ms: 750.491\n",
      "    update_time_ms: 2.659\n",
      "  timestamp: 1625110884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         1571.08</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">  34.356</td><td style=\"text-align: right;\">                50.1</td><td style=\"text-align: right;\">                20.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-41-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.0999999999999\n",
      "  episode_reward_mean: 34.62299999999991\n",
      "  episode_reward_min: 22.799999999999955\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3700\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.3115471573546529\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006594996724743396\n",
      "          policy_loss: -0.018588885141070932\n",
      "          total_loss: 47.37114715576172\n",
      "          vf_explained_var: 0.4623723030090332\n",
      "          vf_loss: 47.38305854797363\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21785818645730615\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01179155771387741\n",
      "          policy_loss: -0.023272950696991757\n",
      "          total_loss: 4.996207907795906\n",
      "          vf_explained_var: 0.2874845266342163\n",
      "          vf_loss: 5.013511285185814\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.41363636363636\n",
      "    ram_util_percent: 61.37727272727275\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 16.400000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.995\n",
      "    policy2: 2.628\n",
      "  policy_reward_min:\n",
      "    policy1: 14.0\n",
      "    policy2: -6.699999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29882910190847684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16581467690776844\n",
      "    mean_inference_ms: 3.3167247154997623\n",
      "    mean_raw_obs_processing_ms: 1.1985660258284394\n",
      "  time_since_restore: 1586.2318966388702\n",
      "  time_this_iter_s: 15.151508808135986\n",
      "  time_total_s: 1586.2318966388702\n",
      "  timers:\n",
      "    learn_throughput: 266.831\n",
      "    learn_time_ms: 14990.734\n",
      "    sample_throughput: 5515.818\n",
      "    sample_time_ms: 725.187\n",
      "    update_time_ms: 2.622\n",
      "  timestamp: 1625110899\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1586.23</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">  34.623</td><td style=\"text-align: right;\">                47.1</td><td style=\"text-align: right;\">                22.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-41-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.8999999999999\n",
      "  episode_reward_mean: 34.3259999999999\n",
      "  episode_reward_min: 20.69999999999991\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3750\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.27907597040757537\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0060969748155912384\n",
      "          policy_loss: -0.026106120640179142\n",
      "          total_loss: 31.99796837568283\n",
      "          vf_explained_var: 0.6687365770339966\n",
      "          vf_loss: 32.01790100336075\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22003423562273383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014186309097567573\n",
      "          policy_loss: -0.022256196447415277\n",
      "          total_loss: 2.887546494603157\n",
      "          vf_explained_var: 0.306301087141037\n",
      "          vf_loss: 2.9026208594441414\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_agent_steps_trained: 752000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.304761904761904\n",
      "    ram_util_percent: 61.495238095238086\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 16.400000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.83\n",
      "    policy2: 2.496\n",
      "  policy_reward_min:\n",
      "    policy1: 14.0\n",
      "    policy2: -6.699999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29874574414931165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16537565421233918\n",
      "    mean_inference_ms: 3.310655755694995\n",
      "    mean_raw_obs_processing_ms: 1.1962271920934398\n",
      "  time_since_restore: 1600.7612438201904\n",
      "  time_this_iter_s: 14.52934718132019\n",
      "  time_total_s: 1600.7612438201904\n",
      "  timers:\n",
      "    learn_throughput: 271.241\n",
      "    learn_time_ms: 14747.038\n",
      "    sample_throughput: 5511.733\n",
      "    sample_time_ms: 725.725\n",
      "    update_time_ms: 2.599\n",
      "  timestamp: 1625110914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1600.76</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">  34.326</td><td style=\"text-align: right;\">                51.9</td><td style=\"text-align: right;\">                20.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-42-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.8999999999999\n",
      "  episode_reward_mean: 34.970999999999904\n",
      "  episode_reward_min: 20.69999999999991\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3800\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.29612492490559816\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0077678941015619785\n",
      "          policy_loss: -0.031062833368196152\n",
      "          total_loss: 33.28314524888992\n",
      "          vf_explained_var: 0.5937671661376953\n",
      "          vf_loss: 33.30634289979935\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23302105255424976\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012709193091723137\n",
      "          policy_loss: -0.0256960645201616\n",
      "          total_loss: 5.840308360755444\n",
      "          vf_explained_var: 0.23388147354125977\n",
      "          vf_loss: 5.859570488333702\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.357142857142858\n",
      "    ram_util_percent: 61.5095238095238\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 17.500000000000007\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.87\n",
      "    policy2: 3.101000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: 12.5\n",
      "    policy2: -5.599999999999991\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2976703015516407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16520400744741626\n",
      "    mean_inference_ms: 3.3024488885097885\n",
      "    mean_raw_obs_processing_ms: 1.1938513253611582\n",
      "  time_since_restore: 1615.8286159038544\n",
      "  time_this_iter_s: 15.06737208366394\n",
      "  time_total_s: 1615.8286159038544\n",
      "  timers:\n",
      "    learn_throughput: 275.33\n",
      "    learn_time_ms: 14528.045\n",
      "    sample_throughput: 5611.931\n",
      "    sample_time_ms: 712.767\n",
      "    update_time_ms: 2.537\n",
      "  timestamp: 1625110929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1615.83</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">  34.971</td><td style=\"text-align: right;\">                51.9</td><td style=\"text-align: right;\">                20.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 768000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-42-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.8999999999999\n",
      "  episode_reward_mean: 35.36999999999991\n",
      "  episode_reward_min: 20.69999999999991\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3825\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2776964367367327\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005776785059424583\n",
      "          policy_loss: -0.019745711993891746\n",
      "          total_loss: 33.27989602088928\n",
      "          vf_explained_var: 0.658515214920044\n",
      "          vf_loss: 33.293793082237244\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.24219743581488729\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014205411192961037\n",
      "          policy_loss: -0.03692645678529516\n",
      "          total_loss: 4.697096571326256\n",
      "          vf_explained_var: 0.28718066215515137\n",
      "          vf_loss: 4.726831540465355\n",
      "    num_agent_steps_sampled: 768000\n",
      "    num_agent_steps_trained: 768000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.391666666666666\n",
      "    ram_util_percent: 61.81666666666666\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 17.500000000000007\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.95\n",
      "    policy2: 3.4200000000000013\n",
      "  policy_reward_min:\n",
      "    policy1: 12.5\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2977513645381037\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16485275133375474\n",
      "    mean_inference_ms: 3.2988903565320116\n",
      "    mean_raw_obs_processing_ms: 1.1918440019892633\n",
      "  time_since_restore: 1632.663117647171\n",
      "  time_this_iter_s: 16.83450174331665\n",
      "  time_total_s: 1632.663117647171\n",
      "  timers:\n",
      "    learn_throughput: 274.569\n",
      "    learn_time_ms: 14568.272\n",
      "    sample_throughput: 5653.9\n",
      "    sample_time_ms: 707.476\n",
      "    update_time_ms: 2.527\n",
      "  timestamp: 1625110946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1632.66</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">   35.37</td><td style=\"text-align: right;\">                45.9</td><td style=\"text-align: right;\">                20.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-42-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.8999999999999\n",
      "  episode_reward_mean: 35.8799999999999\n",
      "  episode_reward_min: 19.799999999999912\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3875\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2829280002042651\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007355675392318517\n",
      "          policy_loss: -0.019168791361153126\n",
      "          total_loss: 34.18053990602493\n",
      "          vf_explained_var: 0.6561027765274048\n",
      "          vf_loss: 34.19226098060608\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2395539660938084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011849016911583021\n",
      "          policy_loss: -0.029323850030777976\n",
      "          total_loss: 4.7967541217803955\n",
      "          vf_explained_var: 0.2726495563983917\n",
      "          vf_loss: 4.820079356431961\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_agent_steps_trained: 776000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.43333333333334\n",
      "    ram_util_percent: 61.90476190476191\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.0\n",
      "    policy2: 16.400000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.57\n",
      "    policy2: 3.3100000000000027\n",
      "  policy_reward_min:\n",
      "    policy1: 10.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29691851451193607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16442748058159043\n",
      "    mean_inference_ms: 3.2891018868951973\n",
      "    mean_raw_obs_processing_ms: 1.188348858658993\n",
      "  time_since_restore: 1647.2247545719147\n",
      "  time_this_iter_s: 14.561636924743652\n",
      "  time_total_s: 1647.2247545719147\n",
      "  timers:\n",
      "    learn_throughput: 276.748\n",
      "    learn_time_ms: 14453.562\n",
      "    sample_throughput: 5670.954\n",
      "    sample_time_ms: 705.349\n",
      "    update_time_ms: 2.508\n",
      "  timestamp: 1625110960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1647.22</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">   35.88</td><td style=\"text-align: right;\">                45.9</td><td style=\"text-align: right;\">                19.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-42-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.299999999999905\n",
      "  episode_reward_mean: 35.28299999999991\n",
      "  episode_reward_min: 19.799999999999912\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3900\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2834771778434515\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006141318059235346\n",
      "          policy_loss: -0.016993618512060493\n",
      "          total_loss: 52.25935709476471\n",
      "          vf_explained_var: 0.5137035846710205\n",
      "          vf_loss: 52.270132184028625\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2183241662569344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009894012386212125\n",
      "          policy_loss: -0.016643081849906594\n",
      "          total_loss: 17.013265937566757\n",
      "          vf_explained_var: 0.2855546772480011\n",
      "          vf_loss: 17.024899914860725\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_agent_steps_trained: 784000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.77916666666667\n",
      "    ram_util_percent: 61.49166666666667\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 44.5\n",
      "    policy2: 43.899999999999984\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.775\n",
      "    policy2: 3.508000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -4.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29619914584174317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.164453116530627\n",
      "    mean_inference_ms: 3.2849695637534553\n",
      "    mean_raw_obs_processing_ms: 1.187658906126473\n",
      "  time_since_restore: 1663.6182746887207\n",
      "  time_this_iter_s: 16.39352011680603\n",
      "  time_total_s: 1663.6182746887207\n",
      "  timers:\n",
      "    learn_throughput: 273.891\n",
      "    learn_time_ms: 14604.349\n",
      "    sample_throughput: 5538.202\n",
      "    sample_time_ms: 722.256\n",
      "    update_time_ms: 2.531\n",
      "  timestamp: 1625110977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1663.62</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">  35.283</td><td style=\"text-align: right;\">                45.3</td><td style=\"text-align: right;\">                19.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-43-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 35.171999999999905\n",
      "  episode_reward_min: 19.799999999999912\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3950\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2654889728873968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0064079228905029595\n",
      "          policy_loss: -0.017270981101319194\n",
      "          total_loss: 39.83958142995834\n",
      "          vf_explained_var: 0.6194511651992798\n",
      "          vf_loss: 39.85036474466324\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2288711559958756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016964149035629816\n",
      "          policy_loss: -0.02652163727907464\n",
      "          total_loss: 9.174199037253857\n",
      "          vf_explained_var: 0.2388264238834381\n",
      "          vf_loss: 9.192132569849491\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_agent_steps_trained: 792000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.35909090909092\n",
      "    ram_util_percent: 61.76818181818181\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 43.899999999999984\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.73\n",
      "    policy2: 3.4419999999999993\n",
      "  policy_reward_min:\n",
      "    policy1: -4.0\n",
      "    policy2: -6.699999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2963178256791921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1641407881261744\n",
      "    mean_inference_ms: 3.2813837494093576\n",
      "    mean_raw_obs_processing_ms: 1.1863202531917092\n",
      "  time_since_restore: 1679.5444095134735\n",
      "  time_this_iter_s: 15.926134824752808\n",
      "  time_total_s: 1679.5444095134735\n",
      "  timers:\n",
      "    learn_throughput: 273.305\n",
      "    learn_time_ms: 14635.644\n",
      "    sample_throughput: 5533.369\n",
      "    sample_time_ms: 722.887\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1625110992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1679.54</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  35.172</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                19.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 800000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-43-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 35.41799999999991\n",
      "  episode_reward_min: 3.5999999999999446\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4000\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2996889129281044\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0076964655600022525\n",
      "          policy_loss: -0.01908352685859427\n",
      "          total_loss: 40.998607099056244\n",
      "          vf_explained_var: 0.49544084072113037\n",
      "          vf_loss: 41.00989830493927\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.24094444466754794\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017073584080208093\n",
      "          policy_loss: -0.024719986133277416\n",
      "          total_loss: 4.69488450139761\n",
      "          vf_explained_var: 0.2028854787349701\n",
      "          vf_loss: 4.7109609469771385\n",
      "    num_agent_steps_sampled: 800000\n",
      "    num_agent_steps_trained: 800000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.369565217391308\n",
      "    ram_util_percent: 62.07826086956521\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 38.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.735\n",
      "    policy2: 2.6830000000000007\n",
      "  policy_reward_min:\n",
      "    policy1: -1.5\n",
      "    policy2: -6.699999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29527442393412784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16401778362198605\n",
      "    mean_inference_ms: 3.2738003461112077\n",
      "    mean_raw_obs_processing_ms: 1.18420900597032\n",
      "  time_since_restore: 1695.791312456131\n",
      "  time_this_iter_s: 16.24690294265747\n",
      "  time_total_s: 1695.791312456131\n",
      "  timers:\n",
      "    learn_throughput: 271.614\n",
      "    learn_time_ms: 14726.802\n",
      "    sample_throughput: 5756.938\n",
      "    sample_time_ms: 694.814\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1625111009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1695.79</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">  35.418</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                 3.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 808000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-43-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 54.59999999999991\n",
      "  episode_reward_mean: 35.909999999999904\n",
      "  episode_reward_min: 3.5999999999999446\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4025\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2836586721241474\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007766731956508011\n",
      "          policy_loss: -0.02051701524760574\n",
      "          total_loss: 49.495555996894836\n",
      "          vf_explained_var: 0.5558496713638306\n",
      "          vf_loss: 49.50820851325989\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23087594518437982\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013697460512048565\n",
      "          policy_loss: -0.027901159541215748\n",
      "          total_loss: 3.861396625638008\n",
      "          vf_explained_var: 0.25941574573516846\n",
      "          vf_loss: 3.8823635429143906\n",
      "    num_agent_steps_sampled: 808000\n",
      "    num_agent_steps_trained: 808000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.92592592592593\n",
      "    ram_util_percent: 61.74074074074074\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 58.0\n",
      "    policy2: 38.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.7\n",
      "    policy2: 2.2100000000000013\n",
      "  policy_reward_min:\n",
      "    policy1: -1.5\n",
      "    policy2: -6.699999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29551404627371725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16373176921219484\n",
      "    mean_inference_ms: 3.2721186901815145\n",
      "    mean_raw_obs_processing_ms: 1.1829279447561099\n",
      "  time_since_restore: 1714.5897042751312\n",
      "  time_this_iter_s: 18.798391819000244\n",
      "  time_total_s: 1714.5897042751312\n",
      "  timers:\n",
      "    learn_throughput: 265.525\n",
      "    learn_time_ms: 15064.475\n",
      "    sample_throughput: 5772.22\n",
      "    sample_time_ms: 692.974\n",
      "    update_time_ms: 2.48\n",
      "  timestamp: 1625111028\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         1714.59</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">   35.91</td><td style=\"text-align: right;\">                54.6</td><td style=\"text-align: right;\">                 3.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 816000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-44-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 54.59999999999991\n",
      "  episode_reward_mean: 36.146999999999906\n",
      "  episode_reward_min: 14.699999999999909\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4075\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.28829260542988777\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008042376401135698\n",
      "          policy_loss: -0.023153640038799495\n",
      "          total_loss: 37.82472234964371\n",
      "          vf_explained_var: 0.62677401304245\n",
      "          vf_loss: 37.839733600616455\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2422818667255342\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013946710008895025\n",
      "          policy_loss: -0.02453091208008118\n",
      "          total_loss: 3.861992470920086\n",
      "          vf_explained_var: 0.27950233221054077\n",
      "          vf_loss: 3.879462853074074\n",
      "    num_agent_steps_sampled: 816000\n",
      "    num_agent_steps_trained: 816000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.22608695652173\n",
      "    ram_util_percent: 61.70434782608695\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 58.0\n",
      "    policy2: 15.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.41\n",
      "    policy2: 1.7370000000000005\n",
      "  policy_reward_min:\n",
      "    policy1: 11.5\n",
      "    policy2: -6.699999999999994\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2950690496019847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16351774704610328\n",
      "    mean_inference_ms: 3.267608389432425\n",
      "    mean_raw_obs_processing_ms: 1.1813908927351189\n",
      "  time_since_restore: 1730.6925072669983\n",
      "  time_this_iter_s: 16.102802991867065\n",
      "  time_total_s: 1730.6925072669983\n",
      "  timers:\n",
      "    learn_throughput: 262.125\n",
      "    learn_time_ms: 15259.892\n",
      "    sample_throughput: 5780.111\n",
      "    sample_time_ms: 692.028\n",
      "    update_time_ms: 2.407\n",
      "  timestamp: 1625111044\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         1730.69</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">  36.147</td><td style=\"text-align: right;\">                54.6</td><td style=\"text-align: right;\">                14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 824000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-44-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 54.59999999999991\n",
      "  episode_reward_mean: 36.18899999999991\n",
      "  episode_reward_min: 14.699999999999909\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4100\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2918240646831691\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006812435181927867\n",
      "          policy_loss: -0.013199584267567843\n",
      "          total_loss: 64.1449259519577\n",
      "          vf_explained_var: 0.45528942346572876\n",
      "          vf_loss: 64.15122902393341\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2069645202718675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010313613078324124\n",
      "          policy_loss: -0.021271296602208167\n",
      "          total_loss: 28.184291124343872\n",
      "          vf_explained_var: 0.2676231265068054\n",
      "          vf_loss: 28.20034122467041\n",
      "    num_agent_steps_sampled: 824000\n",
      "    num_agent_steps_trained: 824000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.66818181818182\n",
      "    ram_util_percent: 61.79999999999998\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 58.0\n",
      "    policy2: 39.499999999999986\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.66\n",
      "    policy2: 2.529\n",
      "  policy_reward_min:\n",
      "    policy1: -0.5\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2943520236817434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1635619252413063\n",
      "    mean_inference_ms: 3.264182855697832\n",
      "    mean_raw_obs_processing_ms: 1.1808951597983912\n",
      "  time_since_restore: 1746.165339231491\n",
      "  time_this_iter_s: 15.472831964492798\n",
      "  time_total_s: 1746.165339231491\n",
      "  timers:\n",
      "    learn_throughput: 261.647\n",
      "    learn_time_ms: 15287.792\n",
      "    sample_throughput: 5746.568\n",
      "    sample_time_ms: 696.068\n",
      "    update_time_ms: 2.439\n",
      "  timestamp: 1625111059\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         1746.17</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">  36.189</td><td style=\"text-align: right;\">                54.6</td><td style=\"text-align: right;\">                14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 832000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-44-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.19999999999992\n",
      "  episode_reward_mean: 35.59499999999991\n",
      "  episode_reward_min: 15.299999999999901\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4150\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2741573555395007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006900431224494241\n",
      "          policy_loss: -0.021427681029308587\n",
      "          total_loss: 45.07684409618378\n",
      "          vf_explained_var: 0.5862284302711487\n",
      "          vf_loss: 45.09128439426422\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21937383199110627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013600820617284626\n",
      "          policy_loss: -0.02764156658668071\n",
      "          total_loss: 10.047201961278915\n",
      "          vf_explained_var: 0.2688412666320801\n",
      "          vf_loss: 10.06795810163021\n",
      "    num_agent_steps_sampled: 832000\n",
      "    num_agent_steps_trained: 832000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.652380952380955\n",
      "    ram_util_percent: 61.733333333333334\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 45.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.13\n",
      "    policy2: 4.464999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -4.5\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2941301191421611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16306141336644192\n",
      "    mean_inference_ms: 3.2571668196997403\n",
      "    mean_raw_obs_processing_ms: 1.1779766118275707\n",
      "  time_since_restore: 1760.5262973308563\n",
      "  time_this_iter_s: 14.360958099365234\n",
      "  time_total_s: 1760.5262973308563\n",
      "  timers:\n",
      "    learn_throughput: 261.664\n",
      "    learn_time_ms: 15286.759\n",
      "    sample_throughput: 5880.372\n",
      "    sample_time_ms: 680.229\n",
      "    update_time_ms: 2.415\n",
      "  timestamp: 1625111074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         1760.53</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">  35.595</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                15.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-44-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.89999999999991\n",
      "  episode_reward_mean: 35.9969999999999\n",
      "  episode_reward_min: 20.699999999999896\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4200\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.28605205193161964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010135989359696396\n",
      "          policy_loss: -0.024289785185828805\n",
      "          total_loss: 53.171104311943054\n",
      "          vf_explained_var: 0.47232311964035034\n",
      "          vf_loss: 53.18513059616089\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2215081756003201\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009004315696074627\n",
      "          policy_loss: -0.021569500619079918\n",
      "          total_loss: 16.934970498085022\n",
      "          vf_explained_var: 0.3745993673801422\n",
      "          vf_loss: 16.95198154449463\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.845\n",
      "    ram_util_percent: 61.61499999999999\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 54.89999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.555\n",
      "    policy2: 3.441999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -22.5\n",
      "    policy2: -6.699999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2930217695582684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16288272801312484\n",
      "    mean_inference_ms: 3.2486487755809104\n",
      "    mean_raw_obs_processing_ms: 1.1754583369729799\n",
      "  time_since_restore: 1774.8970923423767\n",
      "  time_this_iter_s: 14.370795011520386\n",
      "  time_total_s: 1774.8970923423767\n",
      "  timers:\n",
      "    learn_throughput: 262.948\n",
      "    learn_time_ms: 15212.119\n",
      "    sample_throughput: 5836.811\n",
      "    sample_time_ms: 685.306\n",
      "    update_time_ms: 2.429\n",
      "  timestamp: 1625111088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">          1774.9</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">  35.997</td><td style=\"text-align: right;\">                51.9</td><td style=\"text-align: right;\">                20.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 848000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.89999999999991\n",
      "  episode_reward_mean: 36.33299999999991\n",
      "  episode_reward_min: 20.699999999999896\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4225\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.27026319690048695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006855769461253658\n",
      "          policy_loss: -0.02048995642689988\n",
      "          total_loss: 45.55350911617279\n",
      "          vf_explained_var: 0.6111595630645752\n",
      "          vf_loss: 45.56705713272095\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2180658676661551\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011622255551628768\n",
      "          policy_loss: -0.022375551227014512\n",
      "          total_loss: 9.22431880235672\n",
      "          vf_explained_var: 0.19276010990142822\n",
      "          vf_loss: 9.240810573101044\n",
      "    num_agent_steps_sampled: 848000\n",
      "    num_agent_steps_trained: 848000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.033333333333335\n",
      "    ram_util_percent: 61.50952380952381\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 54.89999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.605\n",
      "    policy2: 3.727999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: -22.5\n",
      "    policy2: -6.699999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29322321152998804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16258125153306696\n",
      "    mean_inference_ms: 3.2463372415003047\n",
      "    mean_raw_obs_processing_ms: 1.1738226076415155\n",
      "  time_since_restore: 1789.4692523479462\n",
      "  time_this_iter_s: 14.572160005569458\n",
      "  time_total_s: 1789.4692523479462\n",
      "  timers:\n",
      "    learn_throughput: 266.949\n",
      "    learn_time_ms: 14984.108\n",
      "    sample_throughput: 5821.557\n",
      "    sample_time_ms: 687.101\n",
      "    update_time_ms: 2.412\n",
      "  timestamp: 1625111103\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         1789.47</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">  36.333</td><td style=\"text-align: right;\">                51.9</td><td style=\"text-align: right;\">                20.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 856000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-45-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.899999999999906\n",
      "  episode_reward_mean: 36.131999999999906\n",
      "  episode_reward_min: 14.700000000000026\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4275\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.27970315515995026\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0068431787367444485\n",
      "          policy_loss: -0.017431623593438417\n",
      "          total_loss: 39.11892366409302\n",
      "          vf_explained_var: 0.6292023062705994\n",
      "          vf_loss: 39.12942677736282\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23580879252403975\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01812370226252824\n",
      "          policy_loss: -0.029718765057623386\n",
      "          total_loss: 4.52793250977993\n",
      "          vf_explained_var: 0.2241785228252411\n",
      "          vf_loss: 4.548476226627827\n",
      "    num_agent_steps_sampled: 856000\n",
      "    num_agent_steps_trained: 856000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.70476190476191\n",
      "    ram_util_percent: 61.62380952380952\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 54.89999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.285\n",
      "    policy2: 1.846999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -22.5\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29260545038190455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16224532145442833\n",
      "    mean_inference_ms: 3.2387853060531024\n",
      "    mean_raw_obs_processing_ms: 1.1710993308195825\n",
      "  time_since_restore: 1803.7693433761597\n",
      "  time_this_iter_s: 14.300091028213501\n",
      "  time_total_s: 1803.7693433761597\n",
      "  timers:\n",
      "    learn_throughput: 267.413\n",
      "    learn_time_ms: 14958.159\n",
      "    sample_throughput: 5823.631\n",
      "    sample_time_ms: 686.857\n",
      "    update_time_ms: 2.432\n",
      "  timestamp: 1625111117\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         1803.77</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">  36.132</td><td style=\"text-align: right;\">                45.9</td><td style=\"text-align: right;\">                14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 864000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-45-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.899999999999906\n",
      "  episode_reward_mean: 35.96999999999991\n",
      "  episode_reward_min: 14.700000000000026\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4300\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2770256018266082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0073635741719044745\n",
      "          policy_loss: -0.019001650740392506\n",
      "          total_loss: 43.169180035591125\n",
      "          vf_explained_var: 0.5191107988357544\n",
      "          vf_loss: 43.18072581291199\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2253632778301835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014742273895535618\n",
      "          policy_loss: -0.03645801251695957\n",
      "          total_loss: 3.188169814646244\n",
      "          vf_explained_var: 0.20495711266994476\n",
      "          vf_loss: 3.2171645388007164\n",
      "    num_agent_steps_sampled: 864000\n",
      "    num_agent_steps_trained: 864000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.157894736842106\n",
      "    ram_util_percent: 61.8842105263158\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 47.0\n",
      "    policy2: 28.499999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.97\n",
      "    policy2: 0.9999999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: 13.5\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29174385204063114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16218743691815476\n",
      "    mean_inference_ms: 3.2331848611346823\n",
      "    mean_raw_obs_processing_ms: 1.169741624063118\n",
      "  time_since_restore: 1817.5784902572632\n",
      "  time_this_iter_s: 13.809146881103516\n",
      "  time_total_s: 1817.5784902572632\n",
      "  timers:\n",
      "    learn_throughput: 271.673\n",
      "    learn_time_ms: 14723.561\n",
      "    sample_throughput: 6032.483\n",
      "    sample_time_ms: 663.077\n",
      "    update_time_ms: 2.425\n",
      "  timestamp: 1625111131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         1817.58</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">   35.97</td><td style=\"text-align: right;\">                45.9</td><td style=\"text-align: right;\">                14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 872000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-45-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 36.27299999999991\n",
      "  episode_reward_min: 20.699999999999896\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4350\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2592877075076103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006118778212112375\n",
      "          policy_loss: -0.013852222706191242\n",
      "          total_loss: 34.52606463432312\n",
      "          vf_explained_var: 0.6525859832763672\n",
      "          vf_loss: 34.53372144699097\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2241171202622354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01258031779434532\n",
      "          policy_loss: -0.024947438068920746\n",
      "          total_loss: 3.23670407384634\n",
      "          vf_explained_var: 0.22943460941314697\n",
      "          vf_loss: 3.255282774567604\n",
      "    num_agent_steps_sampled: 872000\n",
      "    num_agent_steps_trained: 872000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.314999999999998\n",
      "    ram_util_percent: 61.834999999999994\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 10.900000000000015\n",
      "  policy_reward_mean:\n",
      "    policy1: 35.79\n",
      "    policy2: 0.48299999999999904\n",
      "  policy_reward_min:\n",
      "    policy1: 20.5\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2916012713801433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16172525902893098\n",
      "    mean_inference_ms: 3.2267900216277496\n",
      "    mean_raw_obs_processing_ms: 1.1670091492141643\n",
      "  time_since_restore: 1831.678962945938\n",
      "  time_this_iter_s: 14.100472688674927\n",
      "  time_total_s: 1831.678962945938\n",
      "  timers:\n",
      "    learn_throughput: 275.178\n",
      "    learn_time_ms: 14536.04\n",
      "    sample_throughput: 5988.317\n",
      "    sample_time_ms: 667.967\n",
      "    update_time_ms: 2.438\n",
      "  timestamp: 1625111145\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         1831.68</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">  36.273</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                20.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 880000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-45-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 35.903999999999904\n",
      "  episode_reward_min: 17.099999999999902\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4400\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2835629410110414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006559956993442029\n",
      "          policy_loss: -0.01645479310536757\n",
      "          total_loss: 44.392465114593506\n",
      "          vf_explained_var: 0.45895153284072876\n",
      "          vf_loss: 44.402278423309326\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23380615003407001\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015592938550980762\n",
      "          policy_loss: -0.03144212576444261\n",
      "          total_loss: 3.05178552120924\n",
      "          vf_explained_var: 0.2233591079711914\n",
      "          vf_loss: 3.075333781540394\n",
      "    num_agent_steps_sampled: 880000\n",
      "    num_agent_steps_trained: 880000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.685\n",
      "    ram_util_percent: 61.594999999999985\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 10.900000000000015\n",
      "  policy_reward_mean:\n",
      "    policy1: 35.41\n",
      "    policy2: 0.4939999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: 17.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2906349232305393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16162241491870216\n",
      "    mean_inference_ms: 3.219837879989786\n",
      "    mean_raw_obs_processing_ms: 1.1650047948575855\n",
      "  time_since_restore: 1845.3188080787659\n",
      "  time_this_iter_s: 13.639845132827759\n",
      "  time_total_s: 1845.3188080787659\n",
      "  timers:\n",
      "    learn_throughput: 280.25\n",
      "    learn_time_ms: 14272.968\n",
      "    sample_throughput: 5958.697\n",
      "    sample_time_ms: 671.288\n",
      "    update_time_ms: 2.319\n",
      "  timestamp: 1625111159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         1845.32</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">  35.904</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                17.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 888000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-46-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.8999999999999\n",
      "  episode_reward_mean: 35.711999999999904\n",
      "  episode_reward_min: 11.399999999999913\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4425\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2562564886175096\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007741676803561859\n",
      "          policy_loss: -0.02819039062887896\n",
      "          total_loss: 45.992348432540894\n",
      "          vf_explained_var: 0.6086419820785522\n",
      "          vf_loss: 46.01270067691803\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22480955021455884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014497204654617235\n",
      "          policy_loss: -0.027734191302442923\n",
      "          total_loss: 2.8901764303445816\n",
      "          vf_explained_var: 0.18784204125404358\n",
      "          vf_loss: 2.9105713292956352\n",
      "    num_agent_steps_sampled: 888000\n",
      "    num_agent_steps_trained: 888000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.695\n",
      "    ram_util_percent: 61.735\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 10.900000000000015\n",
      "  policy_reward_mean:\n",
      "    policy1: 35.625\n",
      "    policy2: 0.08699999999999956\n",
      "  policy_reward_min:\n",
      "    policy1: 17.0\n",
      "    policy2: -6.6999999999999895\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2907730872499763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16130179444114884\n",
      "    mean_inference_ms: 3.216926425793184\n",
      "    mean_raw_obs_processing_ms: 1.163217925069956\n",
      "  time_since_restore: 1859.3198750019073\n",
      "  time_this_iter_s: 14.00106692314148\n",
      "  time_total_s: 1859.3198750019073\n",
      "  timers:\n",
      "    learn_throughput: 289.563\n",
      "    learn_time_ms: 13813.902\n",
      "    sample_throughput: 6145.925\n",
      "    sample_time_ms: 650.838\n",
      "    update_time_ms: 2.236\n",
      "  timestamp: 1625111173\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         1859.32</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">  35.712</td><td style=\"text-align: right;\">                51.9</td><td style=\"text-align: right;\">                11.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 896000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-46-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.8999999999999\n",
      "  episode_reward_mean: 35.0489999999999\n",
      "  episode_reward_min: 11.399999999999913\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4475\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2717755059711635\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0074429627857171\n",
      "          policy_loss: -0.022095425403676927\n",
      "          total_loss: 36.2455113530159\n",
      "          vf_explained_var: 0.658694863319397\n",
      "          vf_loss: 36.26007068157196\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2313003488816321\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014020220609381795\n",
      "          policy_loss: -0.03493007237557322\n",
      "          total_loss: 2.37819354981184\n",
      "          vf_explained_var: 0.2912600040435791\n",
      "          vf_loss: 2.406025905162096\n",
      "    num_agent_steps_sampled: 896000\n",
      "    num_agent_steps_trained: 896000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.19\n",
      "    ram_util_percent: 62.054999999999986\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 10.899999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.885\n",
      "    policy2: 0.1639999999999993\n",
      "  policy_reward_min:\n",
      "    policy1: 17.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29009453211957775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16094530121576053\n",
      "    mean_inference_ms: 3.2088450696309305\n",
      "    mean_raw_obs_processing_ms: 1.1603619298869896\n",
      "  time_since_restore: 1873.3688218593597\n",
      "  time_this_iter_s: 14.048946857452393\n",
      "  time_total_s: 1873.3688218593597\n",
      "  timers:\n",
      "    learn_throughput: 293.838\n",
      "    learn_time_ms: 13612.922\n",
      "    sample_throughput: 6188.185\n",
      "    sample_time_ms: 646.393\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1625111187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         1873.37</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">  35.049</td><td style=\"text-align: right;\">                51.9</td><td style=\"text-align: right;\">                11.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 904000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-46-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.8999999999999\n",
      "  episode_reward_mean: 34.439999999999905\n",
      "  episode_reward_min: 11.399999999999913\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4500\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.27741820691153407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00654564805154223\n",
      "          policy_loss: -0.026817948615644127\n",
      "          total_loss: 43.92541819810867\n",
      "          vf_explained_var: 0.4934638440608978\n",
      "          vf_loss: 43.945608615875244\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2128001330420375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011703058873536065\n",
      "          policy_loss: -0.029215526359621435\n",
      "          total_loss: 3.4336534440517426\n",
      "          vf_explained_var: 0.24739453196525574\n",
      "          vf_loss: 3.4569442495703697\n",
      "    num_agent_steps_sampled: 904000\n",
      "    num_agent_steps_trained: 904000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.59\n",
      "    ram_util_percent: 61.709999999999994\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 14.200000000000012\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.54\n",
      "    policy2: -0.1000000000000009\n",
      "  policy_reward_min:\n",
      "    policy1: 17.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.289313077820691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16093625782316534\n",
      "    mean_inference_ms: 3.2040193788241833\n",
      "    mean_raw_obs_processing_ms: 1.1593389450383957\n",
      "  time_since_restore: 1887.536050081253\n",
      "  time_this_iter_s: 14.16722822189331\n",
      "  time_total_s: 1887.536050081253\n",
      "  timers:\n",
      "    learn_throughput: 296.755\n",
      "    learn_time_ms: 13479.112\n",
      "    sample_throughput: 6159.444\n",
      "    sample_time_ms: 649.409\n",
      "    update_time_ms: 2.245\n",
      "  timestamp: 1625111201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         1887.54</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">   34.44</td><td style=\"text-align: right;\">                51.9</td><td style=\"text-align: right;\">                11.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 912000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 34.53299999999991\n",
      "  episode_reward_min: 12.599999999999921\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4550\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.24796770233660936\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006497509493783582\n",
      "          policy_loss: -0.025395021948497742\n",
      "          total_loss: 37.1281304359436\n",
      "          vf_explained_var: 0.6683326959609985\n",
      "          vf_loss: 37.14694797992706\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2063738340511918\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011706537610734813\n",
      "          policy_loss: -0.018487993991584517\n",
      "          total_loss: 3.776002824306488\n",
      "          vf_explained_var: 0.25419336557388306\n",
      "          vf_loss: 3.7885644137859344\n",
      "    num_agent_steps_sampled: 912000\n",
      "    num_agent_steps_trained: 912000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.520000000000003\n",
      "    ram_util_percent: 61.34500000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 14.200000000000012\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.665\n",
      "    policy2: 0.867999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: 16.0\n",
      "    policy2: -6.6999999999999815\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28937340612212514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16057742901866387\n",
      "    mean_inference_ms: 3.1998116798625746\n",
      "    mean_raw_obs_processing_ms: 1.1574771232470846\n",
      "  time_since_restore: 1901.874102115631\n",
      "  time_this_iter_s: 14.338052034378052\n",
      "  time_total_s: 1901.874102115631\n",
      "  timers:\n",
      "    learn_throughput: 296.983\n",
      "    learn_time_ms: 13468.787\n",
      "    sample_throughput: 6083.744\n",
      "    sample_time_ms: 657.49\n",
      "    update_time_ms: 2.257\n",
      "  timestamp: 1625111215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         1901.87</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">  34.533</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                12.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 920000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-47-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.799999999999905\n",
      "  episode_reward_mean: 35.966999999999906\n",
      "  episode_reward_min: 12.599999999999921\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4600\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.27358304569497705\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00751752796350047\n",
      "          policy_loss: -0.028997768939007074\n",
      "          total_loss: 39.3973907828331\n",
      "          vf_explained_var: 0.5322921872138977\n",
      "          vf_loss: 39.41877752542496\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.20420630555599928\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012449126545106992\n",
      "          policy_loss: -0.020320397030445747\n",
      "          total_loss: 4.787284053862095\n",
      "          vf_explained_var: 0.25425463914871216\n",
      "          vf_loss: 4.801302149891853\n",
      "    num_agent_steps_sampled: 920000\n",
      "    num_agent_steps_trained: 920000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.840000000000003\n",
      "    ram_util_percent: 61.44500000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 54.5\n",
      "    policy2: 20.799999999999933\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.945\n",
      "    policy2: 1.021999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: 12.5\n",
      "    policy2: -6.69999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2885815882101612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16056746085947626\n",
      "    mean_inference_ms: 3.194676808216643\n",
      "    mean_raw_obs_processing_ms: 1.1562669381797348\n",
      "  time_since_restore: 1915.6848890781403\n",
      "  time_this_iter_s: 13.810786962509155\n",
      "  time_total_s: 1915.6848890781403\n",
      "  timers:\n",
      "    learn_throughput: 298.286\n",
      "    learn_time_ms: 13409.969\n",
      "    sample_throughput: 6060.208\n",
      "    sample_time_ms: 660.043\n",
      "    update_time_ms: 2.352\n",
      "  timestamp: 1625111229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         1915.68</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">  35.967</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                12.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 928000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-47-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.799999999999905\n",
      "  episode_reward_mean: 35.1899999999999\n",
      "  episode_reward_min: 12.599999999999921\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4625\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2556845103390515\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0055202865623869\n",
      "          policy_loss: -0.017197587963892147\n",
      "          total_loss: 39.52508723735809\n",
      "          vf_explained_var: 0.6065208911895752\n",
      "          vf_loss: 39.53669512271881\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.19862098433077335\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01404601291869767\n",
      "          policy_loss: -0.028004851192235947\n",
      "          total_loss: 2.33333246037364\n",
      "          vf_explained_var: 0.24053114652633667\n",
      "          vf_loss: 2.354226540774107\n",
      "    num_agent_steps_sampled: 928000\n",
      "    num_agent_steps_trained: 928000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.055\n",
      "    ram_util_percent: 61.789999999999985\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 20.799999999999933\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.465\n",
      "    policy2: 0.724999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: 12.5\n",
      "    policy2: -6.69999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2888877447710474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1603477991769163\n",
      "    mean_inference_ms: 3.1939052772991254\n",
      "    mean_raw_obs_processing_ms: 1.155129836767541\n",
      "  time_since_restore: 1929.821295261383\n",
      "  time_this_iter_s: 14.136406183242798\n",
      "  time_total_s: 1929.821295261383\n",
      "  timers:\n",
      "    learn_throughput: 299.475\n",
      "    learn_time_ms: 13356.688\n",
      "    sample_throughput: 5973.408\n",
      "    sample_time_ms: 669.635\n",
      "    update_time_ms: 2.357\n",
      "  timestamp: 1625111243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         1929.82</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">   35.19</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                12.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 936000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-47-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.29999999999989\n",
      "  episode_reward_mean: 34.19699999999991\n",
      "  episode_reward_min: 4.5000000000000195\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4675\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.25412569660693407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007471303280908614\n",
      "          policy_loss: -0.025338502397062257\n",
      "          total_loss: 38.690449595451355\n",
      "          vf_explained_var: 0.6168878674507141\n",
      "          vf_loss: 38.708222687244415\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.20538444397971034\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009907236744766124\n",
      "          policy_loss: -0.013985345256514847\n",
      "          total_loss: 7.615666974335909\n",
      "          vf_explained_var: 0.25813043117523193\n",
      "          vf_loss: 7.624636933207512\n",
      "    num_agent_steps_sampled: 936000\n",
      "    num_agent_steps_trained: 936000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.359999999999996\n",
      "    ram_util_percent: 61.65\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 29.599999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.23\n",
      "    policy2: 0.9669999999999978\n",
      "  policy_reward_min:\n",
      "    policy1: 9.0\n",
      "    policy2: -6.69999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2884605058310213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16014914262041177\n",
      "    mean_inference_ms: 3.1891919648930696\n",
      "    mean_raw_obs_processing_ms: 1.1532200327453344\n",
      "  time_since_restore: 1943.7497169971466\n",
      "  time_this_iter_s: 13.92842173576355\n",
      "  time_total_s: 1943.7497169971466\n",
      "  timers:\n",
      "    learn_throughput: 300.245\n",
      "    learn_time_ms: 13322.46\n",
      "    sample_throughput: 6000.568\n",
      "    sample_time_ms: 666.604\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1625111257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         1943.75</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">  34.197</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                 4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 944000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-47-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.29999999999989\n",
      "  episode_reward_mean: 33.515999999999906\n",
      "  episode_reward_min: 4.5000000000000195\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4700\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.25230602640658617\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00612779168295674\n",
      "          policy_loss: -0.022795697150286287\n",
      "          total_loss: 45.82136404514313\n",
      "          vf_explained_var: 0.46159565448760986\n",
      "          vf_loss: 45.83795523643494\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21294608525931835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015765251038828865\n",
      "          policy_loss: -0.0345727225067094\n",
      "          total_loss: 2.832099936902523\n",
      "          vf_explained_var: 0.2579686939716339\n",
      "          vf_loss: 2.858691558241844\n",
      "    num_agent_steps_sampled: 944000\n",
      "    num_agent_steps_trained: 944000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.371428571428574\n",
      "    ram_util_percent: 61.766666666666666\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 29.599999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.56\n",
      "    policy2: 0.9559999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: 9.0\n",
      "    policy2: -6.6999999999999895\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28774209016341273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16018219090882824\n",
      "    mean_inference_ms: 3.18557628268156\n",
      "    mean_raw_obs_processing_ms: 1.1524156547868758\n",
      "  time_since_restore: 1958.14470911026\n",
      "  time_this_iter_s: 14.394992113113403\n",
      "  time_total_s: 1958.14470911026\n",
      "  timers:\n",
      "    learn_throughput: 299.018\n",
      "    learn_time_ms: 13377.129\n",
      "    sample_throughput: 5965.791\n",
      "    sample_time_ms: 670.489\n",
      "    update_time_ms: 2.403\n",
      "  timestamp: 1625111272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         1958.14</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">  33.516</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                 4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 952000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-48-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.3999999999999\n",
      "  episode_reward_mean: 33.491999999999905\n",
      "  episode_reward_min: 4.5000000000000195\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4750\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.25140486052259803\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008559684807551093\n",
      "          policy_loss: -0.02557358704507351\n",
      "          total_loss: 31.490682721138\n",
      "          vf_explained_var: 0.6811472773551941\n",
      "          vf_loss: 31.507589638233185\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.20604856638237834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012484652921557426\n",
      "          policy_loss: -0.02542462249402888\n",
      "          total_loss: 3.7711391896009445\n",
      "          vf_explained_var: 0.22165578603744507\n",
      "          vf_loss: 3.7902434915304184\n",
      "    num_agent_steps_sampled: 952000\n",
      "    num_agent_steps_trained: 952000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.154999999999994\n",
      "    ram_util_percent: 61.69500000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 29.599999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.635\n",
      "    policy2: 0.8569999999999993\n",
      "  policy_reward_min:\n",
      "    policy1: 9.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2876020139643025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15973179029107606\n",
      "    mean_inference_ms: 3.179399127286\n",
      "    mean_raw_obs_processing_ms: 1.1496345414919842\n",
      "  time_since_restore: 1972.4895102977753\n",
      "  time_this_iter_s: 14.344801187515259\n",
      "  time_total_s: 1972.4895102977753\n",
      "  timers:\n",
      "    learn_throughput: 298.268\n",
      "    learn_time_ms: 13410.759\n",
      "    sample_throughput: 6048.358\n",
      "    sample_time_ms: 661.336\n",
      "    update_time_ms: 2.409\n",
      "  timestamp: 1625111286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         1972.49</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">  33.492</td><td style=\"text-align: right;\">                47.4</td><td style=\"text-align: right;\">                 4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 960000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-48-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.4999999999999\n",
      "  episode_reward_mean: 33.04199999999991\n",
      "  episode_reward_min: 14.699999999999902\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4800\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.25868048472329974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00693410616077017\n",
      "          policy_loss: -0.02096278520184569\n",
      "          total_loss: 31.207833528518677\n",
      "          vf_explained_var: 0.6010348796844482\n",
      "          vf_loss: 31.221775889396667\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21104215364903212\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013411402323981747\n",
      "          policy_loss: -0.034285324887605384\n",
      "          total_loss: 2.5588218346238136\n",
      "          vf_explained_var: 0.2684345841407776\n",
      "          vf_loss: 2.586317628622055\n",
      "    num_agent_steps_sampled: 960000\n",
      "    num_agent_steps_trained: 960000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.18095238095239\n",
      "    ram_util_percent: 61.776190476190465\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.515\n",
      "    policy2: 0.5269999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: 12.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2866246778159389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15961452928604797\n",
      "    mean_inference_ms: 3.1726086095032406\n",
      "    mean_raw_obs_processing_ms: 1.1477217876546728\n",
      "  time_since_restore: 1987.2270233631134\n",
      "  time_this_iter_s: 14.737513065338135\n",
      "  time_total_s: 1987.2270233631134\n",
      "  timers:\n",
      "    learn_throughput: 295.894\n",
      "    learn_time_ms: 13518.361\n",
      "    sample_throughput: 6028.358\n",
      "    sample_time_ms: 663.531\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1625111301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         1987.23</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">  33.042</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 968000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-48-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.199999999999896\n",
      "  episode_reward_mean: 33.0149999999999\n",
      "  episode_reward_min: 14.699999999999902\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4825\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.22876544995233417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005539232937735505\n",
      "          policy_loss: -0.01443162449868396\n",
      "          total_loss: 38.01633733510971\n",
      "          vf_explained_var: 0.6308506727218628\n",
      "          vf_loss: 38.02516049146652\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.20152171049267054\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010833705091499723\n",
      "          policy_loss: -0.027301175956381485\n",
      "          total_loss: 2.623765956610441\n",
      "          vf_explained_var: 0.25299906730651855\n",
      "          vf_loss: 2.6455825977027416\n",
      "    num_agent_steps_sampled: 968000\n",
      "    num_agent_steps_trained: 968000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.9\n",
      "    ram_util_percent: 61.55\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.455\n",
      "    policy2: 0.56\n",
      "  policy_reward_min:\n",
      "    policy1: 12.0\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2868702321497517\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1593605817930534\n",
      "    mean_inference_ms: 3.170774732299634\n",
      "    mean_raw_obs_processing_ms: 1.1463494989063787\n",
      "  time_since_restore: 2001.348560333252\n",
      "  time_this_iter_s: 14.12153697013855\n",
      "  time_total_s: 2001.348560333252\n",
      "  timers:\n",
      "    learn_throughput: 295.701\n",
      "    learn_time_ms: 13527.181\n",
      "    sample_throughput: 5999.94\n",
      "    sample_time_ms: 666.673\n",
      "    update_time_ms: 2.428\n",
      "  timestamp: 1625111315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         2001.35</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">  33.015</td><td style=\"text-align: right;\">                46.2</td><td style=\"text-align: right;\">                14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 976000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-48-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.199999999999896\n",
      "  episode_reward_mean: 32.5259999999999\n",
      "  episode_reward_min: 6.299999999999931\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4875\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.24252421548590064\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006604652138776146\n",
      "          policy_loss: -0.01377787091769278\n",
      "          total_loss: 40.83487820625305\n",
      "          vf_explained_var: 0.6356974840164185\n",
      "          vf_loss: 40.84196877479553\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21301689697429538\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014656570696388371\n",
      "          policy_loss: -0.021693993447115645\n",
      "          total_loss: 3.2769954688847065\n",
      "          vf_explained_var: 0.23672717809677124\n",
      "          vf_loss: 3.2912696339190006\n",
      "    num_agent_steps_sampled: 976000\n",
      "    num_agent_steps_trained: 976000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.185000000000002\n",
      "    ram_util_percent: 61.514999999999986\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 9.800000000000022\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.01\n",
      "    policy2: 0.5159999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: 7.5\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2863516111020333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1590992624455145\n",
      "    mean_inference_ms: 3.164611066425948\n",
      "    mean_raw_obs_processing_ms: 1.1441151653573014\n",
      "  time_since_restore: 2015.2177004814148\n",
      "  time_this_iter_s: 13.869140148162842\n",
      "  time_total_s: 2015.2177004814148\n",
      "  timers:\n",
      "    learn_throughput: 296.081\n",
      "    learn_time_ms: 13509.817\n",
      "    sample_throughput: 6005.911\n",
      "    sample_time_ms: 666.011\n",
      "    update_time_ms: 2.398\n",
      "  timestamp: 1625111329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         2015.22</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">  32.526</td><td style=\"text-align: right;\">                46.2</td><td style=\"text-align: right;\">                 6.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 984000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-49-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.599999999999895\n",
      "  episode_reward_mean: 32.897999999999904\n",
      "  episode_reward_min: 6.299999999999931\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4900\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2594753201119602\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006647950780461542\n",
      "          policy_loss: -0.0191470542922616\n",
      "          total_loss: 40.532476365566254\n",
      "          vf_explained_var: 0.512898325920105\n",
      "          vf_loss: 40.54489290714264\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21346231969073415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011614294737228192\n",
      "          policy_loss: -0.025354723125929013\n",
      "          total_loss: 2.994142808020115\n",
      "          vf_explained_var: 0.296916127204895\n",
      "          vf_loss: 3.0136177763342857\n",
      "    num_agent_steps_sampled: 984000\n",
      "    num_agent_steps_trained: 984000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.335\n",
      "    ram_util_percent: 61.625000000000014\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 9.800000000000022\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.14\n",
      "    policy2: 0.7579999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: 7.5\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28557782254336467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15907641915908546\n",
      "    mean_inference_ms: 3.160016050696513\n",
      "    mean_raw_obs_processing_ms: 1.1431846839686797\n",
      "  time_since_restore: 2029.183245420456\n",
      "  time_this_iter_s: 13.965544939041138\n",
      "  time_total_s: 2029.183245420456\n",
      "  timers:\n",
      "    learn_throughput: 296.396\n",
      "    learn_time_ms: 13495.463\n",
      "    sample_throughput: 6053.683\n",
      "    sample_time_ms: 660.755\n",
      "    update_time_ms: 2.375\n",
      "  timestamp: 1625111343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         2029.18</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">  32.898</td><td style=\"text-align: right;\">                48.6</td><td style=\"text-align: right;\">                 6.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 992000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-49-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.699999999999896\n",
      "  episode_reward_mean: 32.4959999999999\n",
      "  episode_reward_min: 6.299999999999931\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4950\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.24385864986106753\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0054721335254726\n",
      "          policy_loss: -0.025443819089559838\n",
      "          total_loss: 33.25220447778702\n",
      "          vf_explained_var: 0.6340951919555664\n",
      "          vf_loss: 33.27210694551468\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21782876178622246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013321843085577711\n",
      "          policy_loss: -0.021885565656702965\n",
      "          total_loss: 2.4445151202380657\n",
      "          vf_explained_var: 0.27557265758514404\n",
      "          vf_loss: 2.459656462073326\n",
      "    num_agent_steps_sampled: 992000\n",
      "    num_agent_steps_trained: 992000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.369999999999997\n",
      "    ram_util_percent: 61.82000000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 9.800000000000022\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.485\n",
      "    policy2: 1.0109999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: 7.5\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28544743193702965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15863738986875764\n",
      "    mean_inference_ms: 3.1540415768653713\n",
      "    mean_raw_obs_processing_ms: 1.1405768339721192\n",
      "  time_since_restore: 2043.4470446109772\n",
      "  time_this_iter_s: 14.26379919052124\n",
      "  time_total_s: 2043.4470446109772\n",
      "  timers:\n",
      "    learn_throughput: 296.363\n",
      "    learn_time_ms: 13496.959\n",
      "    sample_throughput: 6137.051\n",
      "    sample_time_ms: 651.779\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1625111357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         2043.45</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">  32.496</td><td style=\"text-align: right;\">                50.7</td><td style=\"text-align: right;\">                 6.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1000000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-49-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.699999999999896\n",
      "  episode_reward_mean: 31.982999999999905\n",
      "  episode_reward_min: 12.899999999999917\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5000\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23850606102496386\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005489326700626407\n",
      "          policy_loss: -0.014752764604054391\n",
      "          total_loss: 59.97450911998749\n",
      "          vf_explained_var: 0.5382956266403198\n",
      "          vf_loss: 59.983705043792725\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23462466429919004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009967702339054085\n",
      "          policy_loss: -0.01751012750901282\n",
      "          total_loss: 23.685682028532028\n",
      "          vf_explained_var: 0.4226605296134949\n",
      "          vf_loss: 23.698146164417267\n",
      "    num_agent_steps_sampled: 1000000\n",
      "    num_agent_steps_trained: 1000000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.40952380952381\n",
      "    ram_util_percent: 62.11428571428572\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 60.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.925\n",
      "    policy2: 4.057999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -32.5\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28444382950081026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15848858837875526\n",
      "    mean_inference_ms: 3.14692010144278\n",
      "    mean_raw_obs_processing_ms: 1.1385914176784013\n",
      "  time_since_restore: 2058.0121915340424\n",
      "  time_this_iter_s: 14.565146923065186\n",
      "  time_total_s: 2058.0121915340424\n",
      "  timers:\n",
      "    learn_throughput: 294.583\n",
      "    learn_time_ms: 13578.534\n",
      "    sample_throughput: 6192.273\n",
      "    sample_time_ms: 645.966\n",
      "    update_time_ms: 2.232\n",
      "  timestamp: 1625111372\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         2058.01</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">  31.983</td><td style=\"text-align: right;\">                50.7</td><td style=\"text-align: right;\">                12.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1008000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-49-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.6999999999999\n",
      "  episode_reward_mean: 32.72099999999991\n",
      "  episode_reward_min: 13.799999999999926\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5025\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23300011595711112\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006959000849747099\n",
      "          policy_loss: -0.023597833671374246\n",
      "          total_loss: 60.041171073913574\n",
      "          vf_explained_var: 0.5113990306854248\n",
      "          vf_loss: 60.05772376060486\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2164139705710113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009235689023626037\n",
      "          policy_loss: -0.012979366787476465\n",
      "          total_loss: 22.195832431316376\n",
      "          vf_explained_var: 0.4040490686893463\n",
      "          vf_loss: 22.204136535525322\n",
      "    num_agent_steps_sampled: 1008000\n",
      "    num_agent_steps_trained: 1008000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.670833333333334\n",
      "    ram_util_percent: 62.07916666666667\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 60.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.3\n",
      "    policy2: 4.4209999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -32.5\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2846654663866623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15824154929139853\n",
      "    mean_inference_ms: 3.145232922766894\n",
      "    mean_raw_obs_processing_ms: 1.1372604130487118\n",
      "  time_since_restore: 2074.7619564533234\n",
      "  time_this_iter_s: 16.749764919281006\n",
      "  time_total_s: 2074.7619564533234\n",
      "  timers:\n",
      "    learn_throughput: 288.824\n",
      "    learn_time_ms: 13849.26\n",
      "    sample_throughput: 6283.576\n",
      "    sample_time_ms: 636.58\n",
      "    update_time_ms: 2.253\n",
      "  timestamp: 1625111389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         2074.76</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">  32.721</td><td style=\"text-align: right;\">                47.7</td><td style=\"text-align: right;\">                13.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1016000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-50-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.799999999999905\n",
      "  episode_reward_mean: 33.98699999999991\n",
      "  episode_reward_min: 14.699999999999925\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5075\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23740224773064256\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006237456123926677\n",
      "          policy_loss: -0.019811995938653126\n",
      "          total_loss: 36.69084280729294\n",
      "          vf_explained_var: 0.6539090871810913\n",
      "          vf_loss: 36.704338788986206\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.24205253645777702\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013194626895710826\n",
      "          policy_loss: -0.02038208072190173\n",
      "          total_loss: 6.346941851079464\n",
      "          vf_explained_var: 0.5687518119812012\n",
      "          vf_loss: 6.360644243657589\n",
      "    num_agent_steps_sampled: 1016000\n",
      "    num_agent_steps_trained: 1016000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.69523809523809\n",
      "    ram_util_percent: 61.57619047619047\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 67.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.62\n",
      "    policy2: 5.366999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -43.0\n",
      "    policy2: -6.6999999999999815\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28421631621973664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15801884393617907\n",
      "    mean_inference_ms: 3.140281087093996\n",
      "    mean_raw_obs_processing_ms: 1.1353487113175043\n",
      "  time_since_restore: 2089.213165283203\n",
      "  time_this_iter_s: 14.45120882987976\n",
      "  time_total_s: 2089.213165283203\n",
      "  timers:\n",
      "    learn_throughput: 287.853\n",
      "    learn_time_ms: 13896.005\n",
      "    sample_throughput: 6229.26\n",
      "    sample_time_ms: 642.131\n",
      "    update_time_ms: 2.189\n",
      "  timestamp: 1625111403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         2089.21</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">  33.987</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1024000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.799999999999905\n",
      "  episode_reward_mean: 33.91499999999991\n",
      "  episode_reward_min: 13.199999999999967\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5100\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2508497373200953\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007742087604128756\n",
      "          policy_loss: -0.023394023010041565\n",
      "          total_loss: 53.029316544532776\n",
      "          vf_explained_var: 0.5134106874465942\n",
      "          vf_loss: 53.04487133026123\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2369976961053908\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011818408063845709\n",
      "          policy_loss: -0.021642359672114253\n",
      "          total_loss: 9.320630669593811\n",
      "          vf_explained_var: 0.23061755299568176\n",
      "          vf_loss: 9.336290091276169\n",
      "    num_agent_steps_sampled: 1024000\n",
      "    num_agent_steps_trained: 1024000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.055\n",
      "    ram_util_percent: 61.595000000000006\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 67.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.33\n",
      "    policy2: 3.585\n",
      "  policy_reward_min:\n",
      "    policy1: -43.0\n",
      "    policy2: -6.69999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2835268912195194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1580251660974227\n",
      "    mean_inference_ms: 3.1366343536891765\n",
      "    mean_raw_obs_processing_ms: 1.1346062793260598\n",
      "  time_since_restore: 2103.5710961818695\n",
      "  time_this_iter_s: 14.357930898666382\n",
      "  time_total_s: 2103.5710961818695\n",
      "  timers:\n",
      "    learn_throughput: 287.876\n",
      "    learn_time_ms: 13894.893\n",
      "    sample_throughput: 6253.968\n",
      "    sample_time_ms: 639.594\n",
      "    update_time_ms: 2.149\n",
      "  timestamp: 1625111418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         2103.57</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\">  33.915</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                13.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1032000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-50-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.799999999999905\n",
      "  episode_reward_mean: 34.040999999999904\n",
      "  episode_reward_min: 13.199999999999967\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5150\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23639861727133393\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006641366278927308\n",
      "          policy_loss: -0.017231517034815624\n",
      "          total_loss: 36.11501634120941\n",
      "          vf_explained_var: 0.6323361396789551\n",
      "          vf_loss: 36.125523924827576\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2240554210729897\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012405158195178956\n",
      "          policy_loss: -0.01921893926919438\n",
      "          total_loss: 5.992793492972851\n",
      "          vf_explained_var: 0.22356614470481873\n",
      "          vf_loss: 6.005732245743275\n",
      "    num_agent_steps_sampled: 1032000\n",
      "    num_agent_steps_trained: 1032000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.925\n",
      "    ram_util_percent: 61.57500000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 60.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.875\n",
      "    policy2: 2.165999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -31.0\n",
      "    policy2: -6.69999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28349010784029405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15765162203033906\n",
      "    mean_inference_ms: 3.131892208701881\n",
      "    mean_raw_obs_processing_ms: 1.132361956812941\n",
      "  time_since_restore: 2117.839247941971\n",
      "  time_this_iter_s: 14.268151760101318\n",
      "  time_total_s: 2117.839247941971\n",
      "  timers:\n",
      "    learn_throughput: 288.099\n",
      "    learn_time_ms: 13884.11\n",
      "    sample_throughput: 6223.441\n",
      "    sample_time_ms: 642.731\n",
      "    update_time_ms: 2.146\n",
      "  timestamp: 1625111432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         2117.84</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">  34.041</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                13.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1040000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-50-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.8999999999999\n",
      "  episode_reward_mean: 34.964999999999904\n",
      "  episode_reward_min: 18.599999999999895\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5200\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2588189085945487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007066527308779769\n",
      "          policy_loss: -0.024600098404334858\n",
      "          total_loss: 40.8539314866066\n",
      "          vf_explained_var: 0.5260671377182007\n",
      "          vf_loss: 40.87137722969055\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.24390399688854814\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012581729009980336\n",
      "          policy_loss: -0.020916677138302475\n",
      "          total_loss: 7.574088558554649\n",
      "          vf_explained_var: 0.2970616817474365\n",
      "          vf_loss: 7.588635645806789\n",
      "    num_agent_steps_sampled: 1040000\n",
      "    num_agent_steps_trained: 1040000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.20952380952381\n",
      "    ram_util_percent: 61.43333333333333\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 21.899999999999995\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.36\n",
      "    policy2: 1.6049999999999995\n",
      "  policy_reward_min:\n",
      "    policy1: 11.5\n",
      "    policy2: -5.6\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2826798785619726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15758947383012228\n",
      "    mean_inference_ms: 3.126628079339726\n",
      "    mean_raw_obs_processing_ms: 1.1308725490749294\n",
      "  time_since_restore: 2132.1855919361115\n",
      "  time_this_iter_s: 14.346343994140625\n",
      "  time_total_s: 2132.1855919361115\n",
      "  timers:\n",
      "    learn_throughput: 288.967\n",
      "    learn_time_ms: 13842.41\n",
      "    sample_throughput: 6204.537\n",
      "    sample_time_ms: 644.689\n",
      "    update_time_ms: 2.155\n",
      "  timestamp: 1625111446\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         2132.19</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\">  34.965</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1048000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-51-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.49999999999991\n",
      "  episode_reward_mean: 34.6049999999999\n",
      "  episode_reward_min: 17.3999999999999\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5225\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2424436816945672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007093300097039901\n",
      "          policy_loss: -0.021399646007921547\n",
      "          total_loss: 34.758048474788666\n",
      "          vf_explained_var: 0.6197205781936646\n",
      "          vf_loss: 34.7722664475441\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22966709965839982\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013976454589283094\n",
      "          policy_loss: -0.021482886644662358\n",
      "          total_loss: 3.431813068687916\n",
      "          vf_explained_var: 0.22299538552761078\n",
      "          vf_loss: 3.446220353245735\n",
      "    num_agent_steps_sampled: 1048000\n",
      "    num_agent_steps_trained: 1048000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.835\n",
      "    ram_util_percent: 61.489999999999995\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 21.899999999999995\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.165\n",
      "    policy2: 1.44\n",
      "  policy_reward_min:\n",
      "    policy1: 11.5\n",
      "    policy2: -5.6\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28296422409455374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15738875045020573\n",
      "    mean_inference_ms: 3.125800323365304\n",
      "    mean_raw_obs_processing_ms: 1.1297256744472108\n",
      "  time_since_restore: 2146.1827161312103\n",
      "  time_this_iter_s: 13.997124195098877\n",
      "  time_total_s: 2146.1827161312103\n",
      "  timers:\n",
      "    learn_throughput: 289.284\n",
      "    learn_time_ms: 13827.223\n",
      "    sample_throughput: 6177.344\n",
      "    sample_time_ms: 647.528\n",
      "    update_time_ms: 2.126\n",
      "  timestamp: 1625111460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         2146.18</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">  34.605</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                17.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1056000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-51-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.7999999999999\n",
      "  episode_reward_mean: 34.0169999999999\n",
      "  episode_reward_min: 16.199999999999903\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5275\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23858831264078617\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007312173467653338\n",
      "          policy_loss: -0.033376879640854895\n",
      "          total_loss: 30.977666795253754\n",
      "          vf_explained_var: 0.6662899851799011\n",
      "          vf_loss: 31.003640353679657\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2199289994314313\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012587353005073965\n",
      "          policy_loss: -0.029174358816817403\n",
      "          total_loss: 2.5733872056007385\n",
      "          vf_explained_var: 0.25923144817352295\n",
      "          vf_loss: 2.596189174801111\n",
      "    num_agent_steps_sampled: 1056000\n",
      "    num_agent_steps_trained: 1056000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.77\n",
      "    ram_util_percent: 61.81500000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 21.899999999999995\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.885\n",
      "    policy2: 1.1319999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: 13.0\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28264055951841854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15723600121117903\n",
      "    mean_inference_ms: 3.1221136210320117\n",
      "    mean_raw_obs_processing_ms: 1.1281871822131344\n",
      "  time_since_restore: 2160.503667831421\n",
      "  time_this_iter_s: 14.320951700210571\n",
      "  time_total_s: 2160.503667831421\n",
      "  timers:\n",
      "    learn_throughput: 288.409\n",
      "    learn_time_ms: 13869.206\n",
      "    sample_throughput: 6146.22\n",
      "    sample_time_ms: 650.806\n",
      "    update_time_ms: 2.125\n",
      "  timestamp: 1625111475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">          2160.5</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\">  34.017</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                16.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1064000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-51-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.79999999999991\n",
      "  episode_reward_mean: 33.83399999999991\n",
      "  episode_reward_min: 15.899999999999912\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5300\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.24242649413645267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006287668409640901\n",
      "          policy_loss: -0.020267354528186843\n",
      "          total_loss: 48.0935400724411\n",
      "          vf_explained_var: 0.4969027638435364\n",
      "          vf_loss: 48.10744118690491\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2201336221769452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008089110182481818\n",
      "          policy_loss: -0.011094792775111273\n",
      "          total_loss: 9.029261536896229\n",
      "          vf_explained_var: 0.3261171281337738\n",
      "          vf_loss: 9.03626099973917\n",
      "    num_agent_steps_sampled: 1064000\n",
      "    num_agent_steps_trained: 1064000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.076190476190476\n",
      "    ram_util_percent: 61.88095238095237\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 38.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.24\n",
      "    policy2: 1.5939999999999994\n",
      "  policy_reward_min:\n",
      "    policy1: 0.0\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28201530181845164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15727674295639862\n",
      "    mean_inference_ms: 3.1189455066309573\n",
      "    mean_raw_obs_processing_ms: 1.1277097465673214\n",
      "  time_since_restore: 2174.8024728298187\n",
      "  time_this_iter_s: 14.298804998397827\n",
      "  time_total_s: 2174.8024728298187\n",
      "  timers:\n",
      "    learn_throughput: 287.779\n",
      "    learn_time_ms: 13899.559\n",
      "    sample_throughput: 6118.126\n",
      "    sample_time_ms: 653.795\n",
      "    update_time_ms: 2.112\n",
      "  timestamp: 1625111489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">          2174.8</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">  33.834</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                15.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1072000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-51-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.79999999999991\n",
      "  episode_reward_mean: 33.095999999999904\n",
      "  episode_reward_min: 11.69999999999991\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5350\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23096564365550876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008015323022846133\n",
      "          policy_loss: -0.023659745638724416\n",
      "          total_loss: 32.68730962276459\n",
      "          vf_explained_var: 0.669015645980835\n",
      "          vf_loss: 32.70285415649414\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2087197955697775\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013060950208455324\n",
      "          policy_loss: -0.020902757256408222\n",
      "          total_loss: 7.728189416229725\n",
      "          vf_explained_var: 0.32847559452056885\n",
      "          vf_loss: 7.742480158805847\n",
      "    num_agent_steps_sampled: 1072000\n",
      "    num_agent_steps_trained: 1072000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.385\n",
      "    ram_util_percent: 61.745000000000005\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 38.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.435\n",
      "    policy2: 2.6609999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -5.599999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2820652995316638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15694578429641887\n",
      "    mean_inference_ms: 3.115051490405435\n",
      "    mean_raw_obs_processing_ms: 1.1259909848245755\n",
      "  time_since_restore: 2189.2522337436676\n",
      "  time_this_iter_s: 14.449760913848877\n",
      "  time_total_s: 2189.2522337436676\n",
      "  timers:\n",
      "    learn_throughput: 287.435\n",
      "    learn_time_ms: 13916.209\n",
      "    sample_throughput: 6100.098\n",
      "    sample_time_ms: 655.727\n",
      "    update_time_ms: 2.107\n",
      "  timestamp: 1625111504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         2189.25</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\">  33.096</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1080000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-51-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.899999999999906\n",
      "  episode_reward_mean: 32.20199999999991\n",
      "  episode_reward_min: 11.69999999999991\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5400\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2427317025139928\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006937732730875723\n",
      "          policy_loss: -0.025318899424746633\n",
      "          total_loss: 36.5800661444664\n",
      "          vf_explained_var: 0.5336323976516724\n",
      "          vf_loss: 36.59836006164551\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22186544071882963\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016126735106809065\n",
      "          policy_loss: -0.02618869737489149\n",
      "          total_loss: 6.2841580510139465\n",
      "          vf_explained_var: 0.29851895570755005\n",
      "          vf_loss: 6.302182629704475\n",
      "    num_agent_steps_sampled: 1080000\n",
      "    num_agent_steps_trained: 1080000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.952380952380956\n",
      "    ram_util_percent: 61.81904761904763\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 30.699999999999953\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.925\n",
      "    policy2: 3.2769999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28130615486181204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1569102447692624\n",
      "    mean_inference_ms: 3.1101551287391773\n",
      "    mean_raw_obs_processing_ms: 1.124721312636579\n",
      "  time_since_restore: 2203.585349559784\n",
      "  time_this_iter_s: 14.333115816116333\n",
      "  time_total_s: 2203.585349559784\n",
      "  timers:\n",
      "    learn_throughput: 288.028\n",
      "    learn_time_ms: 13887.554\n",
      "    sample_throughput: 6050.053\n",
      "    sample_time_ms: 661.151\n",
      "    update_time_ms: 2.142\n",
      "  timestamp: 1625111518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         2203.59</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">  32.202</td><td style=\"text-align: right;\">                45.9</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1088000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-52-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.89999999999991\n",
      "  episode_reward_mean: 32.40899999999991\n",
      "  episode_reward_min: 10.199999999999914\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5425\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23279994213953614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007348718514549546\n",
      "          policy_loss: -0.017887908674310893\n",
      "          total_loss: 50.645334005355835\n",
      "          vf_explained_var: 0.5299735069274902\n",
      "          vf_loss: 50.655781984329224\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21931982412934303\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012110599913285114\n",
      "          policy_loss: -0.023944924338138662\n",
      "          total_loss: 26.25279849767685\n",
      "          vf_explained_var: 0.2455686628818512\n",
      "          vf_loss: 26.270612716674805\n",
      "    num_agent_steps_sampled: 1088000\n",
      "    num_agent_steps_trained: 1088000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.910000000000004\n",
      "    ram_util_percent: 61.779999999999994\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.0\n",
      "    policy2: 40.59999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.185\n",
      "    policy2: 5.224000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2815969180373207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15670666372856817\n",
      "    mean_inference_ms: 3.109288772581098\n",
      "    mean_raw_obs_processing_ms: 1.1237500153899074\n",
      "  time_since_restore: 2217.8616585731506\n",
      "  time_this_iter_s: 14.2763090133667\n",
      "  time_total_s: 2217.8616585731506\n",
      "  timers:\n",
      "    learn_throughput: 293.288\n",
      "    learn_time_ms: 13638.476\n",
      "    sample_throughput: 6033.725\n",
      "    sample_time_ms: 662.94\n",
      "    update_time_ms: 2.129\n",
      "  timestamp: 1625111532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         2217.86</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\">  32.409</td><td style=\"text-align: right;\">                45.9</td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1096000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-52-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.7999999999999\n",
      "  episode_reward_mean: 32.954999999999906\n",
      "  episode_reward_min: 10.199999999999914\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5475\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2429116521961987\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008331216595252044\n",
      "          policy_loss: -0.022531356691615656\n",
      "          total_loss: 61.87284767627716\n",
      "          vf_explained_var: 0.4856888949871063\n",
      "          vf_loss: 61.886942744255066\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22748860716819763\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013159259397070855\n",
      "          policy_loss: -0.024488533177645877\n",
      "          total_loss: 41.19653457403183\n",
      "          vf_explained_var: 0.27434277534484863\n",
      "          vf_loss: 41.2143611907959\n",
      "    num_agent_steps_sampled: 1096000\n",
      "    num_agent_steps_trained: 1096000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.290909090909086\n",
      "    ram_util_percent: 61.772727272727295\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 54.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.465\n",
      "    policy2: 7.49\n",
      "  policy_reward_min:\n",
      "    policy1: -30.0\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2812377426454658\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15652213163384127\n",
      "    mean_inference_ms: 3.105058220952158\n",
      "    mean_raw_obs_processing_ms: 1.122156822617839\n",
      "  time_since_restore: 2233.08606672287\n",
      "  time_this_iter_s: 15.224408149719238\n",
      "  time_total_s: 2233.08606672287\n",
      "  timers:\n",
      "    learn_throughput: 291.556\n",
      "    learn_time_ms: 13719.504\n",
      "    sample_throughput: 6069.688\n",
      "    sample_time_ms: 659.012\n",
      "    update_time_ms: 2.233\n",
      "  timestamp: 1625111548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         2233.09</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">  32.955</td><td style=\"text-align: right;\">                46.8</td><td style=\"text-align: right;\">                10.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-52-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.7999999999999\n",
      "  episode_reward_mean: 32.74499999999991\n",
      "  episode_reward_min: 7.7999999999999154\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5500\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2534395381808281\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006808859929151367\n",
      "          policy_loss: -0.01966734675806947\n",
      "          total_loss: 54.55338394641876\n",
      "          vf_explained_var: 0.44026944041252136\n",
      "          vf_loss: 54.566158056259155\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21999074425548315\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007579030658234842\n",
      "          policy_loss: -0.013432583160465583\n",
      "          total_loss: 22.38509976863861\n",
      "          vf_explained_var: 0.2736104726791382\n",
      "          vf_loss: 22.394695341587067\n",
      "    num_agent_steps_sampled: 1104000\n",
      "    num_agent_steps_trained: 1104000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.526923076923076\n",
      "    ram_util_percent: 61.85769230769232\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 54.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.365\n",
      "    policy2: 7.38\n",
      "  policy_reward_min:\n",
      "    policy1: -30.0\n",
      "    policy2: -6.699999999999993\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2806985330084841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15659907892839933\n",
      "    mean_inference_ms: 3.102856332897137\n",
      "    mean_raw_obs_processing_ms: 1.1220134990132176\n",
      "  time_since_restore: 2251.6507906913757\n",
      "  time_this_iter_s: 18.56472396850586\n",
      "  time_total_s: 2251.6507906913757\n",
      "  timers:\n",
      "    learn_throughput: 283.242\n",
      "    learn_time_ms: 14122.198\n",
      "    sample_throughput: 5912.41\n",
      "    sample_time_ms: 676.543\n",
      "    update_time_ms: 2.303\n",
      "  timestamp: 1625111566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         2251.65</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">  32.745</td><td style=\"text-align: right;\">                46.8</td><td style=\"text-align: right;\">                 7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-53-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.59999999999992\n",
      "  episode_reward_mean: 33.77699999999991\n",
      "  episode_reward_min: 7.7999999999999154\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5550\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.253717967774719\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007236987832584418\n",
      "          policy_loss: -0.0330022034177091\n",
      "          total_loss: 30.307249307632446\n",
      "          vf_explained_var: 0.6179074048995972\n",
      "          vf_loss: 30.332924485206604\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21914513828232884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01101447522523813\n",
      "          policy_loss: -0.02446455304743722\n",
      "          total_loss: 16.21370306611061\n",
      "          vf_explained_var: 0.2255425751209259\n",
      "          vf_loss: 16.232591658830643\n",
      "    num_agent_steps_sampled: 1112000\n",
      "    num_agent_steps_trained: 1112000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.469565217391303\n",
      "    ram_util_percent: 61.88260869565217\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 47.19999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.255\n",
      "    policy2: 6.521999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: -14.5\n",
      "    policy2: -6.699999999999993\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.281392009668674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1566195606752008\n",
      "    mean_inference_ms: 3.106005994801092\n",
      "    mean_raw_obs_processing_ms: 1.122708960425003\n",
      "  time_since_restore: 2267.859538793564\n",
      "  time_this_iter_s: 16.20874810218811\n",
      "  time_total_s: 2267.859538793564\n",
      "  timers:\n",
      "    learn_throughput: 280.103\n",
      "    learn_time_ms: 14280.48\n",
      "    sample_throughput: 5616.53\n",
      "    sample_time_ms: 712.184\n",
      "    update_time_ms: 2.342\n",
      "  timestamp: 1625111582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         2267.86</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\">  33.777</td><td style=\"text-align: right;\">                45.6</td><td style=\"text-align: right;\">                 7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-53-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.599999999999916\n",
      "  episode_reward_mean: 34.18199999999991\n",
      "  episode_reward_min: 15.299999999999905\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5600\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2609911886975169\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0067008822079515085\n",
      "          policy_loss: -0.02537728524475824\n",
      "          total_loss: 38.01330989599228\n",
      "          vf_explained_var: 0.563621461391449\n",
      "          vf_loss: 38.031902849674225\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22851490275934339\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01253008317144122\n",
      "          policy_loss: -0.020784331893082708\n",
      "          total_loss: 12.941604197025299\n",
      "          vf_explained_var: 0.21994535624980927\n",
      "          vf_loss: 12.95604507625103\n",
      "    num_agent_steps_sampled: 1120000\n",
      "    num_agent_steps_trained: 1120000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.67916666666667\n",
      "    ram_util_percent: 63.958333333333336\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 44.5\n",
      "    policy2: 36.199999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.255\n",
      "    policy2: 4.926999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28132761028020214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1569460065102753\n",
      "    mean_inference_ms: 3.108638664356279\n",
      "    mean_raw_obs_processing_ms: 1.1241310104263678\n",
      "  time_since_restore: 2284.4404575824738\n",
      "  time_this_iter_s: 16.580918788909912\n",
      "  time_total_s: 2284.4404575824738\n",
      "  timers:\n",
      "    learn_throughput: 275.977\n",
      "    learn_time_ms: 14493.95\n",
      "    sample_throughput: 5549.537\n",
      "    sample_time_ms: 720.781\n",
      "    update_time_ms: 2.442\n",
      "  timestamp: 1625111599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         2284.44</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">  34.182</td><td style=\"text-align: right;\">                48.6</td><td style=\"text-align: right;\">                15.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-53-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.599999999999916\n",
      "  episode_reward_mean: 33.42599999999991\n",
      "  episode_reward_min: 15.299999999999905\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5625\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23163103172555566\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0047566401335643604\n",
      "          policy_loss: -0.011789395764935762\n",
      "          total_loss: 48.40736836194992\n",
      "          vf_explained_var: 0.5938019752502441\n",
      "          vf_loss: 48.41434186697006\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2225200468674302\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011634271184448153\n",
      "          policy_loss: -0.013730258622672409\n",
      "          total_loss: 19.967250764369965\n",
      "          vf_explained_var: 0.291927695274353\n",
      "          vf_loss: 19.9750913977623\n",
      "    num_agent_steps_sampled: 1128000\n",
      "    num_agent_steps_trained: 1128000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.936363636363637\n",
      "    ram_util_percent: 61.104545454545466\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 44.0\n",
      "    policy2: 53.8\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.06\n",
      "    policy2: 4.365999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -31.0\n",
      "    policy2: -5.6\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2816633969347108\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15677267004823484\n",
      "    mean_inference_ms: 3.1086940278260737\n",
      "    mean_raw_obs_processing_ms: 1.123362937314347\n",
      "  time_since_restore: 2299.5101306438446\n",
      "  time_this_iter_s: 15.06967306137085\n",
      "  time_total_s: 2299.5101306438446\n",
      "  timers:\n",
      "    learn_throughput: 273.982\n",
      "    learn_time_ms: 14599.478\n",
      "    sample_throughput: 5536.461\n",
      "    sample_time_ms: 722.483\n",
      "    update_time_ms: 2.429\n",
      "  timestamp: 1625111614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         2299.51</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">  33.426</td><td style=\"text-align: right;\">                48.6</td><td style=\"text-align: right;\">                15.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-53-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.999999999999915\n",
      "  episode_reward_mean: 33.16799999999991\n",
      "  episode_reward_min: 15.299999999999905\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5675\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2249625655822456\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011607827400439419\n",
      "          policy_loss: -0.018548636115156114\n",
      "          total_loss: 33.35839283466339\n",
      "          vf_explained_var: 0.6509989500045776\n",
      "          vf_loss: 33.37106537818909\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23841854324564338\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013866244233213365\n",
      "          policy_loss: -0.02726339199580252\n",
      "          total_loss: 6.756814152002335\n",
      "          vf_explained_var: 0.471514493227005\n",
      "          vf_loss: 6.777057617902756\n",
      "    num_agent_steps_sampled: 1136000\n",
      "    num_agent_steps_trained: 1136000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.705\n",
      "    ram_util_percent: 61.85999999999999\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 58.2\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.22\n",
      "    policy2: 3.9480000000000013\n",
      "  policy_reward_min:\n",
      "    policy1: -31.5\n",
      "    policy2: -6.699999999999992\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2813704445001348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15661844451968526\n",
      "    mean_inference_ms: 3.1052674060425716\n",
      "    mean_raw_obs_processing_ms: 1.12210899908433\n",
      "  time_since_restore: 2313.962999343872\n",
      "  time_this_iter_s: 14.452868700027466\n",
      "  time_total_s: 2313.962999343872\n",
      "  timers:\n",
      "    learn_throughput: 273.639\n",
      "    learn_time_ms: 14617.821\n",
      "    sample_throughput: 5576.27\n",
      "    sample_time_ms: 717.325\n",
      "    update_time_ms: 2.438\n",
      "  timestamp: 1625111629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         2313.96</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\">  33.168</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                15.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-54-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.799999999999926\n",
      "  episode_reward_mean: 32.8109999999999\n",
      "  episode_reward_min: 13.199999999999982\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5700\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.25183290196582675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014860992698231712\n",
      "          policy_loss: -0.03232403949368745\n",
      "          total_loss: 50.6695591211319\n",
      "          vf_explained_var: 0.5301828384399414\n",
      "          vf_loss: 50.69436037540436\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2573817679658532\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016561844604439102\n",
      "          policy_loss: -0.028423756099073216\n",
      "          total_loss: 14.075084269046783\n",
      "          vf_explained_var: 0.3027312159538269\n",
      "          vf_loss: 14.095123678445816\n",
      "    num_agent_steps_sampled: 1144000\n",
      "    num_agent_steps_trained: 1144000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.242857142857144\n",
      "    ram_util_percent: 62.185714285714276\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 58.2\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.555\n",
      "    policy2: 4.256000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -31.5\n",
      "    policy2: -6.699999999999992\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28078831381150665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1566505335013351\n",
      "    mean_inference_ms: 3.102088204957876\n",
      "    mean_raw_obs_processing_ms: 1.1216416315076825\n",
      "  time_since_restore: 2328.818279504776\n",
      "  time_this_iter_s: 14.85528016090393\n",
      "  time_total_s: 2328.818279504776\n",
      "  timers:\n",
      "    learn_throughput: 272.638\n",
      "    learn_time_ms: 14671.481\n",
      "    sample_throughput: 5564.298\n",
      "    sample_time_ms: 718.869\n",
      "    update_time_ms: 2.524\n",
      "  timestamp: 1625111644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         2328.82</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">  32.811</td><td style=\"text-align: right;\">                46.8</td><td style=\"text-align: right;\">                13.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-54-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.0999999999999\n",
      "  episode_reward_mean: 33.22199999999991\n",
      "  episode_reward_min: 10.499999999999979\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5750\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23627133574336767\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012405671994201839\n",
      "          policy_loss: -0.023141042023780756\n",
      "          total_loss: 44.154469192028046\n",
      "          vf_explained_var: 0.655792236328125\n",
      "          vf_loss: 44.17132955789566\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.25320749217644334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011793323996243998\n",
      "          policy_loss: -0.02021022589178756\n",
      "          total_loss: 21.812495976686478\n",
      "          vf_explained_var: 0.29205575585365295\n",
      "          vf_loss: 21.826735764741898\n",
      "    num_agent_steps_sampled: 1152000\n",
      "    num_agent_steps_trained: 1152000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.86666666666667\n",
      "    ram_util_percent: 60.975\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 47.0\n",
      "    policy2: 39.49999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.33\n",
      "    policy2: 2.8920000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -18.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28111708906224026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1565353518870844\n",
      "    mean_inference_ms: 3.103439528409719\n",
      "    mean_raw_obs_processing_ms: 1.1212706353698925\n",
      "  time_since_restore: 2345.007853746414\n",
      "  time_this_iter_s: 16.189574241638184\n",
      "  time_total_s: 2345.007853746414\n",
      "  timers:\n",
      "    learn_throughput: 270.033\n",
      "    learn_time_ms: 14813.006\n",
      "    sample_throughput: 5323.505\n",
      "    sample_time_ms: 751.385\n",
      "    update_time_ms: 2.5\n",
      "  timestamp: 1625111660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         2345.01</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\">  33.222</td><td style=\"text-align: right;\">                47.1</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-54-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.4999999999999\n",
      "  episode_reward_mean: 32.59799999999991\n",
      "  episode_reward_min: 10.499999999999979\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5800\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2344177351333201\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010900532550294884\n",
      "          policy_loss: -0.026935252884868532\n",
      "          total_loss: 42.40937739610672\n",
      "          vf_explained_var: 0.5766823291778564\n",
      "          vf_loss: 42.430795431137085\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.25368140218779445\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012535772752016783\n",
      "          policy_loss: -0.01669362498796545\n",
      "          total_loss: 12.05580498278141\n",
      "          vf_explained_var: 0.41273370385169983\n",
      "          vf_loss: 12.066152036190033\n",
      "    num_agent_steps_sampled: 1160000\n",
      "    num_agent_steps_trained: 1160000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.861904761904764\n",
      "    ram_util_percent: 61.004761904761914\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 64.8\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.795\n",
      "    policy2: 1.802999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -40.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2807503436674591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15674424767913034\n",
      "    mean_inference_ms: 3.104048558634858\n",
      "    mean_raw_obs_processing_ms: 1.121779520774122\n",
      "  time_since_restore: 2360.071124792099\n",
      "  time_this_iter_s: 15.063271045684814\n",
      "  time_total_s: 2360.071124792099\n",
      "  timers:\n",
      "    learn_throughput: 268.759\n",
      "    learn_time_ms: 14883.227\n",
      "    sample_throughput: 5303.894\n",
      "    sample_time_ms: 754.163\n",
      "    update_time_ms: 2.481\n",
      "  timestamp: 1625111675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         2360.07</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\">  32.598</td><td style=\"text-align: right;\">                49.5</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-54-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.4999999999999\n",
      "  episode_reward_mean: 32.927999999999905\n",
      "  episode_reward_min: 10.499999999999979\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5825\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2229600022546947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010285116790328175\n",
      "          policy_loss: -0.028172020858619362\n",
      "          total_loss: 54.33310651779175\n",
      "          vf_explained_var: 0.5925516486167908\n",
      "          vf_loss: 54.35607159137726\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.24411411629989743\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007682917203055695\n",
      "          policy_loss: -0.012734066811390221\n",
      "          total_loss: 14.965453244745731\n",
      "          vf_explained_var: 0.33603453636169434\n",
      "          vf_loss: 14.974297910928726\n",
      "    num_agent_steps_sampled: 1168000\n",
      "    num_agent_steps_trained: 1168000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.9\n",
      "    ram_util_percent: 62.36666666666667\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 64.8\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.96\n",
      "    policy2: 1.9679999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -40.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28126718301770615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15669939265437566\n",
      "    mean_inference_ms: 3.1064352620317424\n",
      "    mean_raw_obs_processing_ms: 1.1220006998911478\n",
      "  time_since_restore: 2376.7435019016266\n",
      "  time_this_iter_s: 16.672377109527588\n",
      "  time_total_s: 2376.7435019016266\n",
      "  timers:\n",
      "    learn_throughput: 265.187\n",
      "    learn_time_ms: 15083.696\n",
      "    sample_throughput: 5042.97\n",
      "    sample_time_ms: 793.183\n",
      "    update_time_ms: 2.484\n",
      "  timestamp: 1625111692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         2376.74</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\">  32.928</td><td style=\"text-align: right;\">                49.5</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-55-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.0999999999999\n",
      "  episode_reward_mean: 33.215999999999916\n",
      "  episode_reward_min: 11.699999999999935\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5875\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2117365263402462\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00916745059657842\n",
      "          policy_loss: -0.018735508376266807\n",
      "          total_loss: 54.27224266529083\n",
      "          vf_explained_var: 0.6216288805007935\n",
      "          vf_loss: 54.286336183547974\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2465882645919919\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009577970893587917\n",
      "          policy_loss: -0.01942857773974538\n",
      "          total_loss: 15.499835699796677\n",
      "          vf_explained_var: 0.4702819585800171\n",
      "          vf_loss: 15.514415487647057\n",
      "    num_agent_steps_sampled: 1176000\n",
      "    num_agent_steps_trained: 1176000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.117391304347827\n",
      "    ram_util_percent: 62.60000000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 58.199999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.16\n",
      "    policy2: 2.055999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -34.5\n",
      "    policy2: -6.699999999999988\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28143522463441456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15680988872088047\n",
      "    mean_inference_ms: 3.1082275062037805\n",
      "    mean_raw_obs_processing_ms: 1.1228914826716028\n",
      "  time_since_restore: 2392.7259838581085\n",
      "  time_this_iter_s: 15.982481956481934\n",
      "  time_total_s: 2392.7259838581085\n",
      "  timers:\n",
      "    learn_throughput: 263.887\n",
      "    learn_time_ms: 15157.995\n",
      "    sample_throughput: 5031.408\n",
      "    sample_time_ms: 795.006\n",
      "    update_time_ms: 2.398\n",
      "  timestamp: 1625111708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 147\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         2392.73</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\">  33.216</td><td style=\"text-align: right;\">                50.1</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-55-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.0999999999999\n",
      "  episode_reward_mean: 33.149999999999906\n",
      "  episode_reward_min: 11.699999999999935\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5900\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2273365636356175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009956065405276604\n",
      "          policy_loss: -0.029662500164704397\n",
      "          total_loss: 42.508177518844604\n",
      "          vf_explained_var: 0.5807116031646729\n",
      "          vf_loss: 42.53279936313629\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23267264757305384\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014062986359931529\n",
      "          policy_loss: -0.02822425884369295\n",
      "          total_loss: 6.545657753944397\n",
      "          vf_explained_var: 0.3669847548007965\n",
      "          vf_loss: 6.566762633621693\n",
      "    num_agent_steps_sampled: 1184000\n",
      "    num_agent_steps_trained: 1184000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.622727272727275\n",
      "    ram_util_percent: 63.17272727272726\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 58.199999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.665\n",
      "    policy2: 2.484999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -34.5\n",
      "    policy2: -6.699999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2811417256741317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15698532816987806\n",
      "    mean_inference_ms: 3.108153156795952\n",
      "    mean_raw_obs_processing_ms: 1.1236653942074775\n",
      "  time_since_restore: 2408.2286398410797\n",
      "  time_this_iter_s: 15.502655982971191\n",
      "  time_total_s: 2408.2286398410797\n",
      "  timers:\n",
      "    learn_throughput: 269.201\n",
      "    learn_time_ms: 14858.78\n",
      "    sample_throughput: 5073.469\n",
      "    sample_time_ms: 788.415\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1625111723\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         2408.23</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\">   33.15</td><td style=\"text-align: right;\">                50.1</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-55-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.0999999999999\n",
      "  episode_reward_mean: 33.491999999999905\n",
      "  episode_reward_min: 11.699999999999935\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5950\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2384969755075872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011584791980567388\n",
      "          policy_loss: -0.018274011759785935\n",
      "          total_loss: 32.01387083530426\n",
      "          vf_explained_var: 0.6652053594589233\n",
      "          vf_loss: 32.026279866695404\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2481301622465253\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01089438862982206\n",
      "          policy_loss: -0.017790038720704615\n",
      "          total_loss: 5.703280448913574\n",
      "          vf_explained_var: 0.22170192003250122\n",
      "          vf_loss: 5.715555228292942\n",
      "    num_agent_steps_sampled: 1192000\n",
      "    num_agent_steps_trained: 1192000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.856521739130436\n",
      "    ram_util_percent: 62.521739130434774\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 54.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.865\n",
      "    policy2: 1.6269999999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -27.0\n",
      "    policy2: -5.59999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2814120615576893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15679571746491258\n",
      "    mean_inference_ms: 3.1072459860969825\n",
      "    mean_raw_obs_processing_ms: 1.1229251569961298\n",
      "  time_since_restore: 2424.710858821869\n",
      "  time_this_iter_s: 16.482218980789185\n",
      "  time_total_s: 2424.710858821869\n",
      "  timers:\n",
      "    learn_throughput: 268.229\n",
      "    learn_time_ms: 14912.611\n",
      "    sample_throughput: 5251.785\n",
      "    sample_time_ms: 761.646\n",
      "    update_time_ms: 2.324\n",
      "  timestamp: 1625111740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 149\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         2424.71</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\">  33.492</td><td style=\"text-align: right;\">                50.1</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-55-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.39999999999989\n",
      "  episode_reward_mean: 32.68199999999991\n",
      "  episode_reward_min: 7.49999999999992\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6000\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23385249124839902\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0130698015273083\n",
      "          policy_loss: -0.026355884736403823\n",
      "          total_loss: 45.99314433336258\n",
      "          vf_explained_var: 0.5023248791694641\n",
      "          vf_loss: 46.01288318634033\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.24376056669279933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0173137774982024\n",
      "          policy_loss: -0.021472067455761135\n",
      "          total_loss: 2.9445047453045845\n",
      "          vf_explained_var: 0.1935119330883026\n",
      "          vf_loss: 2.9572116807103157\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_agent_steps_trained: 1200000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.7074074074074\n",
      "    ram_util_percent: 61.84074074074074\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 20.799999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.65\n",
      "    policy2: 0.031999999999998724\n",
      "  policy_reward_min:\n",
      "    policy1: 12.0\n",
      "    policy2: -6.69999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2810815296590627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15696352832134455\n",
      "    mean_inference_ms: 3.107011204831619\n",
      "    mean_raw_obs_processing_ms: 1.1232644098950504\n",
      "  time_since_restore: 2443.5726318359375\n",
      "  time_this_iter_s: 18.861773014068604\n",
      "  time_total_s: 2443.5726318359375\n",
      "  timers:\n",
      "    learn_throughput: 264.303\n",
      "    learn_time_ms: 15134.141\n",
      "    sample_throughput: 5195.103\n",
      "    sample_time_ms: 769.956\n",
      "    update_time_ms: 2.292\n",
      "  timestamp: 1625111759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         2443.57</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\">  32.682</td><td style=\"text-align: right;\">                47.4</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-56-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.999999999999915\n",
      "  episode_reward_mean: 32.216999999999906\n",
      "  episode_reward_min: 7.49999999999992\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6025\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.24564681900665164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012811394815798849\n",
      "          policy_loss: -0.024723084061406553\n",
      "          total_loss: 49.326666593551636\n",
      "          vf_explained_var: 0.5660548806190491\n",
      "          vf_loss: 49.34490394592285\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.24996635178104043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01129171637876425\n",
      "          policy_loss: -0.015428548213094473\n",
      "          total_loss: 3.9218547716736794\n",
      "          vf_explained_var: 0.2444627285003662\n",
      "          vf_loss: 3.9315669015049934\n",
      "    num_agent_steps_sampled: 1208000\n",
      "    num_agent_steps_trained: 1208000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.854166666666664\n",
      "    ram_util_percent: 62.34583333333333\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 20.799999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.405\n",
      "    policy2: -0.18800000000000147\n",
      "  policy_reward_min:\n",
      "    policy1: 12.0\n",
      "    policy2: -6.69999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28162931766550225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15693421399566693\n",
      "    mean_inference_ms: 3.1098932429301467\n",
      "    mean_raw_obs_processing_ms: 1.1238835875367168\n",
      "  time_since_restore: 2460.1017985343933\n",
      "  time_this_iter_s: 16.52916669845581\n",
      "  time_total_s: 2460.1017985343933\n",
      "  timers:\n",
      "    learn_throughput: 262.369\n",
      "    learn_time_ms: 15245.696\n",
      "    sample_throughput: 4972.41\n",
      "    sample_time_ms: 804.439\n",
      "    update_time_ms: 2.31\n",
      "  timestamp: 1625111775\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">          2460.1</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\">  32.217</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-56-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.8999999999999\n",
      "  episode_reward_mean: 31.802999999999905\n",
      "  episode_reward_min: 13.199999999999948\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6075\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2559543545357883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018963325303047895\n",
      "          policy_loss: -0.02703658299287781\n",
      "          total_loss: 37.216406881809235\n",
      "          vf_explained_var: 0.6427992582321167\n",
      "          vf_loss: 37.23384338617325\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23734152596443892\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011234763500397094\n",
      "          policy_loss: -0.026144160598050803\n",
      "          total_loss: 2.8982425667345524\n",
      "          vf_explained_var: 0.3932511806488037\n",
      "          vf_loss: 2.918699212372303\n",
      "    num_agent_steps_sampled: 1216000\n",
      "    num_agent_steps_trained: 1216000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.0\n",
      "    ram_util_percent: 61.059090909090905\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 43.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.815\n",
      "    policy2: -0.012000000000001716\n",
      "  policy_reward_min:\n",
      "    policy1: -20.5\n",
      "    policy2: -5.599999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2819451775522163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1571007127077921\n",
      "    mean_inference_ms: 3.113643049876172\n",
      "    mean_raw_obs_processing_ms: 1.1255151831081003\n",
      "  time_since_restore: 2475.7635464668274\n",
      "  time_this_iter_s: 15.661747932434082\n",
      "  time_total_s: 2475.7635464668274\n",
      "  timers:\n",
      "    learn_throughput: 260.552\n",
      "    learn_time_ms: 15352.008\n",
      "    sample_throughput: 4883.928\n",
      "    sample_time_ms: 819.013\n",
      "    update_time_ms: 2.302\n",
      "  timestamp: 1625111791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         2475.76</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\">  31.803</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                13.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-56-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.8999999999999\n",
      "  episode_reward_mean: 30.875999999999912\n",
      "  episode_reward_min: 11.999999999999922\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6100\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.25559239834547043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01256851345533505\n",
      "          policy_loss: -0.022375546977855265\n",
      "          total_loss: 38.282747983932495\n",
      "          vf_explained_var: 0.5036213397979736\n",
      "          vf_loss: 38.29876047372818\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21148952934890985\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012281894407351501\n",
      "          policy_loss: -0.02227119283634238\n",
      "          total_loss: 2.5083980709314346\n",
      "          vf_explained_var: 0.3005514144897461\n",
      "          vf_loss: 2.524451568722725\n",
      "    num_agent_steps_sampled: 1224000\n",
      "    num_agent_steps_trained: 1224000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.80869565217391\n",
      "    ram_util_percent: 61.00434782608695\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 43.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.36\n",
      "    policy2: 0.5159999999999978\n",
      "  policy_reward_min:\n",
      "    policy1: -20.5\n",
      "    policy2: -5.599999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2816515779356204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15728172471237986\n",
      "    mean_inference_ms: 3.1137268528805793\n",
      "    mean_raw_obs_processing_ms: 1.1263498225929482\n",
      "  time_since_restore: 2491.596653699875\n",
      "  time_this_iter_s: 15.833107233047485\n",
      "  time_total_s: 2491.596653699875\n",
      "  timers:\n",
      "    learn_throughput: 258.934\n",
      "    learn_time_ms: 15447.931\n",
      "    sample_throughput: 4872.359\n",
      "    sample_time_ms: 820.958\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1625111807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 153\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">          2491.6</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\">  30.876</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-57-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.8999999999999\n",
      "  episode_reward_mean: 30.836999999999918\n",
      "  episode_reward_min: 5.999999999999945\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6150\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2453766157850623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010266824887366965\n",
      "          policy_loss: -0.02087603820837103\n",
      "          total_loss: 39.54801380634308\n",
      "          vf_explained_var: 0.5995327830314636\n",
      "          vf_loss: 39.56369352340698\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23409859696403146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012929845790495165\n",
      "          policy_loss: -0.02934203197946772\n",
      "          total_loss: 1.9157714769244194\n",
      "          vf_explained_var: 0.30945885181427\n",
      "          vf_loss: 1.9385678172111511\n",
      "    num_agent_steps_sampled: 1232000\n",
      "    num_agent_steps_trained: 1232000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.087500000000006\n",
      "    ram_util_percent: 63.85\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 43.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.76\n",
      "    policy2: 1.076999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -20.5\n",
      "    policy2: -7.799999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28189494511717617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15708143593245302\n",
      "    mean_inference_ms: 3.1128610702152937\n",
      "    mean_raw_obs_processing_ms: 1.1257015939071282\n",
      "  time_since_restore: 2508.4124295711517\n",
      "  time_this_iter_s: 16.815775871276855\n",
      "  time_total_s: 2508.4124295711517\n",
      "  timers:\n",
      "    learn_throughput: 257.512\n",
      "    learn_time_ms: 15533.272\n",
      "    sample_throughput: 5011.885\n",
      "    sample_time_ms: 798.103\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1625111824\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         2508.41</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\">  30.837</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-57-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.49999999999991\n",
      "  episode_reward_mean: 31.115999999999918\n",
      "  episode_reward_min: 5.999999999999945\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6200\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2446759552694857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011487491661682725\n",
      "          policy_loss: -0.020894525077892467\n",
      "          total_loss: 35.94888353347778\n",
      "          vf_explained_var: 0.527084231376648\n",
      "          vf_loss: 35.96396219730377\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22657930478453636\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01309960600337945\n",
      "          policy_loss: -0.030159639252815396\n",
      "          total_loss: 3.3733420073986053\n",
      "          vf_explained_var: 0.2471790611743927\n",
      "          vf_loss: 3.3968700170516968\n",
      "    num_agent_steps_sampled: 1240000\n",
      "    num_agent_steps_trained: 1240000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.847826086956527\n",
      "    ram_util_percent: 64.43478260869564\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 13.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.38\n",
      "    policy2: 0.7359999999999978\n",
      "  policy_reward_min:\n",
      "    policy1: -0.5\n",
      "    policy2: -7.799999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28174106293211787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15727341715017323\n",
      "    mean_inference_ms: 3.11353282109004\n",
      "    mean_raw_obs_processing_ms: 1.1263753100744203\n",
      "  time_since_restore: 2524.508816719055\n",
      "  time_this_iter_s: 16.096387147903442\n",
      "  time_total_s: 2524.508816719055\n",
      "  timers:\n",
      "    learn_throughput: 256.326\n",
      "    learn_time_ms: 15605.138\n",
      "    sample_throughput: 4862.318\n",
      "    sample_time_ms: 822.653\n",
      "    update_time_ms: 2.413\n",
      "  timestamp: 1625111840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 155\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         2524.51</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\">  31.116</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.49999999999991\n",
      "  episode_reward_mean: 31.07399999999991\n",
      "  episode_reward_min: 5.999999999999945\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6225\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.24111131113022566\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01088010887906421\n",
      "          policy_loss: -0.02161408815300092\n",
      "          total_loss: 39.79644852876663\n",
      "          vf_explained_var: 0.5909992456436157\n",
      "          vf_loss: 39.81255429983139\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22883806889876723\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012047237731167115\n",
      "          policy_loss: -0.021810359612572938\n",
      "          total_loss: 1.8920131362974644\n",
      "          vf_explained_var: 0.2862408757209778\n",
      "          vf_loss: 1.9077246189117432\n",
      "    num_agent_steps_sampled: 1248000\n",
      "    num_agent_steps_trained: 1248000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.73\n",
      "    ram_util_percent: 57.55\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 13.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.755\n",
      "    policy2: 1.3189999999999973\n",
      "  policy_reward_min:\n",
      "    policy1: -0.5\n",
      "    policy2: -5.599999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28219782018241274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15714631130719842\n",
      "    mean_inference_ms: 3.1146704183188603\n",
      "    mean_raw_obs_processing_ms: 1.1260564770398307\n",
      "  time_since_restore: 2538.874915599823\n",
      "  time_this_iter_s: 14.366098880767822\n",
      "  time_total_s: 2538.874915599823\n",
      "  timers:\n",
      "    learn_throughput: 259.451\n",
      "    learn_time_ms: 15417.156\n",
      "    sample_throughput: 5127.419\n",
      "    sample_time_ms: 780.12\n",
      "    update_time_ms: 2.403\n",
      "  timestamp: 1625111854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         2538.87</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\">  31.074</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-57-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.49999999999991\n",
      "  episode_reward_mean: 33.10799999999992\n",
      "  episode_reward_min: -1.8000000000000544\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6275\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2535309884697199\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011989142498350702\n",
      "          policy_loss: -0.019312340853502974\n",
      "          total_loss: 36.60686683654785\n",
      "          vf_explained_var: 0.6438410878181458\n",
      "          vf_loss: 36.6201097369194\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.24673407478258014\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01467456721002236\n",
      "          policy_loss: -0.024277253018226475\n",
      "          total_loss: 1.9831787459552288\n",
      "          vf_explained_var: 0.39468204975128174\n",
      "          vf_loss: 2.000027023255825\n",
      "    num_agent_steps_sampled: 1256000\n",
      "    num_agent_steps_trained: 1256000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.18\n",
      "    ram_util_percent: 57.739999999999995\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 13.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.745\n",
      "    policy2: 1.3629999999999978\n",
      "  policy_reward_min:\n",
      "    policy1: 6.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28200560979829126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15703597339717298\n",
      "    mean_inference_ms: 3.1123310796429884\n",
      "    mean_raw_obs_processing_ms: 1.1250370991686338\n",
      "  time_since_restore: 2552.808137655258\n",
      "  time_this_iter_s: 13.93322205543518\n",
      "  time_total_s: 2552.808137655258\n",
      "  timers:\n",
      "    learn_throughput: 262.917\n",
      "    learn_time_ms: 15213.906\n",
      "    sample_throughput: 5138.34\n",
      "    sample_time_ms: 778.461\n",
      "    update_time_ms: 2.403\n",
      "  timestamp: 1625111868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         2552.81</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\">  33.108</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                -1.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-58-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.29999999999991\n",
      "  episode_reward_mean: 33.17999999999991\n",
      "  episode_reward_min: -1.8000000000000544\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6300\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.24417909421026707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010733691859059036\n",
      "          policy_loss: -0.022517229823279195\n",
      "          total_loss: 45.951009809970856\n",
      "          vf_explained_var: 0.46042293310165405\n",
      "          vf_loss: 45.968092262744904\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2182842968031764\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012256132380571216\n",
      "          policy_loss: -0.03211253497283906\n",
      "          total_loss: 2.4303066097199917\n",
      "          vf_explained_var: 0.30415263772010803\n",
      "          vf_loss: 2.4562144726514816\n",
      "    num_agent_steps_sampled: 1264000\n",
      "    num_agent_steps_trained: 1264000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.71\n",
      "    ram_util_percent: 58.11999999999999\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 7.600000000000007\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.52\n",
      "    policy2: 1.659999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: 6.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2814405632072228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15705286235993718\n",
      "    mean_inference_ms: 3.1091941474147045\n",
      "    mean_raw_obs_processing_ms: 1.1243498770344085\n",
      "  time_since_restore: 2566.877822637558\n",
      "  time_this_iter_s: 14.069684982299805\n",
      "  time_total_s: 2566.877822637558\n",
      "  timers:\n",
      "    learn_throughput: 265.316\n",
      "    learn_time_ms: 15076.365\n",
      "    sample_throughput: 5176.969\n",
      "    sample_time_ms: 772.653\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1625111882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         2566.88</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\">   33.18</td><td style=\"text-align: right;\">                45.3</td><td style=\"text-align: right;\">                -1.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.09999999999991\n",
      "  episode_reward_mean: 34.694999999999915\n",
      "  episode_reward_min: 10.49999999999998\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6350\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23996385373175144\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013039452052908018\n",
      "          policy_loss: -0.025721884914673865\n",
      "          total_loss: 35.456013441085815\n",
      "          vf_explained_var: 0.6026760339736938\n",
      "          vf_loss: 35.475133776664734\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23561969958245754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012928832205943763\n",
      "          policy_loss: -0.02691248523478862\n",
      "          total_loss: 2.122238140553236\n",
      "          vf_explained_var: 0.3011570870876312\n",
      "          vf_loss: 2.1426054425537586\n",
      "    num_agent_steps_sampled: 1272000\n",
      "    num_agent_steps_trained: 1272000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.985714285714288\n",
      "    ram_util_percent: 58.666666666666664\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 7.600000000000016\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.035\n",
      "    policy2: 1.6599999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: 6.5\n",
      "    policy2: -4.499999999999982\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2814288892614465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15673319478876466\n",
      "    mean_inference_ms: 3.105668382506468\n",
      "    mean_raw_obs_processing_ms: 1.1225450750319126\n",
      "  time_since_restore: 2581.576708316803\n",
      "  time_this_iter_s: 14.698885679244995\n",
      "  time_total_s: 2581.576708316803\n",
      "  timers:\n",
      "    learn_throughput: 268.219\n",
      "    learn_time_ms: 14913.21\n",
      "    sample_throughput: 5277.918\n",
      "    sample_time_ms: 757.875\n",
      "    update_time_ms: 2.396\n",
      "  timestamp: 1625111897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         2581.58</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\">  34.695</td><td style=\"text-align: right;\">                47.1</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-58-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.09999999999991\n",
      "  episode_reward_mean: 34.154999999999916\n",
      "  episode_reward_min: 5.999999999999952\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6400\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.23094602720811963\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010230292617052328\n",
      "          policy_loss: -0.0252016419544816\n",
      "          total_loss: 41.946721255779266\n",
      "          vf_explained_var: 0.50103759765625\n",
      "          vf_loss: 41.96674311161041\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21487319702282548\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010268649595673196\n",
      "          policy_loss: -0.024724029353819788\n",
      "          total_loss: 3.6730445697903633\n",
      "          vf_explained_var: 0.3073698878288269\n",
      "          vf_loss: 3.692570071667433\n",
      "    num_agent_steps_sampled: 1280000\n",
      "    num_agent_steps_trained: 1280000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.19130434782609\n",
      "    ram_util_percent: 59.5\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 43.5\n",
      "    policy2: 24.099999999999987\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.165\n",
      "    policy2: 1.9899999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: 5.0\n",
      "    policy2: -4.499999999999997\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2806888002227594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15666063680638193\n",
      "    mean_inference_ms: 3.100514542638389\n",
      "    mean_raw_obs_processing_ms: 1.12105733627096\n",
      "  time_since_restore: 2597.275527238846\n",
      "  time_this_iter_s: 15.698818922042847\n",
      "  time_total_s: 2597.275527238846\n",
      "  timers:\n",
      "    learn_throughput: 273.655\n",
      "    learn_time_ms: 14616.964\n",
      "    sample_throughput: 5420.467\n",
      "    sample_time_ms: 737.944\n",
      "    update_time_ms: 2.381\n",
      "  timestamp: 1625111913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         2597.28</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\">  34.155</td><td style=\"text-align: right;\">                47.1</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.7999999999999\n",
      "  episode_reward_mean: 34.433999999999905\n",
      "  episode_reward_min: 5.999999999999952\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6425\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2273476216942072\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00958817829086911\n",
      "          policy_loss: -0.013595336451544426\n",
      "          total_loss: 49.55330407619476\n",
      "          vf_explained_var: 0.552842915058136\n",
      "          vf_loss: 49.56204617023468\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22253302996978164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012104007560992613\n",
      "          policy_loss: -0.023993142211111262\n",
      "          total_loss: 2.5583997145295143\n",
      "          vf_explained_var: 0.2765542268753052\n",
      "          vf_loss: 2.5762651786208153\n",
      "    num_agent_steps_sampled: 1288000\n",
      "    num_agent_steps_trained: 1288000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.14545454545455\n",
      "    ram_util_percent: 60.290909090909096\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 24.099999999999987\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.345\n",
      "    policy2: 2.088999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: 5.0\n",
      "    policy2: -4.499999999999997\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2809781066871757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15649709368114412\n",
      "    mean_inference_ms: 3.1006081926710407\n",
      "    mean_raw_obs_processing_ms: 1.1204734533892486\n",
      "  time_since_restore: 2612.935785293579\n",
      "  time_this_iter_s: 15.660258054733276\n",
      "  time_total_s: 2612.935785293579\n",
      "  timers:\n",
      "    learn_throughput: 274.88\n",
      "    learn_time_ms: 14551.813\n",
      "    sample_throughput: 5586.2\n",
      "    sample_time_ms: 716.05\n",
      "    update_time_ms: 2.424\n",
      "  timestamp: 1625111928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         2612.94</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\">  34.434</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-59-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.7999999999999\n",
      "  episode_reward_mean: 34.66499999999991\n",
      "  episode_reward_min: 5.999999999999952\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6475\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2162245586514473\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009379845345392823\n",
      "          policy_loss: -0.021022748725954443\n",
      "          total_loss: 48.32509821653366\n",
      "          vf_explained_var: 0.5778358578681946\n",
      "          vf_loss: 48.34137284755707\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21284931153059006\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011947005550609902\n",
      "          policy_loss: -0.015597382182022557\n",
      "          total_loss: 1.8839512057602406\n",
      "          vf_explained_var: 0.28922873735427856\n",
      "          vf_loss: 1.8935003988444805\n",
      "    num_agent_steps_sampled: 1296000\n",
      "    num_agent_steps_trained: 1296000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.03333333333333\n",
      "    ram_util_percent: 60.79583333333333\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 8.700000000000005\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.84\n",
      "    policy2: 1.8249999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: 5.0\n",
      "    policy2: -5.599999999999993\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28081798360812704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15640851566624386\n",
      "    mean_inference_ms: 3.0987443871903055\n",
      "    mean_raw_obs_processing_ms: 1.1198072113965856\n",
      "  time_since_restore: 2629.7924761772156\n",
      "  time_this_iter_s: 16.856690883636475\n",
      "  time_total_s: 2629.7924761772156\n",
      "  timers:\n",
      "    learn_throughput: 272.45\n",
      "    learn_time_ms: 14681.568\n",
      "    sample_throughput: 5667.722\n",
      "    sample_time_ms: 705.751\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1625111945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         2629.79</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\">  34.665</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.7999999999999\n",
      "  episode_reward_mean: 34.7609999999999\n",
      "  episode_reward_min: 17.699999999999903\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6500\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.22046523028984666\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009326205872639548\n",
      "          policy_loss: -0.016885988152353093\n",
      "          total_loss: 43.75933909416199\n",
      "          vf_explained_var: 0.5231469869613647\n",
      "          vf_loss: 43.77150458097458\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2177793108858168\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013042160106124356\n",
      "          policy_loss: -0.027416320866905153\n",
      "          total_loss: 2.2818492725491524\n",
      "          vf_explained_var: 0.28750181198120117\n",
      "          vf_loss: 2.302663005888462\n",
      "    num_agent_steps_sampled: 1304000\n",
      "    num_agent_steps_trained: 1304000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.216\n",
      "    ram_util_percent: 62.916\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 8.700000000000005\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.815\n",
      "    policy2: 1.9459999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: 14.5\n",
      "    policy2: -5.599999999999993\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28040661076540707\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15650266614895825\n",
      "    mean_inference_ms: 3.0972273926376226\n",
      "    mean_raw_obs_processing_ms: 1.1198058606459425\n",
      "  time_since_restore: 2647.1809163093567\n",
      "  time_this_iter_s: 17.388440132141113\n",
      "  time_total_s: 2647.1809163093567\n",
      "  timers:\n",
      "    learn_throughput: 269.655\n",
      "    learn_time_ms: 14833.781\n",
      "    sample_throughput: 5639.116\n",
      "    sample_time_ms: 709.331\n",
      "    update_time_ms: 2.303\n",
      "  timestamp: 1625111963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         2647.18</td><td style=\"text-align: right;\">652000</td><td style=\"text-align: right;\">  34.761</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                17.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-59-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.1999999999999\n",
      "  episode_reward_mean: 34.49399999999991\n",
      "  episode_reward_min: 15.899999999999908\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6550\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.22121370490640402\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012302072544116527\n",
      "          policy_loss: -0.020638422603951767\n",
      "          total_loss: 34.98120719194412\n",
      "          vf_explained_var: 0.5532575845718384\n",
      "          vf_loss: 34.99561721086502\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2222320414148271\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011011709910235368\n",
      "          policy_loss: -0.01755351049359888\n",
      "          total_loss: 2.0164461880922318\n",
      "          vf_explained_var: 0.30996251106262207\n",
      "          vf_loss: 2.028425019234419\n",
      "    num_agent_steps_sampled: 1312000\n",
      "    num_agent_steps_trained: 1312000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.945454545454538\n",
      "    ram_util_percent: 63.400000000000006\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 8.700000000000005\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.175\n",
      "    policy2: 1.3189999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: 15.5\n",
      "    policy2: -5.599999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28074258398066493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15635664872393598\n",
      "    mean_inference_ms: 3.097507331633961\n",
      "    mean_raw_obs_processing_ms: 1.1195566903238605\n",
      "  time_since_restore: 2663.010507106781\n",
      "  time_this_iter_s: 15.829590797424316\n",
      "  time_total_s: 2663.010507106781\n",
      "  timers:\n",
      "    learn_throughput: 271.612\n",
      "    learn_time_ms: 14726.873\n",
      "    sample_throughput: 5574.329\n",
      "    sample_time_ms: 717.575\n",
      "    update_time_ms: 2.338\n",
      "  timestamp: 1625111979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         2663.01</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\">  34.494</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                15.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_12-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.29999999999991\n",
      "  episode_reward_mean: 33.21599999999991\n",
      "  episode_reward_min: 8.999999999999936\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6600\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2167542432434857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0091228888486512\n",
      "          policy_loss: -0.01421427502646111\n",
      "          total_loss: 45.66939890384674\n",
      "          vf_explained_var: 0.47786056995391846\n",
      "          vf_loss: 45.67899560928345\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.216612639836967\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012799161399016157\n",
      "          policy_loss: -0.020569579952280037\n",
      "          total_loss: 2.9076109528541565\n",
      "          vf_explained_var: 0.22321711480617523\n",
      "          vf_loss: 2.921700932085514\n",
      "    num_agent_steps_sampled: 1320000\n",
      "    num_agent_steps_trained: 1320000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.121739130434776\n",
      "    ram_util_percent: 63.81739130434782\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 9.800000000000008\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.04\n",
      "    policy2: 1.1759999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: 8.0\n",
      "    policy2: -6.699999999999993\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28042284377922294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1565019494662981\n",
      "    mean_inference_ms: 3.096816875107943\n",
      "    mean_raw_obs_processing_ms: 1.1197205663816718\n",
      "  time_since_restore: 2678.6499860286713\n",
      "  time_this_iter_s: 15.639478921890259\n",
      "  time_total_s: 2678.6499860286713\n",
      "  timers:\n",
      "    learn_throughput: 271.899\n",
      "    learn_time_ms: 14711.356\n",
      "    sample_throughput: 5761.697\n",
      "    sample_time_ms: 694.24\n",
      "    update_time_ms: 2.346\n",
      "  timestamp: 1625111994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         2678.65</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\">  33.216</td><td style=\"text-align: right;\">                45.3</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-00-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.9999999999999\n",
      "  episode_reward_mean: 32.58299999999991\n",
      "  episode_reward_min: 8.999999999999936\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6625\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.21668453561142087\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01135477764182724\n",
      "          policy_loss: -0.01901558364625089\n",
      "          total_loss: 53.20595681667328\n",
      "          vf_explained_var: 0.5217186212539673\n",
      "          vf_loss: 53.21922433376312\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22596211871132255\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011995920329354703\n",
      "          policy_loss: -0.02383388642920181\n",
      "          total_loss: 2.154336307197809\n",
      "          vf_explained_var: 0.2713179588317871\n",
      "          vf_loss: 2.1720972433686256\n",
      "    num_agent_steps_sampled: 1328000\n",
      "    num_agent_steps_trained: 1328000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.0\n",
      "    ram_util_percent: 63.950000000000024\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 9.800000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.835\n",
      "    policy2: 1.7479999999999989\n",
      "  policy_reward_min:\n",
      "    policy1: 8.0\n",
      "    policy2: -6.699999999999993\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2807712881457989\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15636039953646777\n",
      "    mean_inference_ms: 3.097522844900917\n",
      "    mean_raw_obs_processing_ms: 1.1193566257276115\n",
      "  time_since_restore: 2694.548259973526\n",
      "  time_this_iter_s: 15.898273944854736\n",
      "  time_total_s: 2694.548259973526\n",
      "  timers:\n",
      "    learn_throughput: 269.436\n",
      "    learn_time_ms: 14845.826\n",
      "    sample_throughput: 5612.204\n",
      "    sample_time_ms: 712.732\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1625112010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         2694.55</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\">  32.583</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-00-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.9999999999999\n",
      "  episode_reward_mean: 32.612999999999914\n",
      "  episode_reward_min: 8.999999999999936\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6675\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.21538107795640826\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010739857898443006\n",
      "          policy_loss: -0.015469940175535157\n",
      "          total_loss: 43.212106227874756\n",
      "          vf_explained_var: 0.5664441585540771\n",
      "          vf_loss: 43.222140073776245\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2082415777258575\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012461019810871221\n",
      "          policy_loss: -0.026336576614994556\n",
      "          total_loss: 2.082527332007885\n",
      "          vf_explained_var: 0.40583980083465576\n",
      "          vf_loss: 2.102555528283119\n",
      "    num_agent_steps_sampled: 1336000\n",
      "    num_agent_steps_trained: 1336000\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.843478260869574\n",
      "    ram_util_percent: 62.92173913043479\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 10.90000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.26\n",
      "    policy2: 2.352999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: 8.0\n",
      "    policy2: -3.399999999999997\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28089167868800613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1564046345824147\n",
      "    mean_inference_ms: 3.0986907017267775\n",
      "    mean_raw_obs_processing_ms: 1.1199323825142866\n",
      "  time_since_restore: 2710.701154947281\n",
      "  time_this_iter_s: 16.152894973754883\n",
      "  time_total_s: 2710.701154947281\n",
      "  timers:\n",
      "    learn_throughput: 265.83\n",
      "    learn_time_ms: 15047.194\n",
      "    sample_throughput: 5454.68\n",
      "    sample_time_ms: 733.315\n",
      "    update_time_ms: 2.419\n",
      "  timestamp: 1625112026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">          2710.7</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\">  32.613</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-00-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.9999999999999\n",
      "  episode_reward_mean: 33.42299999999991\n",
      "  episode_reward_min: 11.699999999999925\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6700\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2069063000380993\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008866777963703498\n",
      "          policy_loss: -0.01646372675895691\n",
      "          total_loss: 47.40563225746155\n",
      "          vf_explained_var: 0.4623560905456543\n",
      "          vf_loss: 47.417606830596924\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.20832913368940353\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01280846977897454\n",
      "          policy_loss: -0.026443445007316768\n",
      "          total_loss: 2.4558191746473312\n",
      "          vf_explained_var: 0.2754477262496948\n",
      "          vf_loss: 2.475778304040432\n",
      "    num_agent_steps_sampled: 1344000\n",
      "    num_agent_steps_trained: 1344000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.154166666666665\n",
      "    ram_util_percent: 60.020833333333336\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 10.90000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.07\n",
      "    policy2: 2.352999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: 8.5\n",
      "    policy2: -3.3999999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28061519947126745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15656990671556514\n",
      "    mean_inference_ms: 3.098551496674917\n",
      "    mean_raw_obs_processing_ms: 1.1205507385487772\n",
      "  time_since_restore: 2727.123549938202\n",
      "  time_this_iter_s: 16.42239499092102\n",
      "  time_total_s: 2727.123549938202\n",
      "  timers:\n",
      "    learn_throughput: 261.852\n",
      "    learn_time_ms: 15275.792\n",
      "    sample_throughput: 5409.717\n",
      "    sample_time_ms: 739.41\n",
      "    update_time_ms: 2.553\n",
      "  timestamp: 1625112043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         2727.12</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\">  33.423</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-01-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.9999999999999\n",
      "  episode_reward_mean: 33.500999999999905\n",
      "  episode_reward_min: 12.299999999999914\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6750\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.21435610111802816\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010512712833588012\n",
      "          policy_loss: -0.0234015857859049\n",
      "          total_loss: 40.08094775676727\n",
      "          vf_explained_var: 0.5776660442352295\n",
      "          vf_loss: 40.099027037620544\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.21751248044893146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014977337123127654\n",
      "          policy_loss: -0.024540371625334956\n",
      "          total_loss: 3.1083216592669487\n",
      "          vf_explained_var: 0.3077752888202667\n",
      "          vf_loss: 3.1252797544002533\n",
      "    num_agent_steps_sampled: 1352000\n",
      "    num_agent_steps_trained: 1352000\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.94166666666667\n",
      "    ram_util_percent: 62.34166666666666\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 47.0\n",
      "    policy2: 12.000000000000012\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.5\n",
      "    policy2: 2.0009999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: 7.5\n",
      "    policy2: -5.5999999999999925\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28126878844254516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15655658225551686\n",
      "    mean_inference_ms: 3.102298353685778\n",
      "    mean_raw_obs_processing_ms: 1.1215351328285441\n",
      "  time_since_restore: 2744.4915618896484\n",
      "  time_this_iter_s: 17.368011951446533\n",
      "  time_total_s: 2744.4915618896484\n",
      "  timers:\n",
      "    learn_throughput: 258.061\n",
      "    learn_time_ms: 15500.21\n",
      "    sample_throughput: 5116.447\n",
      "    sample_time_ms: 781.793\n",
      "    update_time_ms: 2.55\n",
      "  timestamp: 1625112060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         2744.49</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\">  33.501</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                12.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-01-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.5999999999999\n",
      "  episode_reward_mean: 32.609999999999914\n",
      "  episode_reward_min: 6.000000000000017\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6800\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2170128095895052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007484836882213131\n",
      "          policy_loss: -0.007072918291669339\n",
      "          total_loss: 63.3177444934845\n",
      "          vf_explained_var: 0.3791038393974304\n",
      "          vf_loss: 63.32102870941162\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23342740861698985\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013340885954676196\n",
      "          policy_loss: -0.016367964155506343\n",
      "          total_loss: 20.042811572551727\n",
      "          vf_explained_var: 0.37741708755493164\n",
      "          vf_loss: 20.052425242960453\n",
      "    num_agent_steps_sampled: 1360000\n",
      "    num_agent_steps_trained: 1360000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.426086956521733\n",
      "    ram_util_percent: 62.408695652173925\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 67.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.63\n",
      "    policy2: 2.9800000000000004\n",
      "  policy_reward_min:\n",
      "    policy1: -41.5\n",
      "    policy2: -5.5999999999999925\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28121911984890935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15684564126590636\n",
      "    mean_inference_ms: 3.104691612392257\n",
      "    mean_raw_obs_processing_ms: 1.1228216625420926\n",
      "  time_since_restore: 2760.6181559562683\n",
      "  time_this_iter_s: 16.126594066619873\n",
      "  time_total_s: 2760.6181559562683\n",
      "  timers:\n",
      "    learn_throughput: 257.552\n",
      "    learn_time_ms: 15530.867\n",
      "    sample_throughput: 5038.415\n",
      "    sample_time_ms: 793.9\n",
      "    update_time_ms: 2.528\n",
      "  timestamp: 1625112076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         2760.62</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\">   32.61</td><td style=\"text-align: right;\">                48.6</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-01-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.5999999999999\n",
      "  episode_reward_mean: 32.72399999999991\n",
      "  episode_reward_min: 6.000000000000017\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6825\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.21780216647312045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010549621743848547\n",
      "          policy_loss: -0.023908889677841216\n",
      "          total_loss: 36.60573881864548\n",
      "          vf_explained_var: 0.5831892490386963\n",
      "          vf_loss: 36.62430739402771\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23955459892749786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02512041007867083\n",
      "          policy_loss: -0.029194995382567868\n",
      "          total_loss: 2.906319811940193\n",
      "          vf_explained_var: 0.30022674798965454\n",
      "          vf_loss: 2.922797605395317\n",
      "    num_agent_steps_sampled: 1368000\n",
      "    num_agent_steps_trained: 1368000\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.66086956521739\n",
      "    ram_util_percent: 61.7695652173913\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 67.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.81\n",
      "    policy2: 2.914000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -41.5\n",
      "    policy2: -5.5999999999999925\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2815741178606337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15671156221836607\n",
      "    mean_inference_ms: 3.1054148389168255\n",
      "    mean_raw_obs_processing_ms: 1.122370679379411\n",
      "  time_since_restore: 2776.360114812851\n",
      "  time_this_iter_s: 15.741958856582642\n",
      "  time_total_s: 2776.360114812851\n",
      "  timers:\n",
      "    learn_throughput: 257.389\n",
      "    learn_time_ms: 15540.706\n",
      "    sample_throughput: 5048.873\n",
      "    sample_time_ms: 792.256\n",
      "    update_time_ms: 2.507\n",
      "  timestamp: 1625112092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         2776.36</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\">  32.724</td><td style=\"text-align: right;\">                48.6</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-01-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.299999999999905\n",
      "  episode_reward_mean: 33.749999999999915\n",
      "  episode_reward_min: 6.000000000000017\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6875\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.21999898832291365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00967420803499408\n",
      "          policy_loss: -0.01003492617746815\n",
      "          total_loss: 44.01618945598602\n",
      "          vf_explained_var: 0.5957474112510681\n",
      "          vf_loss: 44.02132749557495\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.7593750000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23727521440014243\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009440409878152423\n",
      "          policy_loss: -0.023838887544116005\n",
      "          total_loss: 2.3923235684633255\n",
      "          vf_explained_var: 0.3659172058105469\n",
      "          vf_loss: 2.408993635326624\n",
      "    num_agent_steps_sampled: 1376000\n",
      "    num_agent_steps_trained: 1376000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.852173913043476\n",
      "    ram_util_percent: 62.50434782608696\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 67.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.385\n",
      "    policy2: 3.365000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -41.5\n",
      "    policy2: -4.499999999999991\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28146900183844453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15668830870629413\n",
      "    mean_inference_ms: 3.1045605636334948\n",
      "    mean_raw_obs_processing_ms: 1.1218893087089736\n",
      "  time_since_restore: 2792.372878074646\n",
      "  time_this_iter_s: 16.012763261795044\n",
      "  time_total_s: 2792.372878074646\n",
      "  timers:\n",
      "    learn_throughput: 258.809\n",
      "    learn_time_ms: 15455.392\n",
      "    sample_throughput: 5045.153\n",
      "    sample_time_ms: 792.84\n",
      "    update_time_ms: 2.587\n",
      "  timestamp: 1625112108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         2792.37</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\">   33.75</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-02-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.799999999999905\n",
      "  episode_reward_mean: 33.36899999999991\n",
      "  episode_reward_min: 9.899999999999968\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6900\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.22293304931372404\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011049154039938003\n",
      "          policy_loss: -0.020793142612092197\n",
      "          total_loss: 52.960651993751526\n",
      "          vf_explained_var: 0.4275742471218109\n",
      "          vf_loss: 52.975852370262146\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.7593750000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23210770497098565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008277086017187685\n",
      "          policy_loss: -0.030286547989817336\n",
      "          total_loss: 2.6499309837818146\n",
      "          vf_explained_var: 0.4034806787967682\n",
      "          vf_loss: 2.6739321276545525\n",
      "    num_agent_steps_sampled: 1384000\n",
      "    num_agent_steps_trained: 1384000\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.22083333333334\n",
      "    ram_util_percent: 62.84166666666667\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 19.69999999999992\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.17\n",
      "    policy2: 2.1989999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: 4.5\n",
      "    policy2: -4.500000000000004\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28119433337448635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15688638634807484\n",
      "    mean_inference_ms: 3.1050500268864183\n",
      "    mean_raw_obs_processing_ms: 1.1225437790067139\n",
      "  time_since_restore: 2809.545366048813\n",
      "  time_this_iter_s: 17.17248797416687\n",
      "  time_total_s: 2809.545366048813\n",
      "  timers:\n",
      "    learn_throughput: 259.671\n",
      "    learn_time_ms: 15404.115\n",
      "    sample_throughput: 4863.446\n",
      "    sample_time_ms: 822.462\n",
      "    update_time_ms: 2.562\n",
      "  timestamp: 1625112126\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         2809.55</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\">  33.369</td><td style=\"text-align: right;\">                46.8</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-02-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.89999999999991\n",
      "  episode_reward_mean: 32.58899999999992\n",
      "  episode_reward_min: -49.50000000000007\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6950\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.21623583836480975\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016494034833158366\n",
      "          policy_loss: -0.03617893095361069\n",
      "          total_loss: 54.19572901725769\n",
      "          vf_explained_var: 0.4942289888858795\n",
      "          vf_loss: 54.22355842590332\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.7593750000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23760444670915604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014532550718286075\n",
      "          policy_loss: -0.028670506115304306\n",
      "          total_loss: 3.310332976281643\n",
      "          vf_explained_var: 0.40425562858581543\n",
      "          vf_loss: 3.327967833727598\n",
      "    num_agent_steps_sampled: 1392000\n",
      "    num_agent_steps_trained: 1392000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.92\n",
      "    ram_util_percent: 64.85999999999999\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 19.69999999999992\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.665\n",
      "    policy2: 1.9239999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -39.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2818353003855125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15692257945278354\n",
      "    mean_inference_ms: 3.1089439693371257\n",
      "    mean_raw_obs_processing_ms: 1.1234725733057258\n",
      "  time_since_restore: 2827.22754406929\n",
      "  time_this_iter_s: 17.682178020477295\n",
      "  time_total_s: 2827.22754406929\n",
      "  timers:\n",
      "    learn_throughput: 256.605\n",
      "    learn_time_ms: 15588.17\n",
      "    sample_throughput: 4856.866\n",
      "    sample_time_ms: 823.576\n",
      "    update_time_ms: 2.568\n",
      "  timestamp: 1625112143\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         2827.23</td><td style=\"text-align: right;\">696000</td><td style=\"text-align: right;\">  32.589</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">               -49.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-02-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.89999999999991\n",
      "  episode_reward_mean: 32.65799999999991\n",
      "  episode_reward_min: -49.50000000000007\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7000\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.22391396202147007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00959914390114136\n",
      "          policy_loss: -0.022862165642436594\n",
      "          total_loss: 46.81623840332031\n",
      "          vf_explained_var: 0.45141756534576416\n",
      "          vf_loss: 46.83424139022827\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.7593750000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.20837497198954225\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008938589060562663\n",
      "          policy_loss: -0.01951913896482438\n",
      "          total_loss: 5.561063528060913\n",
      "          vf_explained_var: 0.28691565990448\n",
      "          vf_loss: 5.5737950429320335\n",
      "    num_agent_steps_sampled: 1400000\n",
      "    num_agent_steps_trained: 1400000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.812000000000005\n",
      "    ram_util_percent: 62.364000000000004\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 17.499999999999986\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.195\n",
      "    policy2: 2.463\n",
      "  policy_reward_min:\n",
      "    policy1: -39.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2817430966049372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15727039159503778\n",
      "    mean_inference_ms: 3.1112193621932653\n",
      "    mean_raw_obs_processing_ms: 1.1249100272718409\n",
      "  time_since_restore: 2844.6534090042114\n",
      "  time_this_iter_s: 17.425864934921265\n",
      "  time_total_s: 2844.6534090042114\n",
      "  timers:\n",
      "    learn_throughput: 253.877\n",
      "    learn_time_ms: 15755.68\n",
      "    sample_throughput: 4793.132\n",
      "    sample_time_ms: 834.527\n",
      "    update_time_ms: 2.618\n",
      "  timestamp: 1625112161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         2844.65</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\">  32.658</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">               -49.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-02-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.4999999999999\n",
      "  episode_reward_mean: 34.02599999999991\n",
      "  episode_reward_min: -49.50000000000007\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7025\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.20179722364991903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010360228407080285\n",
      "          policy_loss: -0.019091066365945153\n",
      "          total_loss: 48.753127574920654\n",
      "          vf_explained_var: 0.5255646109580994\n",
      "          vf_loss: 48.76697289943695\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.7593750000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2266395720653236\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005456808463350171\n",
      "          policy_loss: -0.009498657600488514\n",
      "          total_loss: 9.040446802973747\n",
      "          vf_explained_var: 0.2749364972114563\n",
      "          vf_loss: 9.045801915228367\n",
      "    num_agent_steps_sampled: 1408000\n",
      "    num_agent_steps_trained: 1408000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.820833333333336\n",
      "    ram_util_percent: 58.75\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 30.699999999999953\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.365\n",
      "    policy2: 2.660999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -39.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28210053611595326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15715289864642773\n",
      "    mean_inference_ms: 3.1116756815098796\n",
      "    mean_raw_obs_processing_ms: 1.1243879211105063\n",
      "  time_since_restore: 2861.2049810886383\n",
      "  time_this_iter_s: 16.55157208442688\n",
      "  time_total_s: 2861.2049810886383\n",
      "  timers:\n",
      "    learn_throughput: 252.592\n",
      "    learn_time_ms: 15835.812\n",
      "    sample_throughput: 4878.308\n",
      "    sample_time_ms: 819.956\n",
      "    update_time_ms: 2.543\n",
      "  timestamp: 1625112177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">          2861.2</td><td style=\"text-align: right;\">704000</td><td style=\"text-align: right;\">  34.026</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">               -49.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-03-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.1999999999999\n",
      "  episode_reward_mean: 35.13599999999991\n",
      "  episode_reward_min: 10.499999999999918\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7075\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.21377791184931993\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012634761922527105\n",
      "          policy_loss: -0.019614091113908216\n",
      "          total_loss: 41.71421056985855\n",
      "          vf_explained_var: 0.5917768478393555\n",
      "          vf_loss: 41.727427661418915\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.7593750000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2331296820193529\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007645679630513769\n",
      "          policy_loss: -0.0190679648949299\n",
      "          total_loss: 4.48478252440691\n",
      "          vf_explained_var: 0.3659772574901581\n",
      "          vf_loss: 4.498044490814209\n",
      "    num_agent_steps_sampled: 1416000\n",
      "    num_agent_steps_trained: 1416000\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.528\n",
      "    ram_util_percent: 62.592000000000006\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 30.699999999999953\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.43\n",
      "    policy2: 3.7059999999999995\n",
      "  policy_reward_min:\n",
      "    policy1: -1.5\n",
      "    policy2: -5.599999999999991\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2820816309299578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15715635100644765\n",
      "    mean_inference_ms: 3.1113234319925396\n",
      "    mean_raw_obs_processing_ms: 1.1242759083000027\n",
      "  time_since_restore: 2878.6188101768494\n",
      "  time_this_iter_s: 17.41382908821106\n",
      "  time_total_s: 2878.6188101768494\n",
      "  timers:\n",
      "    learn_throughput: 250.559\n",
      "    learn_time_ms: 15964.273\n",
      "    sample_throughput: 4898.693\n",
      "    sample_time_ms: 816.544\n",
      "    update_time_ms: 3.457\n",
      "  timestamp: 1625112195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         2878.62</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\">  35.136</td><td style=\"text-align: right;\">                52.2</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-03-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.1999999999999\n",
      "  episode_reward_mean: 35.85599999999991\n",
      "  episode_reward_min: 10.499999999999918\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7100\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.2073258738964796\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0073325229532201774\n",
      "          policy_loss: -0.01641579464194365\n",
      "          total_loss: 78.31391596794128\n",
      "          vf_explained_var: 0.40429461002349854\n",
      "          vf_loss: 78.3266190290451\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.7593750000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.2097033360041678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005826980544952676\n",
      "          policy_loss: -0.012114530516555533\n",
      "          total_loss: 23.63738840818405\n",
      "          vf_explained_var: 0.41396772861480713\n",
      "          vf_loss: 23.64507833123207\n",
      "    num_agent_steps_sampled: 1424000\n",
      "    num_agent_steps_trained: 1424000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.458333333333332\n",
      "    ram_util_percent: 66.97916666666667\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 30.699999999999953\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.425\n",
      "    policy2: 3.431\n",
      "  policy_reward_min:\n",
      "    policy1: -1.5\n",
      "    policy2: -5.599999999999992\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.28168720216632254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1572790807831863\n",
      "    mean_inference_ms: 3.1107925932650664\n",
      "    mean_raw_obs_processing_ms: 1.1245735632199159\n",
      "  time_since_restore: 2895.5402953624725\n",
      "  time_this_iter_s: 16.92148518562317\n",
      "  time_total_s: 2895.5402953624725\n",
      "  timers:\n",
      "    learn_throughput: 249.961\n",
      "    learn_time_ms: 16002.5\n",
      "    sample_throughput: 4827.635\n",
      "    sample_time_ms: 828.563\n",
      "    update_time_ms: 3.444\n",
      "  timestamp: 1625112212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         2895.54</td><td style=\"text-align: right;\">712000</td><td style=\"text-align: right;\">  35.856</td><td style=\"text-align: right;\">                52.2</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-03-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.699999999999896\n",
      "  episode_reward_mean: 35.14499999999991\n",
      "  episode_reward_min: 15.899999999999922\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7150\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.18840395519509912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009152248705504462\n",
      "          policy_loss: -0.021580108907073736\n",
      "          total_loss: 44.6403426527977\n",
      "          vf_explained_var: 0.588764488697052\n",
      "          vf_loss: 44.65728962421417\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.7593750000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.23405207553878427\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014987842965638265\n",
      "          policy_loss: -0.029863160307286307\n",
      "          total_loss: 3.000192992389202\n",
      "          vf_explained_var: 0.3775119185447693\n",
      "          vf_loss: 3.018674746155739\n",
      "    num_agent_steps_sampled: 1432000\n",
      "    num_agent_steps_trained: 1432000\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.23913043478261\n",
      "    ram_util_percent: 63.40000000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.5\n",
      "    policy2: 73.6\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.12\n",
      "    policy2: 4.0249999999999995\n",
      "  policy_reward_min:\n",
      "    policy1: -52.0\n",
      "    policy2: -5.599999999999992\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2822262930139857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15723642324166176\n",
      "    mean_inference_ms: 3.113734452398211\n",
      "    mean_raw_obs_processing_ms: 1.1249433050120081\n",
      "  time_since_restore: 2911.5269105434418\n",
      "  time_this_iter_s: 15.986615180969238\n",
      "  time_total_s: 2911.5269105434418\n",
      "  timers:\n",
      "    learn_throughput: 251.895\n",
      "    learn_time_ms: 15879.638\n",
      "    sample_throughput: 4917.791\n",
      "    sample_time_ms: 813.373\n",
      "    update_time_ms: 3.43\n",
      "  timestamp: 1625112228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         2911.53</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\">  35.145</td><td style=\"text-align: right;\">                50.7</td><td style=\"text-align: right;\">                15.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_7f8d1_00000:\n",
      "  agent_timesteps_total: 1440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-01_13-04-02\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.9999999999999\n",
      "  episode_reward_mean: 36.81599999999991\n",
      "  episode_reward_min: 12.89999999999991\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7200\n",
      "  experiment_id: 9a7578a6603e466f8f19eb8a71ff19dd\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062500000000001\n",
      "          cur_lr: 0.0001\n",
      "          entropy: 0.19940048549324274\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011309475332382135\n",
      "          policy_loss: -0.02954732798389159\n",
      "          total_loss: 56.96694266796112\n",
      "          vf_explained_var: 0.458646297454834\n",
      "          vf_loss: 56.990763902664185\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.7593750000000001\n",
      "          cur_lr: 0.0002\n",
      "          entropy: 0.22656512958928943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0100451329126372\n",
      "          policy_loss: -0.02493914557271637\n",
      "          total_loss: 1.9867265447974205\n",
      "          vf_explained_var: 0.3182090222835541\n",
      "          vf_loss: 2.004037693142891\n",
      "    num_agent_steps_sampled: 1440000\n",
      "    num_agent_steps_trained: 1440000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.015\n",
      "    ram_util_percent: 57.82000000000001\n",
      "  pid: 21670\n",
      "  policy_reward_max:\n",
      "    policy1: 53.5\n",
      "    policy2: 73.6\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.495\n",
      "    policy2: 3.320999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -52.0\n",
      "    policy2: -3.4000000000000066\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2818022297993172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15733733473910458\n",
      "    mean_inference_ms: 3.1128775918875533\n",
      "    mean_raw_obs_processing_ms: 1.1250056775722568\n",
      "  time_since_restore: 2925.9923446178436\n",
      "  time_this_iter_s: 14.465434074401855\n",
      "  time_total_s: 2925.9923446178436\n",
      "  timers:\n",
      "    learn_throughput: 254.306\n",
      "    learn_time_ms: 15729.104\n",
      "    sample_throughput: 5014.599\n",
      "    sample_time_ms: 797.671\n",
      "    update_time_ms: 3.455\n",
      "  timestamp: 1625112242\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: 7f8d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>RUNNING </td><td>172.30.1.40:21670</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         2925.99</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\">  36.816</td><td style=\"text-align: right;\">                  54</td><td style=\"text-align: right;\">                12.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_7f8d1_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         2925.99</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\">  36.816</td><td style=\"text-align: right;\">                  54</td><td style=\"text-align: right;\">                12.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-01 13:04:03,100\tINFO tune.py:549 -- Total run time: 2952.21 seconds (2951.97 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "# Solution to Exercise #2\n",
    "\n",
    "# Run for longer this time (100 iterations) and try to reach 40.0 reward (sum of both agents).\n",
    "stop = {\n",
    "    \"training_iteration\": 180,  # we have the 15min break now to run this many iterations\n",
    "    \"episode_reward_mean\": 60.0,  # sum of both agents' rewards. Probably won't reach it, but we should try nevertheless :)\n",
    "}\n",
    "\n",
    "# tune_config.update({\n",
    "# ???\n",
    "# })\n",
    "\n",
    "# analysis = tune.run(...)\n",
    "\n",
    "tune_config[\"lr\"] = 0.0001\n",
    "tune_config[\"train_batch_size\"] = 4000\n",
    "tune_config[\"num_envs_per_worker\"] = 5\n",
    "tune_config[\"num_workers\"] = 5\n",
    "\n",
    "analysis = tune.run(\"PPO\", config=tune_config, stop=stop, checkpoint_at_end=True, checkpoint_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d282a-4ad1-4d5f-9dec-00afb8154048",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "------------------\n",
    "\n",
    "\n",
    "(while the above experiment is running (and hopefully learning))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do we extract any checkpoint from a trial of a tune.run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5160e4d0-8feb-411d-a457-dfc10d50e909",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-01 13:05:16,160\tWARNING ppo.py:135 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=5 num_envs_per_worker=5 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 160.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis object at 0x7f8196a0b400>\n",
      "Found best checkpoint for trial #2: /Users/parksurk/ray_results/PPO/PPO_MultiAgentArena_7f8d1_00000_0_2021-07-01_12-14-51/checkpoint_000180/checkpoint-180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "2021-07-01 13:05:26,819\tINFO trainable.py:101 -- Trainable.setup took 10.661 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-07-01 13:05:26,822\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "2021-07-01 13:05:26,849\tINFO trainable.py:377 -- Restored on 172.30.1.40 from checkpoint: /Users/parksurk/ray_results/PPO/PPO_MultiAgentArena_7f8d1_00000_0_2021-07-01_12-14-51/checkpoint_000180/checkpoint-180\n",
      "2021-07-01 13:05:26,850\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 180, '_timesteps_total': None, '_time_total': 2925.9923446178436, '_episodes_total': 7200}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPO"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the exercise) returned an Analysis object, from which we can access any checkpoint\n",
    "# (given we set checkpoint_freq or checkpoint_at_end to reasonable values) like so:\n",
    "print(analysis)\n",
    "# Get all trials (we only have one).\n",
    "trials = analysis.trials\n",
    "# Assuming, the first trial was the best, we'd like to extract this trial's best checkpoint \"\":\n",
    "best_checkpoint = analysis.get_best_checkpoint(trial=trials[0], metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(f\"Found best checkpoint for trial #2: {best_checkpoint}\")\n",
    "\n",
    "# Undo the grid-search config, which RLlib doesn't understand.\n",
    "rllib_config = tune_config.copy()\n",
    "rllib_config[\"lr\"] = 0.00005\n",
    "rllib_config[\"train_batch_size\"] = 4000\n",
    "\n",
    "# Restore a RLlib Trainer from the checkpoint.\n",
    "new_trainer = PPOTrainer(config=rllib_config)\n",
    "new_trainer.restore(best_checkpoint)\n",
    "new_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a2e104d-72f8-4b80-bf9a-2f8cbd25d9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300ced6cef0b4a2c92c5270da015bfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        a1 = new_trainer.compute_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "        a2 = new_trainer.compute_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "        actions = {\"agent1\": a1, \"agent2\": a2}\n",
    "        obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.07)\n",
    "\n",
    "        if dones[\"agent1\"] is True:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3f56e-c4e1-4503-9ce5-589e826d1e5a",
   "metadata": {},
   "source": [
    "## Let's talk about customization options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f940b-697c-4d1d-af28-1d174331dc3c",
   "metadata": {},
   "source": [
    "### Deep Dive: How do we customize RLlib's RL loop?\n",
    "\n",
    "RLlib offers a callbacks API that allows you to add custom behavior to\n",
    "all major events during the environment sampling- and learning process.\n",
    "\n",
    "**Our problem:** So far, we can only see standard stats, such as rewards, episode lengths, etc..\n",
    "This does not give us enough insights sometimes into important questions, such as: How many times\n",
    "have both agents collided? or How many times has agent1 discovered a new field?\n",
    "\n",
    "In the following cell, we will create custom callback \"hooks\" that will allow us to\n",
    "add these stats to the returned metrics dict, and which will therefore be displayed in tensorboard!\n",
    "\n",
    "For that we will override RLlib's DefaultCallbacks class and implement the\n",
    "`on_episode_start`, `on_episode_step`, and `on_episode_end` methods therein:\n",
    "\n",
    "RLlib는 환경 샘플링 및 학습 과정 동안의 모든 주요 이벤트에 대응하는 사용자 정의 행동을 추가 할 수있는 콜백 API를 제공합니다.\n",
    "\n",
    "**문제 :** 지금까지는 Reward, 에피소드 길이 등과 같은 표준 통계만 볼 수 있었습니다.\n",
    "이것은 때때로 다음과 같은 중요한 질문에 대한 충분한 통찰력을 제공하지 않습니다.\n",
    "두 요원이 충돌 했습니까? 또는 agent1이 새 필드를 몇 번이나 발견 했습니까?\n",
    "\n",
    "다음 셀에서는 사용자 정의 Callback \"hook\"를 생성하고, 이런 통계치를 반환된 메트릭 딕셔너리에 추가하여 텐서 보드에 표시합니다!\n",
    "\n",
    "이를 위해 RLlib의 DefaultCallbacks 클래스를 재정의하고\n",
    "`on_episode_start`,`on_episode_step` 및`on_episode_end` 메소드를 구현합니다. :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adcb9733-01bc-426b-ad57-7983fc7db8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the DefaultCallbacks with your own and implement any methods (hooks)\n",
    "# that you need.\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self,\n",
    "                         *,\n",
    "                         worker,\n",
    "                         base_env,\n",
    "                         policies,\n",
    "                         episode: MultiAgentEpisode,\n",
    "                         env_index,\n",
    "                         **kwargs):\n",
    "        # We will use the `MultiAgentEpisode` object being passed into\n",
    "        # all episode-related callbacks. It comes with a user_data property (dict),\n",
    "        # which we can write arbitrary data into.\n",
    "\n",
    "        # At the end of an episode, we'll transfer that data into the `hist_data`, and `custom_metrics`\n",
    "        # properties to make sure our custom data is displayed in TensorBoard.\n",
    "\n",
    "        # The episode is starting:\n",
    "        # Set per-episode object to capture, which states (observations)\n",
    "        # have been visited by agent1.\n",
    "        episode.user_data[\"new_fields_discovered\"] = 0\n",
    "        # Set per-episode agent2-blocks counter (how many times has agent2 blocked agent1?).\n",
    "        episode.user_data[\"num_collisions\"] = 0\n",
    "\n",
    "    def on_episode_step(self,\n",
    "                        *,\n",
    "                        worker,\n",
    "                        base_env,\n",
    "                        episode: MultiAgentEpisode,\n",
    "                        env_index,\n",
    "                        **kwargs):\n",
    "        # Get both rewards.\n",
    "        ag1_r = episode.prev_reward_for(\"agent1\")\n",
    "        ag2_r = episode.prev_reward_for(\"agent2\")\n",
    "\n",
    "        # Agent1 discovered a new field.\n",
    "        if ag1_r == 1.0:\n",
    "            episode.user_data[\"new_fields_discovered\"] += 1\n",
    "        # Collision.\n",
    "        elif ag2_r == 1.0:\n",
    "            episode.user_data[\"num_collisions\"] += 1\n",
    "\n",
    "    def on_episode_end(self,\n",
    "                       *,\n",
    "                       worker,\n",
    "                       base_env,\n",
    "                       policies,\n",
    "                       episode: MultiAgentEpisode,\n",
    "                       env_index,\n",
    "                       **kwargs):\n",
    "        # Episode is done:\n",
    "        # Write scalar values (sum over rewards) to `custom_metrics` and\n",
    "        # time-series data (rewards per time step) to `hist_data`.\n",
    "        # Both will be visible then in TensorBoard.\n",
    "        episode.custom_metrics[\"new_fields_discovered\"] = episode.user_data[\"new_fields_discovered\"]\n",
    "        episode.custom_metrics[\"num_collisions\"] = episode.user_data[\"num_collisions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd2fe8eb-c52f-4a26-9067-96ad9fe160a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44012)\u001b[0m 2021-07-01 13:28:53,236\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=44012)\u001b[0m 2021-07-01 13:28:53,236\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=44012)\u001b[0m 2021-07-01 13:29:08,596\tINFO trainable.py:101 -- Trainable.setup took 15.361 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=44012)\u001b[0m 2021-07-01 13:29:08,597\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 55\n",
      "    new_fields_discovered_mean: 33.25\n",
      "    new_fields_discovered_min: 19\n",
      "    num_collisions_max: 7\n",
      "    num_collisions_mean: 1.3\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.09999999999998\n",
      "  episode_reward_mean: -9.119999999999996\n",
      "  episode_reward_min: -31.50000000000003\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.366931676864624\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0194054264575243\n",
      "          model: {}\n",
      "          policy_loss: -0.05213673785328865\n",
      "          total_loss: 25.09111976623535\n",
      "          vf_explained_var: 0.11452364176511765\n",
      "          vf_loss: 25.139379501342773\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.075\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12687578462900367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06460477586398991\n",
      "    mean_inference_ms: 1.524347319567293\n",
      "    mean_raw_obs_processing_ms: 0.30565238058418415\n",
      "  time_since_restore: 4.9797210693359375\n",
      "  time_this_iter_s: 4.9797210693359375\n",
      "  time_total_s: 4.9797210693359375\n",
      "  timers:\n",
      "    learn_throughput: 977.247\n",
      "    learn_time_ms: 4093.129\n",
      "    load_throughput: 100862.803\n",
      "    load_time_ms: 39.658\n",
      "    sample_throughput: 4770.942\n",
      "    sample_time_ms: 838.409\n",
      "    update_time_ms: 2.479\n",
      "  timestamp: 1625113753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>RUNNING </td><td>172.30.1.40:44012</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.97972</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">   -9.12</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 55\n",
      "    new_fields_discovered_mean: 35.63333333333333\n",
      "    new_fields_discovered_min: 19\n",
      "    num_collisions_max: 20\n",
      "    num_collisions_mean: 1.7833333333333334\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-29-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.09999999999998\n",
      "  episode_reward_mean: -5.194999999999993\n",
      "  episode_reward_min: -31.50000000000003\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2905468940734863\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02482198178768158\n",
      "          model: {}\n",
      "          policy_loss: -0.05899612605571747\n",
      "          total_loss: 18.5784854888916\n",
      "          vf_explained_var: 0.3858039379119873\n",
      "          vf_loss: 18.632516860961914\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.98\n",
      "    ram_util_percent: 63.1\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12113256906445004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06174447229307377\n",
      "    mean_inference_ms: 1.3828936686488367\n",
      "    mean_raw_obs_processing_ms: 0.2942981934324153\n",
      "  time_since_restore: 13.076663255691528\n",
      "  time_this_iter_s: 4.015966176986694\n",
      "  time_total_s: 13.076663255691528\n",
      "  timers:\n",
      "    learn_throughput: 1111.214\n",
      "    learn_time_ms: 3599.668\n",
      "    load_throughput: 262462.509\n",
      "    load_time_ms: 15.24\n",
      "    sample_throughput: 5499.757\n",
      "    sample_time_ms: 727.305\n",
      "    update_time_ms: 2.765\n",
      "  timestamp: 1625113761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>RUNNING </td><td>172.30.1.40:44012</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         13.0767</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -5.195</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 55\n",
      "    new_fields_discovered_mean: 36.67\n",
      "    new_fields_discovered_min: 19\n",
      "    num_collisions_max: 20\n",
      "    num_collisions_mean: 1.73\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.09999999999998\n",
      "  episode_reward_mean: -3.629999999999992\n",
      "  episode_reward_min: -31.50000000000003\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2443851232528687\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020394615828990936\n",
      "          model: {}\n",
      "          policy_loss: -0.05789593607187271\n",
      "          total_loss: 14.557127952575684\n",
      "          vf_explained_var: 0.4738118350505829\n",
      "          vf_loss: 14.608908653259277\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.21666666666667\n",
      "    ram_util_percent: 63.16666666666668\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1186090061947701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06062627872796014\n",
      "    mean_inference_ms: 1.3189941436260477\n",
      "    mean_raw_obs_processing_ms: 0.2897123132055757\n",
      "  time_since_restore: 20.34970259666443\n",
      "  time_this_iter_s: 3.6598031520843506\n",
      "  time_total_s: 20.34970259666443\n",
      "  timers:\n",
      "    learn_throughput: 1191.519\n",
      "    learn_time_ms: 3357.059\n",
      "    load_throughput: 388589.958\n",
      "    load_time_ms: 10.294\n",
      "    sample_throughput: 5822.529\n",
      "    sample_time_ms: 686.987\n",
      "    update_time_ms: 3.755\n",
      "  timestamp: 1625113769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>RUNNING </td><td>172.30.1.40:44012</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         20.3497</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   -3.63</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 51\n",
      "    new_fields_discovered_mean: 37.99\n",
      "    new_fields_discovered_min: 23\n",
      "    num_collisions_max: 7\n",
      "    num_collisions_mean: 1.68\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-29-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.500000000000018\n",
      "  episode_reward_mean: -1.6679999999999893\n",
      "  episode_reward_min: -25.500000000000004\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 140\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1852091550827026\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01788623444736004\n",
      "          model: {}\n",
      "          policy_loss: -0.053150419145822525\n",
      "          total_loss: 18.78753662109375\n",
      "          vf_explained_var: 0.30266791582107544\n",
      "          vf_loss: 18.8326416015625\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.849999999999994\n",
      "    ram_util_percent: 62.55000000000001\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1181131675957404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05994540772966953\n",
      "    mean_inference_ms: 1.2478134582451255\n",
      "    mean_raw_obs_processing_ms: 0.2885084364351136\n",
      "  time_since_restore: 27.87849760055542\n",
      "  time_this_iter_s: 3.5366289615631104\n",
      "  time_total_s: 27.87849760055542\n",
      "  timers:\n",
      "    learn_throughput: 1224.828\n",
      "    learn_time_ms: 3265.765\n",
      "    load_throughput: 485014.793\n",
      "    load_time_ms: 8.247\n",
      "    sample_throughput: 5755.429\n",
      "    sample_time_ms: 694.996\n",
      "    update_time_ms: 3.358\n",
      "  timestamp: 1625113776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>RUNNING </td><td>172.30.1.40:44012</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         27.8785</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  -1.668</td><td style=\"text-align: right;\">                16.5</td><td style=\"text-align: right;\">               -25.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 53\n",
      "    new_fields_discovered_mean: 39.17\n",
      "    new_fields_discovered_min: 23\n",
      "    num_collisions_max: 11\n",
      "    num_collisions_mean: 2.07\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-29-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999922\n",
      "  episode_reward_mean: 0.3210000000000104\n",
      "  episode_reward_min: -25.500000000000004\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 180\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1406888961791992\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018338419497013092\n",
      "          model: {}\n",
      "          policy_loss: -0.05762483552098274\n",
      "          total_loss: 14.989665985107422\n",
      "          vf_explained_var: 0.3391481935977936\n",
      "          vf_loss: 15.03903865814209\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.67999999999999\n",
      "    ram_util_percent: 62.779999999999994\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11803921048789587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05954944749197705\n",
      "    mean_inference_ms: 1.220679958973994\n",
      "    mean_raw_obs_processing_ms: 0.287172490330815\n",
      "  time_since_restore: 35.29966473579407\n",
      "  time_this_iter_s: 3.7090871334075928\n",
      "  time_total_s: 35.29966473579407\n",
      "  timers:\n",
      "    learn_throughput: 1240.582\n",
      "    learn_time_ms: 3224.292\n",
      "    load_throughput: 568117.902\n",
      "    load_time_ms: 7.041\n",
      "    sample_throughput: 5899.269\n",
      "    sample_time_ms: 678.05\n",
      "    update_time_ms: 3.243\n",
      "  timestamp: 1625113784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>RUNNING </td><td>172.30.1.40:44012</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         35.2997</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   0.321</td><td style=\"text-align: right;\">                19.5</td><td style=\"text-align: right;\">               -25.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 40.29\n",
      "    new_fields_discovered_min: 23\n",
      "    num_collisions_max: 15\n",
      "    num_collisions_mean: 2.0\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-29-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.59999999999993\n",
      "  episode_reward_mean: 1.9890000000000065\n",
      "  episode_reward_min: -25.500000000000004\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 220\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0832452774047852\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019080184400081635\n",
      "          model: {}\n",
      "          policy_loss: -0.05616436526179314\n",
      "          total_loss: 17.575571060180664\n",
      "          vf_explained_var: 0.39673444628715515\n",
      "          vf_loss: 17.623151779174805\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.11666666666667\n",
      "    ram_util_percent: 63.38333333333333\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11705946621546788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05900932722258489\n",
      "    mean_inference_ms: 1.1999797503963319\n",
      "    mean_raw_obs_processing_ms: 0.28446686390339354\n",
      "  time_since_restore: 42.64056086540222\n",
      "  time_this_iter_s: 3.592686891555786\n",
      "  time_total_s: 42.64056086540222\n",
      "  timers:\n",
      "    learn_throughput: 1295.681\n",
      "    learn_time_ms: 3087.18\n",
      "    load_throughput: 1358941.178\n",
      "    load_time_ms: 2.943\n",
      "    sample_throughput: 6027.603\n",
      "    sample_time_ms: 663.614\n",
      "    update_time_ms: 3.136\n",
      "  timestamp: 1625113791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>RUNNING </td><td>172.30.1.40:44012</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         42.6406</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   1.989</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -25.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 42.38\n",
      "    new_fields_discovered_min: 28\n",
      "    num_collisions_max: 15\n",
      "    num_collisions_mean: 1.99\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-30-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.79999999999997\n",
      "  episode_reward_mean: 5.181000000000004\n",
      "  episode_reward_min: -13.799999999999978\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 260\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0272111892700195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018489399924874306\n",
      "          model: {}\n",
      "          policy_loss: -0.05418701097369194\n",
      "          total_loss: 18.107131958007812\n",
      "          vf_explained_var: 0.39192599058151245\n",
      "          vf_loss: 18.152997970581055\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.22857142857143\n",
      "    ram_util_percent: 61.24285714285714\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11667348228892842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.059042112853453066\n",
      "    mean_inference_ms: 1.1960474588607206\n",
      "    mean_raw_obs_processing_ms: 0.2844542811948252\n",
      "  time_since_restore: 51.0676908493042\n",
      "  time_this_iter_s: 4.719112873077393\n",
      "  time_total_s: 51.0676908493042\n",
      "  timers:\n",
      "    learn_throughput: 1289.26\n",
      "    learn_time_ms: 3102.555\n",
      "    load_throughput: 1334235.908\n",
      "    load_time_ms: 2.998\n",
      "    sample_throughput: 5868.51\n",
      "    sample_time_ms: 681.604\n",
      "    update_time_ms: 3.621\n",
      "  timestamp: 1625113800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>RUNNING </td><td>172.30.1.40:44012</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         51.0677</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   5.181</td><td style=\"text-align: right;\">                22.8</td><td style=\"text-align: right;\">               -13.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 43.47\n",
      "    new_fields_discovered_min: 32\n",
      "    num_collisions_max: 13\n",
      "    num_collisions_mean: 2.1\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.79999999999997\n",
      "  episode_reward_mean: 6.878999999999999\n",
      "  episode_reward_min: -11.399999999999988\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 300\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9820687770843506\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01893707364797592\n",
      "          model: {}\n",
      "          policy_loss: -0.054366763681173325\n",
      "          total_loss: 20.352720260620117\n",
      "          vf_explained_var: 0.3411625623703003\n",
      "          vf_loss: 20.3985652923584\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.64\n",
      "    ram_util_percent: 61.279999999999994\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11778914039848339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05979875821379459\n",
      "    mean_inference_ms: 1.2111722145934218\n",
      "    mean_raw_obs_processing_ms: 0.28801715821855434\n",
      "  time_since_restore: 58.388362884521484\n",
      "  time_this_iter_s: 3.5008461475372314\n",
      "  time_total_s: 58.388362884521484\n",
      "  timers:\n",
      "    learn_throughput: 1297.415\n",
      "    learn_time_ms: 3083.054\n",
      "    load_throughput: 1308664.987\n",
      "    load_time_ms: 3.057\n",
      "    sample_throughput: 5662.511\n",
      "    sample_time_ms: 706.4\n",
      "    update_time_ms: 3.15\n",
      "  timestamp: 1625113807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>RUNNING </td><td>172.30.1.40:44012</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         58.3884</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">   6.879</td><td style=\"text-align: right;\">                22.8</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 43.78\n",
      "    new_fields_discovered_min: 30\n",
      "    num_collisions_max: 13\n",
      "    num_collisions_mean: 2.16\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-30-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.79999999999997\n",
      "  episode_reward_mean: 7.3439999999999985\n",
      "  episode_reward_min: -11.999999999999984\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 340\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9339596033096313\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018967602401971817\n",
      "          model: {}\n",
      "          policy_loss: -0.05348660424351692\n",
      "          total_loss: 19.252872467041016\n",
      "          vf_explained_var: 0.36730077862739563\n",
      "          vf_loss: 19.297826766967773\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.7\n",
      "    ram_util_percent: 62.6\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1184970013486138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060285077793669206\n",
      "    mean_inference_ms: 1.2195518935003238\n",
      "    mean_raw_obs_processing_ms: 0.29029058719865936\n",
      "  time_since_restore: 65.66904592514038\n",
      "  time_this_iter_s: 3.5117712020874023\n",
      "  time_total_s: 65.66904592514038\n",
      "  timers:\n",
      "    learn_throughput: 1298.426\n",
      "    learn_time_ms: 3080.652\n",
      "    load_throughput: 1329330.629\n",
      "    load_time_ms: 3.009\n",
      "    sample_throughput: 5846.909\n",
      "    sample_time_ms: 684.122\n",
      "    update_time_ms: 3.141\n",
      "  timestamp: 1625113814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>RUNNING </td><td>172.30.1.40:44012</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">          65.669</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">   7.344</td><td style=\"text-align: right;\">                22.8</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 43.53\n",
      "    new_fields_discovered_min: 30\n",
      "    num_collisions_max: 10\n",
      "    num_collisions_mean: 2.06\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-30-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.1\n",
      "  episode_reward_mean: 6.839999999999993\n",
      "  episode_reward_min: -11.999999999999984\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 380\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8836515545845032\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0193608608096838\n",
      "          model: {}\n",
      "          policy_loss: -0.05156983435153961\n",
      "          total_loss: 21.538658142089844\n",
      "          vf_explained_var: 0.32879477739334106\n",
      "          vf_loss: 21.581518173217773\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.6\n",
      "    ram_util_percent: 62.1\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11743091133172903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05979976120390223\n",
      "    mean_inference_ms: 1.2058889349238162\n",
      "    mean_raw_obs_processing_ms: 0.2879508911144666\n",
      "  time_since_restore: 72.80030512809753\n",
      "  time_this_iter_s: 3.540674924850464\n",
      "  time_total_s: 72.80030512809753\n",
      "  timers:\n",
      "    learn_throughput: 1308.534\n",
      "    learn_time_ms: 3056.855\n",
      "    load_throughput: 1189679.414\n",
      "    load_time_ms: 3.362\n",
      "    sample_throughput: 5903.358\n",
      "    sample_time_ms: 677.58\n",
      "    update_time_ms: 3.056\n",
      "  timestamp: 1625113821\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>RUNNING </td><td>172.30.1.40:44012</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         72.8003</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">    6.84</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_d2605_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 61\n",
      "    new_fields_discovered_mean: 44.25\n",
      "    new_fields_discovered_min: 30\n",
      "    num_collisions_max: 10\n",
      "    num_collisions_mean: 2.25\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_13-30-25\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 33.299999999999905\n",
      "  episode_reward_mean: 8.00999999999999\n",
      "  episode_reward_min: -11.999999999999984\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 400\n",
      "  experiment_id: 1b52ca63368549ca92f67da3fee55fbb\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.84686279296875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019242260605096817\n",
      "          model: {}\n",
      "          policy_loss: -0.052279114723205566\n",
      "          total_loss: 30.78931999206543\n",
      "          vf_explained_var: 0.32996872067451477\n",
      "          vf_loss: 30.832937240600586\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.0\n",
      "    ram_util_percent: 62.580000000000005\n",
      "  pid: 44012\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11659898863153878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05941030508355318\n",
      "    mean_inference_ms: 1.1956826340121645\n",
      "    mean_raw_obs_processing_ms: 0.28604903474285515\n",
      "  time_since_restore: 76.32245922088623\n",
      "  time_this_iter_s: 3.5221540927886963\n",
      "  time_total_s: 76.32245922088623\n",
      "  timers:\n",
      "    learn_throughput: 1310.804\n",
      "    learn_time_ms: 3051.563\n",
      "    load_throughput: 1176803.445\n",
      "    load_time_ms: 3.399\n",
      "    sample_throughput: 6061.176\n",
      "    sample_time_ms: 659.938\n",
      "    update_time_ms: 3.098\n",
      "  timestamp: 1625113825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: d2605_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_d2605_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         76.3225</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    8.01</td><td style=\"text-align: right;\">                33.3</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-01 13:30:26,036\tINFO tune.py:549 -- Total run time: 101.23 seconds (100.75 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f819120e9a0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_trainer.stop()\n",
    "# Setting up our config to point to our new custom callbacks class:\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,\n",
    "    \"callbacks\": MyCallbacks,  # by default, this would point to `rllib.agents.callbacks.DefaultCallbacks`, which does nothing.\n",
    "    \"num_workers\": 5,  # we know now: this speeds up things!\n",
    "}\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop={\"training_iteration\": 20},\n",
    "    checkpoint_at_end=True,\n",
    "    # If you'd like to restore the tune run from an existing checkpoint file, you can do the following:\n",
    "    #restore=\"/Users/sven/ray_results/PPO/PPO_MultiAgentArena_fd451_00000_0_2021-05-25_15-13-26/checkpoint_000010/checkpoint-10\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa6a24",
   "metadata": {},
   "source": [
    "### Let's check tensorboard for the new custom metrics!\n",
    "\n",
    "1. Head over to the Anyscale project view and click on the \"TensorBoard\" butten:\n",
    "\n",
    "<img src=\"images/tensorboard_button.png\" width=1000>\n",
    "\n",
    "Alternatively - if you ran this locally on your own machine:\n",
    "\n",
    "1. Head over to ~/ray_results/PPO/PPO_MultiAgentArena_[some key]_00000_0_[date]_[time]/\n",
    "1. In that directory, you should see a `event.out....` file.\n",
    "1. Run `tensorboard --logdir .` and head to https://localhost:6006\n",
    "\n",
    "<img src=\"images/tensorboard.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac90dc-097d-4f10-b5ea-c4c1167f1f3a",
   "metadata": {},
   "source": [
    "### Deep Dive: Writing custom Models in tf or torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5516d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "tf1, tf, tf_version = try_import_tf()\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "# Custom Neural Network Models.\n",
    "class MyKerasModel(TFModelV2):\n",
    "    \"\"\"Custom model for policy gradient algorithms.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        super(MyKerasModel, self).__init__(obs_space, action_space,\n",
    "                                           num_outputs, model_config, name)\n",
    "        \n",
    "        # Keras Input layer.\n",
    "        self.inputs = tf.keras.layers.Input(\n",
    "            shape=obs_space.shape, name=\"observations\")\n",
    "\n",
    "        # Hidden layer (shared by action logits outputs and value output).\n",
    "        layer_1 = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            name=\"layer1\",\n",
    "            activation=tf.nn.relu)(self.inputs)\n",
    "        \n",
    "        # Action logits output.\n",
    "        logits = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            name=\"out\",\n",
    "            activation=None)(layer_1)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        value_out = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            name=\"value\",\n",
    "            activation=None)(layer_1)\n",
    "\n",
    "        # The actual Keras model:\n",
    "        self.base_model = tf.keras.Model(self.inputs,\n",
    "                                         [logits, value_out])\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers and calculate the \"value\"\n",
    "        # of the observation and store it for when `value_function` is called.\n",
    "        logits, self.cur_value = self.base_model(input_dict[\"obs\"])\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return tf.reshape(self.cur_value, [-1])\n",
    "\n",
    "\n",
    "class MyTorchModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.device = torch.device(\"cuda\"\n",
    "                                   if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Hidden layer (shared by action logits outputs and value output).\n",
    "        self.layer_1 = nn.Linear(obs_space.shape[0], 16).to(self.device)\n",
    "\n",
    "        # Action logits output.\n",
    "        self.layer_out = nn.Linear(16, num_outputs).to(self.device)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        self.value_branch = nn.Linear(16, 1).to(self.device)\n",
    "        self.cur_value = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers.\n",
    "        layer_1_out = self.layer_1(input_dict[\"obs\"])\n",
    "        logits = self.layer_out(layer_1_out)\n",
    "\n",
    "        # Calculate the \"value\" of the observation and store it for\n",
    "        # when `value_function` is called.\n",
    "        self.cur_value = self.value_branch(layer_1_out).squeeze(1)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return self.cur_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "controversial-repair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch-output=(tensor([[ 0.6229, -0.7027]], grad_fn=<AddmmBackward>), [])\n"
     ]
    }
   ],
   "source": [
    "# Do a quick test on the custom model classes.\n",
    "#test_model_tf = MyKerasModel(\n",
    "#    obs_space=gym.spaces.Box(-1.0, 1.0, (2, )),\n",
    "#    action_space=None,\n",
    "#    num_outputs=2,\n",
    "#    model_config={},\n",
    "#    name=\"MyModel\",\n",
    "#)\n",
    "\n",
    "#print(\"TF-output={}\".format(test_model_tf({\"obs\": np.array([[0.5, 0.5]])})))\n",
    "\n",
    "# For PyTorch, you can do:\n",
    "test_model_torch = MyTorchModel(\n",
    "    obs_space=gym.spaces.Box(-1.0, 1.0, (2, )),\n",
    "    action_space=None,\n",
    "    num_outputs=2,\n",
    "    model_config={},\n",
    "    name=\"MyModel\",\n",
    ")\n",
    "print(\"Torch-output={}\".format(test_model_torch({\"obs\": torch.from_numpy(np.array([[0.5, 0.5]], dtype=np.float32))})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2237526a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_9889f_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=66554)\u001b[0m 2021-07-01 14:24:30,416\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/parksurk/anaconda3/envs/rllib/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=66555)\u001b[0m 2021-07-01 14:24:40,338\tWARNING catalog.py:548 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[2m\u001b[36m(pid=66554)\u001b[0m 2021-07-01 14:24:40,452\tWARNING catalog.py:548 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[2m\u001b[36m(pid=66554)\u001b[0m 2021-07-01 14:24:40,554\tINFO trainable.py:101 -- Trainable.setup took 10.139 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=66554)\u001b[0m 2021-07-01 14:24:40,555\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=68951)\u001b[0m 2021-07-01 14:24:41,218\tWARNING catalog.py:548 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[2m\u001b[36m(pid=68949)\u001b[0m 2021-07-01 14:24:41,282\tWARNING catalog.py:548 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[2m\u001b[36m(pid=68950)\u001b[0m 2021-07-01 14:24:41,274\tWARNING catalog.py:548 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[2m\u001b[36m(pid=68948)\u001b[0m 2021-07-01 14:24:41,345\tWARNING catalog.py:548 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_9889f_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 47\n",
      "    new_fields_discovered_mean: 29.2\n",
      "    new_fields_discovered_min: 17\n",
      "    num_collisions_max: 5\n",
      "    num_collisions_mean: 1.25\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_14-24-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.000000000000023\n",
      "  episode_reward_mean: -15.045000000000002\n",
      "  episode_reward_min: -31.500000000000036\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 8219602d9bd44911809d9131340f776b\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.3840322196483612\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0024876923416741192\n",
      "          policy_loss: -0.005995301267830655\n",
      "          total_loss: 38.67150855064392\n",
      "          vf_explained_var: 0.0033528003841638565\n",
      "          vf_loss: 38.67700678110123\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.34285714285715\n",
      "    ram_util_percent: 62.0\n",
      "  pid: 66554\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10975150396104465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06327688545360233\n",
      "    mean_inference_ms: 1.2435235287483195\n",
      "    mean_raw_obs_processing_ms: 0.33128695594996893\n",
      "  time_since_restore: 4.35012412071228\n",
      "  time_this_iter_s: 4.35012412071228\n",
      "  time_total_s: 4.35012412071228\n",
      "  timers:\n",
      "    learn_throughput: 1468.01\n",
      "    learn_time_ms: 2724.777\n",
      "    sample_throughput: 2470.222\n",
      "    sample_time_ms: 1619.288\n",
      "    update_time_ms: 1.72\n",
      "  timestamp: 1625117084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 9889f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_9889f_00000</td><td>RUNNING </td><td>172.30.1.40:66554</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.35012</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -15.045</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_9889f_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 53\n",
      "    new_fields_discovered_mean: 33.18333333333333\n",
      "    new_fields_discovered_min: 16\n",
      "    num_collisions_max: 7\n",
      "    num_collisions_mean: 1.1666666666666667\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_14-24-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.999999999999922\n",
      "  episode_reward_mean: -9.130000000000003\n",
      "  episode_reward_min: -35.40000000000004\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 8219602d9bd44911809d9131340f776b\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.3656450547277927\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003002431447384879\n",
      "          policy_loss: -0.0022404889023164287\n",
      "          total_loss: 25.51762819290161\n",
      "          vf_explained_var: 0.05033271759748459\n",
      "          vf_loss: 25.519568383693695\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.64\n",
      "    ram_util_percent: 62.2\n",
      "  pid: 66554\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1026019057903363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057932917709920775\n",
      "    mean_inference_ms: 1.1282550813334746\n",
      "    mean_raw_obs_processing_ms: 0.2987427531543274\n",
      "  time_since_restore: 10.562437057495117\n",
      "  time_this_iter_s: 2.9966278076171875\n",
      "  time_total_s: 10.562437057495117\n",
      "  timers:\n",
      "    learn_throughput: 1566.573\n",
      "    learn_time_ms: 2553.344\n",
      "    sample_throughput: 4170.908\n",
      "    sample_time_ms: 959.024\n",
      "    update_time_ms: 2.033\n",
      "  timestamp: 1625117091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 9889f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_9889f_00000</td><td>RUNNING </td><td>172.30.1.40:66554</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         10.5624</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   -9.13</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">               -35.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_9889f_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 53\n",
      "    new_fields_discovered_mean: 34.65\n",
      "    new_fields_discovered_min: 16\n",
      "    num_collisions_max: 8\n",
      "    num_collisions_mean: 1.38\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-07-01_14-24-56\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.999999999999922\n",
      "  episode_reward_mean: -6.842999999999997\n",
      "  episode_reward_min: -35.40000000000004\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 8219602d9bd44911809d9131340f776b\n",
      "  hostname: SKCC17N00536.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.025\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.3530115261673927\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0003534120914991945\n",
      "          policy_loss: -0.005451907229144126\n",
      "          total_loss: 18.50159814953804\n",
      "          vf_explained_var: 0.13075627386569977\n",
      "          vf_loss: 18.507041066884995\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.30.1.40\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.35\n",
      "    ram_util_percent: 62.275000000000006\n",
      "  pid: 66554\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1022925296159217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057509291608708324\n",
      "    mean_inference_ms: 1.1116416022410753\n",
      "    mean_raw_obs_processing_ms: 0.29433503485596957\n",
      "  time_since_restore: 15.565790176391602\n",
      "  time_this_iter_s: 2.345473051071167\n",
      "  time_total_s: 15.565790176391602\n",
      "  timers:\n",
      "    learn_throughput: 1732.454\n",
      "    learn_time_ms: 2308.864\n",
      "    sample_throughput: 5023.707\n",
      "    sample_time_ms: 796.225\n",
      "    update_time_ms: 1.925\n",
      "  timestamp: 1625117096\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 9889f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_9889f_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         15.5658</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -6.843</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">               -35.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.12 GiB objects<br>Result logdir: /Users/parksurk/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_9889f_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         15.5658</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -6.843</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">               -35.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-01 14:24:56,857\tINFO tune.py:549 -- Total run time: 33.11 seconds (32.55 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f8192ce3610>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up our custom model and re-run the experiment.\n",
    "#config.update({\n",
    "#    \"model\": {\n",
    "#        \"custom_model\": MyKerasModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "#        \"custom_model_config\": {\n",
    "#            #\"layers\": [128, 128],\n",
    "#        },\n",
    "#    },\n",
    "#})\n",
    "\n",
    "config.update({\n",
    "    \"model\": {\n",
    "        \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "        \"custom_model_config\": {\n",
    "            \"layers\": [128, 128],\n",
    "        },\n",
    "    },\n",
    "})\n",
    "\n",
    "#tune.run(\n",
    "#    \"PPO\",\n",
    "#    config=config,  # for torch users: config=dict(config, **{\"framework\": \"torch\"}),\n",
    "#    stop={\n",
    "#        \"training_iteration\": 5,\n",
    "#    },\n",
    "#)\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=dict(config, **{\"framework\": \"torch\"}),  # for torch users: config=dict(config, **{\"framework\": \"torch\"}),\n",
    "    stop={\n",
    "        \"training_iteration\": 5,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ad95",
   "metadata": {},
   "source": [
    "### Deep Dive: A closer look at RLlib's components\n",
    "#### (Depending on time left and amount of questions having been accumulated :)\n",
    "\n",
    "We already took a quick look inside an RLlib Trainer object and extracted its Policy(ies) and the Policy's model (neural network). Here is a much more detailed overview of what's inside a Trainer object.\n",
    "\n",
    "At the core is the so-called `WorkerSet` sitting under `Trainer.workers`. A WorkerSet is a group of `RolloutWorker` (`rllib.evaluation.rollout_worker.py`) objects that always consists of a \"local worker\" (`Trainer.workers.local_worker()`) and n \"remote workers\" (`Trainer.workers.remote_workers()`).\n",
    "\n",
    "\n",
    "우리는 이미 RLlib Trainer 객체를 내부를 간단히 보고 정책과 정책 모델(신경망)을 추출했습니다. 다음은 트레이너 내부에 대한 훨씬 더 많은 개체에 대한 간략한 Overview입니다.\n",
    "\n",
    "핵심은 `Trainer.workers` 아래에 있는 소위 `WorkerSet`에 있습니다. WorkerSet은 항상 1개의 \"Local worker\"(`Trainer.workers.local_worker()`)와 n개의 \"Remote worker\"(`Trainer.workers.remote_workers()`)로 이루어진 `RolloutWorker` (`rllib.evaluation.rollout_worker.py`) 의 그룹입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f37549",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_structure.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d72883",
   "metadata": {},
   "source": [
    "### Scaling RLlib\n",
    "\n",
    "Scaling RLlib works by parallelizing the \"jobs\" that the remote `RolloutWorkers` do. In a vanilla RL algorithm, like PPO, DQN, and many others, the `@ray.remote` labeled RolloutWorkers in the figure above are responsible for interacting with one or more environments and thereby collecting experiences. Observations are produced by the environment, actions are then computed by the Policy(ies) copy located on the remote worker and sent to the environment in order to produce yet another observation. This cycle is repeated endlessly and only sometimes interrupted to send experience batches (\"train batches\") of a certain size to the \"local worker\". There these batches are used to call `Policy.learn_on_batch()`, which performs a loss calculation, followed by a model weights update, and a subsequent weights broadcast back to all the remote workers.\n",
    "\n",
    "RLlib 확장은 원격 `RolloutWorkers`가 수행하는 \"작업(Jobs)\"을 병렬화하여 작동합니다. PPO, DQN 등과 같은 바닐라 RL 알고리즘에서 위의 그림에서 RolloutWorkers라는 레이블이 붙은`@ray.remote`는 하나 이상의 환경과 상호 작용하여 경험을 수집합니다. Observation은 환경에 의해 생성되고 Action은 원격 작업자(Remote worker)에 있는 Policy 사본에 의해 계산되고 또 다른 Observation을 생성하기 위해 환경으로 전송됩니다. 이 사이클은 끝없이 반복되며 특정 크기의 경험(Experience) 배치(\"학습 배치\")를 \"로컬 작업자(Local worker)\"에게 보내기 위해 때때로 중단됩니다. 여기에서 이러한 배치는 Loss 계산을 수행하고 모델 가중치 업데이트를 수행하고 후속 가중치가 모든 원격 작업자에게 다시 브로드 캐스트되는 `Policy.learn_on_batch()`를 호출하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials.git\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/master/rllib.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f116b3-0962-44ff-9c08-558c6890abd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "rllib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
